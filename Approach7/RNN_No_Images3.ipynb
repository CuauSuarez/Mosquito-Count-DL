{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RNN_No_Images3.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMa7qcO9jTyodaKkv1iE9F1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"FUEm9QBvYyXr","colab_type":"code","colab":{}},"source":["# Epale Mario! mi piernita\n","\n","import argparse\n","import yaml\n","import time\n","import datetime\n","import cv2\n","import numpy as np\n","import pandas as pd\n","import random\n","\n","from skimage import io, transform\n","import matplotlib.pyplot as plt\n","from scipy.ndimage import zoom\n","from scipy import ndimage, misc\n","\n","import torch\n","import torch.utils.data\n","from torch import nn, optim\n","from torch.nn import functional as F\n","from torchvision import datasets, transforms, utils\n","from torchvision.utils import save_image\n","from torch.utils.data import Dataset, DataLoader, TensorDataset\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import torchvision\n","import torch\n","from torch.autograd import Variable\n","import torch.optim as optim\n","\n","from google.colab import drive\n","import os\n","\n","import warnings\n","from collections import defaultdict\n","\n","from scipy import stats\n","from sklearn.metrics import mean_absolute_error"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vkVVfsA_Y9x5","colab_type":"code","outputId":"5ec76d8f-ff33-4bca-b781-0123bcb00f83","executionInfo":{"status":"ok","timestamp":1587005041461,"user_tz":300,"elapsed":114656,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":128}},"source":["drive.mount('/content/drive', force_remount = True)\n","os.chdir('/content/drive/My Drive/Mosquito-Tec/DA-RNN')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mAFzqb7tZAwM","colab_type":"code","colab":{}},"source":["# Open Data\n","\n","train_csv_file = '/content/drive/My Drive/Colab/mosquito/Final_Mosquito_train6_W.csv'\n","train_frame = pd.read_csv(train_csv_file)\n","\n","train_frame[\"TRAPSET\"] = pd.to_datetime(train_frame[\"TRAPSET\"])\n","train_frame[\"TRAPCOLLECT\"] = pd.to_datetime(train_frame[\"TRAPCOLLECT\"])\n","\n","train_frame[\"TRAPDAYS\"] = (train_frame[\"TRAPCOLLECT\"] - train_frame[\"TRAPSET\"]).dt.days\n","\n","train_frame[\"TRAPSET\"] = train_frame[\"TRAPSET\"].dt.week\n","train_frame[\"TRAPCOLLECT\"] = train_frame[\"TRAPCOLLECT\"].dt.week\n","\n","# Test data\n","\n","test_csv_file = '/content/drive/My Drive/Colab/mosquito/Final_Mosquito_test6_W.csv'\n","test_frame = pd.read_csv(test_csv_file)\n","\n","test_frame[\"TRAPSET\"] = pd.to_datetime(test_frame[\"TRAPSET\"])\n","test_frame[\"TRAPCOLLECT\"] = pd.to_datetime(test_frame[\"TRAPCOLLECT\"])\n","\n","test_frame[\"TRAPDAYS\"] = (test_frame[\"TRAPCOLLECT\"] - test_frame[\"TRAPSET\"]).dt.days\n","\n","test_frame[\"TRAPSET\"] = test_frame[\"TRAPSET\"].dt.week\n","test_frame[\"TRAPCOLLECT\"] = test_frame[\"TRAPCOLLECT\"].dt.week"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W8m5PuFCRrfe","colab_type":"code","colab":{}},"source":["non_temporal_columns = [\"OBJECTID\", \"X\", \"Y\", \"TRAPTYPE\", \"ATTRACTANTUSED\",\n","                        \"TRAPID\", \"LATITUDE\", \"LONGITUDE\", \"ADDRESS\", \"TOWN\",\n","                        \"STATE\", \"COUNTY\", \"TRAPSITE\", \"TRAPSET\", \"SETTIMEOFDAY\",\n","                        \"YEAR\", \"TRAPCOLLECT\", \"COLLECTTIMEOFDAY\", \"GENUS\",\n","                        \"SPECIES\", \"LIFESTAGE\", \"EGGSCOLLECTED\", \"LARVAECOLLECTED\",\n","                        \"PUPAECOLLECTED\", \"REPORTDATE\", \"TRAPDAYS\"]\n","\n","non_num_columns = [\"LATITUDE\", \"LONGITUDE\", \"TRAPDAYS\", \"TRAPSET\"]\n","\n","categorical_columnsT = [\"TRAPTYPE\", \"ATTRACTANTSUSED\", \"TRAPID\", \"ADDRESS\", \"TOWN\",\n","                        \"STATE\", \"COUNTY\", \"TRAPSITE\", \"TRAPSET\", \"SETTIMEOFDAY\",\n","                        \"TRAPCOLLECT\", \"COLLECTTIMEOFDAY\", \"GENUS\",\n","                        \"SPECIES\", \"LIFESTAGE\", \"EGGSCOLLECTED\", \"LARVAECOLLECTED\",\n","                        \"PUPAECOLLECTED\", \"REPORTDATE\"]\n","\n","categorical_columns = [\"TRAPTYPE\", \"ATTRACTANTSUSED\"]\n","\n","temporal_columns = [\"sunriseTime\", \"sunsetTime\", \"moonPhase\", \"precipIntensity\",\n","                    \"precipIntensityMax\", \"precipProbability\", \"temperatureHigh\",\n","                    \"temperatureHighTime\", \"temperatureLow\", \"temperatureLowTime\",\n","                    \"apparentTemperatureHigh\", \"apparentTemperatureHighTime\",\n","                    \"apparentTemperatureLow\", \"apparentTemperatureLowTime\",\n","                    \"dewPoint\", \"humidity\", \"pressure\", \"windSpeed\", \"windGust\",\n","                    \"windGustTime\", \"windBearing\", \"cloudCover\", \"uvIndex\", \n","                    \"uvIndexTime\", \"visibility\", \"temperatureMin\", \"temperatureMinTime\",\n","                    \"temperatureMax\", \"temperatureMaxTime\", \"apparentTemperatureMin\",\n","                    \"apparentTemperatureMinTime\", \"apparentTemperatureMax\", \"apparentTemperatureMaxTime\",\n","                    \"icon\", \"time\", \"precipIntensityMaxTime\", \"precipType\", \"summary\"] \n","\n","numerical_columns = [\"temperatureHigh\", \"uvIndex\", \"precipIntensityMaxTime\", \n","                     \"sunriseTime\", \"sunsetTime\", \"temperatureLow\", \"temperatureLowTime\",\n","                     \"temperatureHighTime\", \"humidity\"]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eKTqERJZEAvP","colab_type":"code","colab":{}},"source":["SEQ_LENGTH = 14\n","NUM_ENTRIES = train_frame.shape[0]\n","\n","column_means = {}\n","column_maxs = {}\n","column_mins = {}\n","\n","for col in numerical_columns:\n","    column_means[col] = 0\n","    column_maxs[col] = 0\n","    column_mins[col] = np.inf\n","\n","    for i in range(1, SEQ_LENGTH + 1):\n","        train_frame[col + str(i)].replace(to_replace=r'No*', value=np.nan, regex=True, inplace=True)\n","        test_frame[col + str(i)].replace(to_replace=r'No*', value=np.nan, regex=True, inplace=True)\n","\n","        if(train_frame[col + str(i)].dtype == np.dtype(object)):\n","            train_frame[col + str(i)] = train_frame[col + str(i)].astype(\"float64\", copy=\"False\")\n","\n","        if(test_frame[col + str(i)].dtype == np.dtype(object)):\n","            test_frame[col + str(i)] = test_frame[col + str(i)].astype(\"float64\", copy=\"False\")\n","\n","        cur_col = train_frame[col + str(i)]\n","\n","        col_mean = cur_col.mean()\n","\n","        train_frame[col + str(i)].replace(to_replace=np.nan, value=0.0, inplace=True)\n","        test_frame[col + str(i)].replace(to_replace=np.nan, value=0.0, inplace=True)\n","\n","        column_means[col] += col_mean * NUM_ENTRIES\n","        if cur_col.max() > column_maxs[col]:\n","            column_maxs[col] = cur_col.max()\n","\n","        if cur_col.min() < column_mins[col]:\n","            column_mins[col] = cur_col.min()\n","\n","    column_means[col] /= SEQ_LENGTH * NUM_ENTRIES\n","\n","    for i in range(1, SEQ_LENGTH + 1):\n","        test_frame[col + str(i)] = (test_frame[col + str(i)] - column_means[col]) / (column_maxs[col] - column_mins[col])\n","        train_frame[col + str(i)] = (train_frame[col + str(i)] - column_means[col]) / (column_maxs[col] - column_mins[col])\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aoXrda1jLe0z","colab_type":"code","colab":{}},"source":["# Standardize Numerical Columns (not Temporal)\n","\n","num_means = {}\n","num_maxs = {}\n","num_mins = {}\n","\n","for col in non_num_columns:\n","    cur_col = train_frame[col]\n","    num_means[col] = cur_col.mean()\n","    num_maxs[col] = cur_col.max()\n","    num_mins[col] = cur_col.min()\n","\n","    train_frame[col] = (cur_col - num_means[col]) / (num_maxs[col] - num_mins[col])\n","    test_frame[col] = (test_frame[col] - num_means[col]) / (num_maxs[col] - num_mins[col])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fT25ApU8vdWW","colab_type":"code","outputId":"f150f7cd-21f3-48df-b30e-69ab5fe2ab2f","executionInfo":{"status":"ok","timestamp":1587005292894,"user_tz":300,"elapsed":404,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Convert categories\n","ats_train = []\n","ats_test = []\n","\n","for category in categorical_columns:\n","    train_frame[category] = train_frame[category].astype('category')\n","    test_frame[category] = test_frame[category].astype('category')\n","    ats_train.append(train_frame[category].cat.codes.values)\n","    ats_test.append(test_frame[category].cat.codes.values)\n","\n","train_cat = np.stack(ats_train, 1)\n","test_cat = np.stack(ats_test, 1)\n","\n","categorical_column_sizes = [len(train_frame[column].cat.categories) for column in categorical_columns]\n","embedding_sizes = [(col_size, min(50, (col_size + 1) // 2)) for col_size in categorical_column_sizes]\n","\n","print(embedding_sizes)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[(3, 2), (4, 2)]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KOy39SYAGsgy","colab_type":"code","colab":{}},"source":["# Organize info by batches/sequences\n","# Shape [784, 14, 35] -> Train\n","\n","#train_features = np.zeros(shape=(train_frame.shape[0], SEQ_LENGTH, len(numerical_columns)+len(non_num_columns)))\n","#test_features = np.zeros(shape=(test_frame.shape[0], SEQ_LENGTH, len(numerical_columns)+len(non_num_columns)))\n","\n","train_features = np.zeros(shape=(train_frame.shape[0], SEQ_LENGTH, len(numerical_columns)))\n","test_features = np.zeros(shape=(test_frame.shape[0], SEQ_LENGTH, len(numerical_columns)))\n","\n","for i in range(1, SEQ_LENGTH + 1):\n","    for row, col in enumerate(numerical_columns):\n","        train_features[:, i-1, row] = train_frame[col + str(i)]\n","        test_features[:, i-1, row] = test_frame[col + str(i)]\n","\n","    '''    \n","    for row, col in enumerate(non_num_columns):\n","        train_features[:, i-1, row+len(numerical_columns)] = train_frame[col]\n","        test_features[:, i-1, row+len(numerical_columns)] = test_frame[col]\n","    '''"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J0aIP-SKMQ9R","colab_type":"code","colab":{}},"source":["# Organize info that is not temporal\n","train_y = train_frame[\"TOTAL\"].to_numpy()\n","test_y = test_frame[\"TOTAL\"].to_numpy()\n","\n","train_X = np.zeros(shape=(train_frame.shape[0], len(non_num_columns)))\n","test_X = np.zeros(shape=(test_frame.shape[0], len(non_num_columns)))\n","\n","for row, col in enumerate(non_num_columns):\n","    train_X[:, row] = train_frame[col]\n","    test_X[:, row] = test_frame[col]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4SLM3B2ZtcU4","colab_type":"text"},"source":["### Danger! Cuau Validation: DNR"]},{"cell_type":"code","metadata":{"id":"ICC4CaoOy4eh","colab_type":"code","outputId":"7b682d93-78d4-42eb-c0c9-293f2a11284f","executionInfo":{"status":"ok","timestamp":1587005296327,"user_tz":300,"elapsed":413,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Create DataLoaders\n","# number of subprocesses to use for data loading\n","num_workers = 0\n","# how many samples per batch to load\n","batch_size = 16\n","# percentage of training set to use as validation\n","valid_size = 0.2\n","\n","\n","# obtain training indices that will be used for validation\n","num_train = len(train_X)\n","indices = list(range(num_train))\n","np.random.shuffle(indices)\n","split = int(np.floor(valid_size * num_train))\n","train_idx, valid_idx = indices[split:], indices[:split]\n","\n","print(len(train_idx), len(valid_idx))\n","\n","# define samplers for obtaining training and validation batches\n","train_sampler = SubsetRandomSampler(train_idx)\n","valid_sampler = SubsetRandomSampler(valid_idx)\n","\n","train_data = TensorDataset(torch.from_numpy(train_features), \n","                           torch.from_numpy(train_X), \n","                           torch.from_numpy(train_cat),\n","                           torch.from_numpy(train_y))\n","\n","test_data = TensorDataset(torch.from_numpy(test_features), \n","                          torch.from_numpy(test_X), \n","                          torch.from_numpy(test_cat),\n","                          torch.from_numpy(test_y))\n","\n","\n","# prepare data loaders (combine dataset and sampler)\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size,\n","    sampler = train_sampler, num_workers = num_workers, drop_last=True)\n","valid_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, \n","    sampler = valid_sampler, num_workers = num_workers, drop_last=True)\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size, \n","    num_workers = num_workers, shuffle = False, drop_last=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["596 149\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"i1VwYmtPtgGO","colab_type":"code","outputId":"19ab58ae-5bed-448a-867e-253019c42182","executionInfo":{"status":"ok","timestamp":1586981681621,"user_tz":300,"elapsed":596,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["# Create DataLoaders\n","# number of subprocesses to use for data loading\n","num_workers = 0\n","# how many samples per batch to load\n","batch_size = 4\n","# percentage of training set to use as validation\n","valid_size = 0.15\n","\n","\n","# obtain training indices that will be used for validation\n","num_train = len(train_X)\n","indices = list(range(num_train))\n","np.random.shuffle(indices)\n","split = int(np.floor(valid_size * num_train))\n","train_idx, valid_idx = indices[split:], indices[:split]\n","\n","print(len(train_idx), len(valid_idx))\n","\n","# define samplers for obtaining training and validation batches\n","train_sampler = SubsetRandomSampler(train_idx)\n","valid_sampler = SubsetRandomSampler(valid_idx)\n","\n","train_data = TensorDataset(torch.from_numpy(train_features[train_idx]), \n","                           torch.from_numpy(train_X[train_idx]), \n","                           torch.from_numpy(train_cat[train_idx]),\n","                           torch.from_numpy(train_y[train_idx]))\n","\n","valid_data = TensorDataset(torch.from_numpy(train_features[valid_idx]), \n","                           torch.from_numpy(train_X[valid_idx]), \n","                           torch.from_numpy(train_cat[valid_idx]),\n","                           torch.from_numpy(train_y[valid_idx]))\n","\n","test_data = TensorDataset(torch.from_numpy(test_features), \n","                          torch.from_numpy(test_X), \n","                          torch.from_numpy(test_cat),\n","                          torch.from_numpy(test_y))\n","\n","print(len(train_data), len(valid_data))\n","\n","# prepare data loaders (combine dataset and sampler)\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n","    num_workers=num_workers, shuffle=True, drop_last=True)\n","valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, \n","    num_workers=num_workers, shuffle=True, drop_last=True)\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n","    num_workers=num_workers, shuffle=False, drop_last=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["634 111\n","634 111\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JBf5h7sS3nVc","colab_type":"code","colab":{}},"source":["dataiter = iter(train_loader)\n","sample_features, sample_X, sample_cat, sample_y = dataiter.next()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M9CE7bQE36vD","colab_type":"code","outputId":"9315b42a-e342-4ef3-a6ca-49bb2556ecb6","executionInfo":{"status":"ok","timestamp":1586904168421,"user_tz":300,"elapsed":381,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":90}},"source":["print(sample_features.shape)\n","print(sample_X.shape)\n","print(sample_cat.shape)\n","print(sample_y.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["torch.Size([16, 14, 5])\n","torch.Size([16, 3])\n","torch.Size([16, 2])\n","torch.Size([16])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uwzut-CseGO3","colab_type":"code","outputId":"e820e79c-42df-4c40-9bde-72c557861fdf","executionInfo":{"status":"ok","timestamp":1587005299365,"user_tz":300,"elapsed":390,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# First checking if GPU is available\n","train_on_gpu = torch.cuda.is_available()\n","\n","if(train_on_gpu):\n","    print('Training on GPU.')\n","else:\n","    print('No GPU available, training on CPU.')\n","\n","#train_on_gpu = False"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Training on GPU.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fKaB5Hx1377j","colab_type":"code","colab":{}},"source":["class MosquitoLSTM(nn.Module):\n","    def __init__(self, num_numerical_columns, hidden_dim, n_layers):\n","        super(MosquitoLSTM, self).__init__()\n","\n","        self.n_layers = n_layers\n","        self.hidden_dim = hidden_dim\n","\n","        '''\n","        embed_size = 0\n","        for embed in embedding_sizes:\n","            embed_size += embed[1]\n","\n","        self.all_embeddings = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in embedding_sizes])\n","        '''\n","\n","        # For LSTM\n","        self.lstm = nn.LSTM(num_numerical_columns, hidden_dim, n_layers,\n","                            dropout=0.3, batch_first=True)\n","        \n","        # dropout layer\n","        self.dropout = nn.Dropout(0.3)\n","\n","        self.dense = nn.Sequential(\n","            \n","            nn.Linear(hidden_dim, 1)\n","        )\n","        self.ReLU = nn.ReLU()\n","\n","    def forward(self, X_temp, hidden):\n","        lstm_out, hidden = self.lstm(X_temp, hidden)\n","    \n","        # stack up lstm outputs\n","        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n","        print(lstm_out.shape)\n","        # dropout and fully-connected layer\n","        out = self.dense(lstm_out)\n","        # sigmoid function\n","        out = self.ReLU(out)\n","        \n","        print(out.shape)\n","        # reshape to be batch_size first\n","        out = out.view(batch_size, -1)\n","        print(out.shape)\n","        out = out[:, -1]\n","        print(out.shape)\n","\n","\n","\n","        return out, hidden\n","\n","    def init_hidden(self, batch_size):\n","        ''' Initializes hidden state '''\n","        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n","        # initialized to zero, for hidden state and cell state of LSTM\n","        weight = next(self.parameters()).data\n","        \n","        if (train_on_gpu):\n","            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n","                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n","        else:\n","            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n","                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n","        \n","        return hidden\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MO7LlGaj7oIe","colab_type":"code","colab":{}},"source":["class MosquitoRedux(nn.Module):\n","    def __init__(self, \n","                 lstm_columns, lstm_hidden_dim, lstm_n_layers,\n","                 num_columns, num_hidden_dim,\n","                 embedding_sizes, cat_hidden_dim):\n","        super(MosquitoRedux, self).__init__()\n","\n","        self.lstm_n_layers = lstm_n_layers\n","        self.lstm_hidden_dim = lstm_hidden_dim\n","\n","        self.num_hidden_dim = num_hidden_dim\n","        self.cat_hidden_dim = cat_hidden_dim\n","\n","        embed_size = 0\n","        for embed in embedding_sizes:\n","            embed_size += embed[1]\n","\n","        self.all_embeddings = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in embedding_sizes])\n","\n","        # For LSTM\n","        self.lstm = nn.LSTM(lstm_columns, lstm_hidden_dim, lstm_n_layers,\n","                            dropout=0.3, batch_first=True)\n","        \n","        # LSTM dense layer\n","        self.lstm_dense = nn.Sequential(\n","            nn.BatchNorm1d(lstm_hidden_dim),\n","            nn.Dropout(0.3),\n","            nn.Linear(lstm_hidden_dim, 1),\n","            nn.ReLU()\n","        )\n","        \n","        self.num_dense = nn.Sequential(\n","            nn.Dropout(0.3),\n","            nn.Linear(num_columns, num_hidden_dim),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm1d(num_hidden_dim),\n","            nn.Dropout(0.3),\n","            nn.Linear(num_hidden_dim, num_hidden_dim),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm1d(num_hidden_dim),\n","            nn.Dropout(0.3),\n","            nn.Linear(num_hidden_dim, num_hidden_dim),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm1d(num_hidden_dim),\n","            nn.Linear(num_hidden_dim, 1),\n","            nn.ReLU()\n","        )\n","\n","        self.cat_dense = nn.Sequential(\n","            nn.Dropout(0.3),\n","            nn.Linear(embed_size, cat_hidden_dim),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm1d(cat_hidden_dim),\n","            nn.Dropout(0.3),\n","            nn.Linear(cat_hidden_dim, cat_hidden_dim),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm1d(cat_hidden_dim),\n","            nn.Dropout(0.3),\n","            nn.Linear(cat_hidden_dim, cat_hidden_dim),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm1d(cat_hidden_dim),\n","            nn.Linear(cat_hidden_dim, 1),\n","            nn.ReLU()\n","        )\n","\n","    def forward(self, X_temp, hidden, X_num, X_cat):\n","        # Forward lstm\n","        lstm_out, hidden = self.lstm(X_temp, hidden)\n","        # stack up lstm outputs\n","        lstm_out = lstm_out.contiguous().view(-1, self.lstm_hidden_dim)\n","        # dropout and fully-connected layer\n","        lstm_out = self.lstm_dense(lstm_out)\n","      \n","        # reshape to be batch_size first\n","        lstm_out = lstm_out.view(batch_size, -1)\n","        lstm_out = lstm_out[:, -1]\n","        lstm_out = lstm_out.view(batch_size, -1)\n","\n","        # Forward num\n","        num_out = self.num_dense(X_num)\n","\n","        # Forward cat\n","        embeddings = []\n","        for i, e in enumerate(self.all_embeddings):\n","            embed = e(X_cat[:, i])\n","            embeddings.append(embed)\n","\n","        cat_out = torch.cat(embeddings, 1)\n","        cat_out = self.cat_dense(cat_out)\n","\n","        return lstm_out, hidden, num_out, cat_out\n","\n","    def init_hidden(self, batch_size):\n","        ''' Initializes hidden state '''\n","        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n","        # initialized to zero, for hidden state and cell state of LSTM\n","        weight = next(self.parameters()).data\n","        \n","        if (train_on_gpu):\n","            hidden = (weight.new(self.lstm_n_layers, batch_size, self.lstm_hidden_dim).zero_().cuda(),\n","                  weight.new(self.lstm_n_layers, batch_size, self.lstm_hidden_dim).zero_().cuda())\n","        else:\n","            hidden = (weight.new(self.lstm_n_layers, batch_size, self.lstm_hidden_dim).zero_(),\n","                      weight.new(self.lstm_n_layers, batch_size, self.lstm_hidden_dim).zero_())\n","        \n","        return hidden"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MjjvR7ARfbN5","colab_type":"code","outputId":"a5b56b1b-30e6-4da6-c2f7-e72d484b8f19","executionInfo":{"status":"ok","timestamp":1586933793908,"user_tz":300,"elapsed":813,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Instantiate the model w/ hyperparams\n","a_h = 3\n","\n","lstm_hidden_dim = np.int(len(train_idx) / (a_h * len(numerical_columns) + 1))\n","print(lstm_hidden_dim)\n","lstm_n_layers = 2\n","\n","num_hidden_dim = 15\n","cat_hidden_dim = 15\n","\n","#net = MosquitoLSTM(len(numerical_columns), hidden_dim, n_layers).double()\n","net = MosquitoRedux(\n","    len(numerical_columns), lstm_hidden_dim, lstm_n_layers,\n","    len(non_num_columns), num_hidden_dim,\n","    embedding_sizes, cat_hidden_dim\n",").double()\n","\n","weight_lstm = 1.0\n","weight_num = 2.0\n","weight_cat = 1.0"],"execution_count":0,"outputs":[{"output_type":"stream","text":["21\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"caOIYuZ5foOw","colab_type":"code","colab":{}},"source":["lr = 0.001\n","\n","criterion = nn.MSELoss()\n","#optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n","optimizer = torch.optim.RMSprop(net.parameters(), lr=lr)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jzkXPhqufsic","colab_type":"code","outputId":"efaf72ba-3e55-48c5-9b5d-9948f9ed06ef","executionInfo":{"status":"ok","timestamp":1586923700434,"user_tz":300,"elapsed":168759,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# training params\n","\n","epochs = 400 # 3-4 is approx where I noticed the validation loss stop decreasing\n","\n","counter = 0\n","print_every = 10\n","clip = 10 # gradient clipping\n","\n","# move model to GPU, if available\n","if(train_on_gpu):\n","    net.cuda()\n","\n","net.train()\n","# train for some number of epochs\n","valid_loss_min = np.Inf\n","\n","for epoch in range(epochs):\n","    # initialize hidden state\n","    h = net.init_hidden(batch_size)\n","\n","    train_loss = 0.0\n","    valid_loss = 0.0\n","\n","    net.train()\n","\n","    # batch loop\n","    for X_temp, X_num, X_cat, labels in train_loader:\n","        counter += 1\n","\n","        if(train_on_gpu):\n","            X_temp, X_num, X_cat, labels = X_temp.cuda(), X_num.cuda(), X_cat.type(torch.LongTensor).cuda(), labels.cuda()\n","\n","        # Creating new variables for the hidden state, otherwise\n","        # we'd backprop through the entire training history\n","        h = tuple([each.data for each in h])\n","        # zero accumulated gradients\n","        net.zero_grad()\n","\n","        # get the output from the model\n","        lstm_output, h, num_output, cat_output = net(X_temp, h, X_num, X_cat)\n","\n","        output = (weight_lstm*lstm_output + weight_num*num_output + weight_cat*cat_output) \n","\n","        # calculate the loss and perform backprop\n","        loss = criterion(output.squeeze(), labels.double())\n","        loss.backward()\n","        train_loss += loss.item()\n","        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","        nn.utils.clip_grad_norm_(net.parameters(), clip)\n","        optimizer.step()\n","\n","    net.eval()\n","\n","    val_h = net.init_hidden(batch_size)\n","\n","    for X_temp, X_num, X_cat, labels in valid_loader:\n","        \n","        \n","        if(train_on_gpu):\n","            X_temp, X_num, X_cat, labels = X_temp.cuda(), X_num.cuda(), X_cat.type(torch.LongTensor).cuda(), labels.cuda()\n","\n","        # Creating new variables for the hidden state, otherwise\n","        # we'd backprop through the entire training history\n","        val_h = tuple([each.data for each in val_h])\n","\n","        # get the output from the model\n","        lstm_output, h, num_output, cat_output = net(X_temp, h, X_num, X_cat)\n","\n","        # calculate the loss and perform backprop\n","        output = (weight_lstm*lstm_output + weight_num*num_output + weight_cat*cat_output) \n","        loss = criterion(output.squeeze(), labels.double())\n","        valid_loss += loss.item()\n","\n","    # save model if validation loss has decreased\n","    if valid_loss <= valid_loss_min:\n","        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","        valid_loss_min,\n","        valid_loss))\n","        torch.save(net.state_dict(), 'lstm_test.pt')\n","        valid_loss_min = valid_loss\n","        \n","    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n","            epoch, train_loss, valid_loss))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Validation loss decreased (inf --> 1063.810599).  Saving model ...\n","Epoch: 0 \tTraining Loss: 4902.986458 \tValidation Loss: 1063.810599\n","Validation loss decreased (1063.810599 --> 1010.740524).  Saving model ...\n","Epoch: 1 \tTraining Loss: 4528.126603 \tValidation Loss: 1010.740524\n","Epoch: 2 \tTraining Loss: 4321.250742 \tValidation Loss: 1019.153390\n","Validation loss decreased (1010.740524 --> 790.559787).  Saving model ...\n","Epoch: 3 \tTraining Loss: 4298.738146 \tValidation Loss: 790.559787\n","Validation loss decreased (790.559787 --> 788.113032).  Saving model ...\n","Epoch: 4 \tTraining Loss: 4151.630487 \tValidation Loss: 788.113032\n","Epoch: 5 \tTraining Loss: 4001.181914 \tValidation Loss: 942.267813\n","Epoch: 6 \tTraining Loss: 4183.231023 \tValidation Loss: 953.540055\n","Epoch: 7 \tTraining Loss: 4021.229664 \tValidation Loss: 910.900171\n","Epoch: 8 \tTraining Loss: 4069.782589 \tValidation Loss: 942.354421\n","Epoch: 9 \tTraining Loss: 4039.964342 \tValidation Loss: 968.322827\n","Epoch: 10 \tTraining Loss: 3960.657928 \tValidation Loss: 859.746226\n","Epoch: 11 \tTraining Loss: 3992.804889 \tValidation Loss: 962.385893\n","Epoch: 12 \tTraining Loss: 4073.931558 \tValidation Loss: 916.914474\n","Epoch: 13 \tTraining Loss: 4020.756905 \tValidation Loss: 945.938688\n","Epoch: 14 \tTraining Loss: 4063.930741 \tValidation Loss: 976.674579\n","Epoch: 15 \tTraining Loss: 4040.273404 \tValidation Loss: 892.743500\n","Epoch: 16 \tTraining Loss: 3139.971909 \tValidation Loss: 954.365069\n","Epoch: 17 \tTraining Loss: 4060.167099 \tValidation Loss: 953.029332\n","Epoch: 18 \tTraining Loss: 4059.023678 \tValidation Loss: 915.728428\n","Epoch: 19 \tTraining Loss: 3963.972188 \tValidation Loss: 956.412105\n","Epoch: 20 \tTraining Loss: 3917.386838 \tValidation Loss: 892.234124\n","Epoch: 21 \tTraining Loss: 3974.464383 \tValidation Loss: 939.849364\n","Epoch: 22 \tTraining Loss: 3922.569558 \tValidation Loss: 951.745127\n","Epoch: 23 \tTraining Loss: 3838.124238 \tValidation Loss: 907.878958\n","Epoch: 24 \tTraining Loss: 3973.836096 \tValidation Loss: 928.306695\n","Epoch: 25 \tTraining Loss: 3925.872197 \tValidation Loss: 1010.771514\n","Epoch: 26 \tTraining Loss: 4072.696164 \tValidation Loss: 966.208509\n","Epoch: 27 \tTraining Loss: 3929.462006 \tValidation Loss: 936.945429\n","Epoch: 28 \tTraining Loss: 3881.088143 \tValidation Loss: 935.232045\n","Epoch: 29 \tTraining Loss: 3815.092979 \tValidation Loss: 933.868704\n","Validation loss decreased (788.113032 --> 778.794350).  Saving model ...\n","Epoch: 30 \tTraining Loss: 3926.549424 \tValidation Loss: 778.794350\n","Epoch: 31 \tTraining Loss: 3892.851435 \tValidation Loss: 911.501090\n","Epoch: 32 \tTraining Loss: 3829.624486 \tValidation Loss: 936.745191\n","Epoch: 33 \tTraining Loss: 3779.653069 \tValidation Loss: 931.099982\n","Epoch: 34 \tTraining Loss: 3516.694271 \tValidation Loss: 930.604605\n","Epoch: 35 \tTraining Loss: 3835.122952 \tValidation Loss: 962.641713\n","Epoch: 36 \tTraining Loss: 3825.273889 \tValidation Loss: 915.044680\n","Epoch: 37 \tTraining Loss: 3844.533442 \tValidation Loss: 922.052588\n","Epoch: 38 \tTraining Loss: 3808.159224 \tValidation Loss: 897.883912\n","Epoch: 39 \tTraining Loss: 3793.423553 \tValidation Loss: 924.209021\n","Epoch: 40 \tTraining Loss: 3769.365939 \tValidation Loss: 816.985080\n","Epoch: 41 \tTraining Loss: 3787.416049 \tValidation Loss: 887.653672\n","Epoch: 42 \tTraining Loss: 3850.656385 \tValidation Loss: 920.602154\n","Epoch: 43 \tTraining Loss: 3738.353130 \tValidation Loss: 896.850612\n","Validation loss decreased (778.794350 --> 591.720069).  Saving model ...\n","Epoch: 44 \tTraining Loss: 3823.879824 \tValidation Loss: 591.720069\n","Epoch: 45 \tTraining Loss: 3697.565320 \tValidation Loss: 941.674602\n","Epoch: 46 \tTraining Loss: 3837.695961 \tValidation Loss: 924.278358\n","Epoch: 47 \tTraining Loss: 3778.464097 \tValidation Loss: 896.809567\n","Epoch: 48 \tTraining Loss: 3555.551868 \tValidation Loss: 899.409208\n","Epoch: 49 \tTraining Loss: 3806.429891 \tValidation Loss: 710.097916\n","Epoch: 50 \tTraining Loss: 3845.138778 \tValidation Loss: 886.939085\n","Epoch: 51 \tTraining Loss: 3713.275382 \tValidation Loss: 906.733160\n","Epoch: 52 \tTraining Loss: 3830.486929 \tValidation Loss: 801.166579\n","Epoch: 53 \tTraining Loss: 3789.678678 \tValidation Loss: 910.083177\n","Epoch: 54 \tTraining Loss: 3805.242171 \tValidation Loss: 904.318094\n","Epoch: 55 \tTraining Loss: 3712.499105 \tValidation Loss: 939.002967\n","Epoch: 56 \tTraining Loss: 3706.938634 \tValidation Loss: 822.772953\n","Epoch: 57 \tTraining Loss: 3799.861291 \tValidation Loss: 877.054421\n","Epoch: 58 \tTraining Loss: 3727.523757 \tValidation Loss: 855.292781\n","Epoch: 59 \tTraining Loss: 3637.569505 \tValidation Loss: 739.856807\n","Epoch: 60 \tTraining Loss: 3746.457208 \tValidation Loss: 894.757579\n","Epoch: 61 \tTraining Loss: 3761.585555 \tValidation Loss: 830.044012\n","Epoch: 62 \tTraining Loss: 3706.307010 \tValidation Loss: 907.800647\n","Epoch: 63 \tTraining Loss: 3728.783246 \tValidation Loss: 921.097553\n","Epoch: 64 \tTraining Loss: 3666.874648 \tValidation Loss: 931.827210\n","Epoch: 65 \tTraining Loss: 3773.551674 \tValidation Loss: 909.992542\n","Epoch: 66 \tTraining Loss: 3702.316191 \tValidation Loss: 958.139127\n","Epoch: 67 \tTraining Loss: 3722.387655 \tValidation Loss: 959.816195\n","Epoch: 68 \tTraining Loss: 3672.287460 \tValidation Loss: 923.672115\n","Epoch: 69 \tTraining Loss: 3701.387339 \tValidation Loss: 912.007884\n","Epoch: 70 \tTraining Loss: 3638.101025 \tValidation Loss: 910.381558\n","Epoch: 71 \tTraining Loss: 3709.616767 \tValidation Loss: 919.665182\n","Epoch: 72 \tTraining Loss: 3649.086501 \tValidation Loss: 919.311700\n","Epoch: 73 \tTraining Loss: 3653.564676 \tValidation Loss: 925.249300\n","Epoch: 74 \tTraining Loss: 3677.197416 \tValidation Loss: 804.082561\n","Epoch: 75 \tTraining Loss: 3618.983928 \tValidation Loss: 924.193861\n","Epoch: 76 \tTraining Loss: 3699.573488 \tValidation Loss: 924.935913\n","Epoch: 77 \tTraining Loss: 3642.728597 \tValidation Loss: 921.201771\n","Epoch: 78 \tTraining Loss: 3652.893454 \tValidation Loss: 902.159388\n","Epoch: 79 \tTraining Loss: 3667.545241 \tValidation Loss: 903.247526\n","Epoch: 80 \tTraining Loss: 3691.709887 \tValidation Loss: 921.190952\n","Epoch: 81 \tTraining Loss: 3748.633995 \tValidation Loss: 910.733033\n","Epoch: 82 \tTraining Loss: 3799.552397 \tValidation Loss: 922.346110\n","Epoch: 83 \tTraining Loss: 3670.691268 \tValidation Loss: 911.055476\n","Epoch: 84 \tTraining Loss: 3759.329491 \tValidation Loss: 831.196758\n","Epoch: 85 \tTraining Loss: 3715.411835 \tValidation Loss: 913.852533\n","Epoch: 86 \tTraining Loss: 3282.234081 \tValidation Loss: 927.197403\n","Epoch: 87 \tTraining Loss: 3660.966221 \tValidation Loss: 937.666703\n","Epoch: 88 \tTraining Loss: 3705.784096 \tValidation Loss: 843.550976\n","Epoch: 89 \tTraining Loss: 3625.987921 \tValidation Loss: 948.693986\n","Epoch: 90 \tTraining Loss: 3690.189483 \tValidation Loss: 912.741569\n","Epoch: 91 \tTraining Loss: 3633.442380 \tValidation Loss: 793.465999\n","Epoch: 92 \tTraining Loss: 3630.126814 \tValidation Loss: 929.162555\n","Epoch: 93 \tTraining Loss: 3645.351647 \tValidation Loss: 921.577764\n","Epoch: 94 \tTraining Loss: 3648.943836 \tValidation Loss: 894.533381\n","Epoch: 95 \tTraining Loss: 3693.441114 \tValidation Loss: 921.648583\n","Epoch: 96 \tTraining Loss: 3738.272412 \tValidation Loss: 920.143394\n","Epoch: 97 \tTraining Loss: 3658.535249 \tValidation Loss: 905.608811\n","Epoch: 98 \tTraining Loss: 3666.129216 \tValidation Loss: 905.274010\n","Epoch: 99 \tTraining Loss: 3441.777583 \tValidation Loss: 819.735320\n","Epoch: 100 \tTraining Loss: 3679.638255 \tValidation Loss: 851.920358\n","Epoch: 101 \tTraining Loss: 3705.595721 \tValidation Loss: 926.534809\n","Epoch: 102 \tTraining Loss: 3706.040590 \tValidation Loss: 916.556420\n","Epoch: 103 \tTraining Loss: 3640.544424 \tValidation Loss: 925.670826\n","Epoch: 104 \tTraining Loss: 3694.157592 \tValidation Loss: 902.804939\n","Epoch: 105 \tTraining Loss: 3671.265869 \tValidation Loss: 911.449376\n","Epoch: 106 \tTraining Loss: 3600.695581 \tValidation Loss: 917.226483\n","Epoch: 107 \tTraining Loss: 3597.463141 \tValidation Loss: 777.557816\n","Epoch: 108 \tTraining Loss: 3564.609682 \tValidation Loss: 913.159427\n","Epoch: 109 \tTraining Loss: 3536.765363 \tValidation Loss: 908.219703\n","Epoch: 110 \tTraining Loss: 3574.456269 \tValidation Loss: 927.243309\n","Epoch: 111 \tTraining Loss: 3695.334839 \tValidation Loss: 917.889419\n","Epoch: 112 \tTraining Loss: 3617.250422 \tValidation Loss: 906.111012\n","Epoch: 113 \tTraining Loss: 3602.880812 \tValidation Loss: 915.932495\n","Epoch: 114 \tTraining Loss: 3576.340550 \tValidation Loss: 926.671038\n","Epoch: 115 \tTraining Loss: 3595.682456 \tValidation Loss: 775.426441\n","Epoch: 116 \tTraining Loss: 3640.864624 \tValidation Loss: 909.583169\n","Epoch: 117 \tTraining Loss: 3621.551306 \tValidation Loss: 927.255655\n","Epoch: 118 \tTraining Loss: 3664.787632 \tValidation Loss: 914.830602\n","Epoch: 119 \tTraining Loss: 3637.295170 \tValidation Loss: 798.658374\n","Epoch: 120 \tTraining Loss: 3557.988209 \tValidation Loss: 925.550273\n","Epoch: 121 \tTraining Loss: 3600.511852 \tValidation Loss: 928.277747\n","Epoch: 122 \tTraining Loss: 3625.307896 \tValidation Loss: 929.659994\n","Epoch: 123 \tTraining Loss: 3534.787615 \tValidation Loss: 921.181980\n","Epoch: 124 \tTraining Loss: 3636.903017 \tValidation Loss: 892.886237\n","Epoch: 125 \tTraining Loss: 3649.309664 \tValidation Loss: 930.386726\n","Epoch: 126 \tTraining Loss: 3643.022649 \tValidation Loss: 931.689690\n","Epoch: 127 \tTraining Loss: 3672.332533 \tValidation Loss: 950.917113\n","Epoch: 128 \tTraining Loss: 3610.542003 \tValidation Loss: 915.306162\n","Epoch: 129 \tTraining Loss: 3744.210562 \tValidation Loss: 841.656105\n","Epoch: 130 \tTraining Loss: 3613.791214 \tValidation Loss: 921.087533\n","Epoch: 131 \tTraining Loss: 3545.076699 \tValidation Loss: 925.706388\n","Epoch: 132 \tTraining Loss: 3618.269981 \tValidation Loss: 920.080927\n","Epoch: 133 \tTraining Loss: 3633.871237 \tValidation Loss: 904.006784\n","Epoch: 134 \tTraining Loss: 3601.798511 \tValidation Loss: 922.647791\n","Epoch: 135 \tTraining Loss: 3617.329790 \tValidation Loss: 882.643336\n","Epoch: 136 \tTraining Loss: 3695.112376 \tValidation Loss: 699.886881\n","Epoch: 137 \tTraining Loss: 3656.851439 \tValidation Loss: 920.755421\n","Epoch: 138 \tTraining Loss: 3658.539342 \tValidation Loss: 930.604008\n","Epoch: 139 \tTraining Loss: 3529.643260 \tValidation Loss: 932.745322\n","Epoch: 140 \tTraining Loss: 3642.430436 \tValidation Loss: 866.465129\n","Epoch: 141 \tTraining Loss: 3613.374312 \tValidation Loss: 904.432699\n","Epoch: 142 \tTraining Loss: 3650.061819 \tValidation Loss: 784.380610\n","Epoch: 143 \tTraining Loss: 3633.042436 \tValidation Loss: 927.562125\n","Epoch: 144 \tTraining Loss: 3660.086430 \tValidation Loss: 922.936115\n","Epoch: 145 \tTraining Loss: 3605.468107 \tValidation Loss: 909.079174\n","Epoch: 146 \tTraining Loss: 3617.740311 \tValidation Loss: 891.483170\n","Epoch: 147 \tTraining Loss: 3615.915308 \tValidation Loss: 841.758215\n","Epoch: 148 \tTraining Loss: 3600.912477 \tValidation Loss: 935.860477\n","Epoch: 149 \tTraining Loss: 3599.621786 \tValidation Loss: 933.734369\n","Epoch: 150 \tTraining Loss: 3534.424035 \tValidation Loss: 939.470172\n","Epoch: 151 \tTraining Loss: 3561.862491 \tValidation Loss: 833.217541\n","Epoch: 152 \tTraining Loss: 3658.471355 \tValidation Loss: 849.866639\n","Epoch: 153 \tTraining Loss: 3557.228939 \tValidation Loss: 944.469956\n","Epoch: 154 \tTraining Loss: 3637.361514 \tValidation Loss: 924.508053\n","Epoch: 155 \tTraining Loss: 3653.466488 \tValidation Loss: 923.251413\n","Epoch: 156 \tTraining Loss: 3665.492621 \tValidation Loss: 918.796751\n","Epoch: 157 \tTraining Loss: 3564.617360 \tValidation Loss: 933.976653\n","Epoch: 158 \tTraining Loss: 3625.671366 \tValidation Loss: 903.856580\n","Epoch: 159 \tTraining Loss: 3680.497660 \tValidation Loss: 924.245049\n","Epoch: 160 \tTraining Loss: 3566.035442 \tValidation Loss: 930.347567\n","Epoch: 161 \tTraining Loss: 3559.542651 \tValidation Loss: 925.730617\n","Epoch: 162 \tTraining Loss: 3566.301508 \tValidation Loss: 897.633383\n","Epoch: 163 \tTraining Loss: 3579.783542 \tValidation Loss: 925.028039\n","Epoch: 164 \tTraining Loss: 3580.390539 \tValidation Loss: 890.190170\n","Epoch: 165 \tTraining Loss: 3576.304464 \tValidation Loss: 936.868676\n","Epoch: 166 \tTraining Loss: 3508.348380 \tValidation Loss: 934.628205\n","Epoch: 167 \tTraining Loss: 3597.737559 \tValidation Loss: 923.391940\n","Epoch: 168 \tTraining Loss: 3540.709296 \tValidation Loss: 914.745624\n","Epoch: 169 \tTraining Loss: 3541.578382 \tValidation Loss: 932.293748\n","Epoch: 170 \tTraining Loss: 3465.667943 \tValidation Loss: 928.255773\n","Epoch: 171 \tTraining Loss: 3614.827921 \tValidation Loss: 912.127889\n","Epoch: 172 \tTraining Loss: 3683.318276 \tValidation Loss: 930.414844\n","Epoch: 173 \tTraining Loss: 3572.880304 \tValidation Loss: 939.581809\n","Epoch: 174 \tTraining Loss: 3592.019166 \tValidation Loss: 930.452961\n","Epoch: 175 \tTraining Loss: 3624.637177 \tValidation Loss: 859.312566\n","Epoch: 176 \tTraining Loss: 3642.327468 \tValidation Loss: 929.095772\n","Epoch: 177 \tTraining Loss: 3559.140243 \tValidation Loss: 740.271827\n","Epoch: 178 \tTraining Loss: 3622.336794 \tValidation Loss: 865.574912\n","Epoch: 179 \tTraining Loss: 3561.864278 \tValidation Loss: 921.433308\n","Epoch: 180 \tTraining Loss: 3564.221080 \tValidation Loss: 918.825146\n","Epoch: 181 \tTraining Loss: 3560.351687 \tValidation Loss: 730.900245\n","Epoch: 182 \tTraining Loss: 3593.602856 \tValidation Loss: 933.365504\n","Epoch: 183 \tTraining Loss: 3574.040950 \tValidation Loss: 944.175354\n","Epoch: 184 \tTraining Loss: 3592.038857 \tValidation Loss: 929.715188\n","Epoch: 185 \tTraining Loss: 3627.410544 \tValidation Loss: 940.450092\n","Epoch: 186 \tTraining Loss: 3502.958156 \tValidation Loss: 892.954046\n","Epoch: 187 \tTraining Loss: 3252.482286 \tValidation Loss: 884.178012\n","Epoch: 188 \tTraining Loss: 3556.089174 \tValidation Loss: 887.475454\n","Epoch: 189 \tTraining Loss: 3508.234551 \tValidation Loss: 936.333267\n","Epoch: 190 \tTraining Loss: 3532.562039 \tValidation Loss: 903.107510\n","Epoch: 191 \tTraining Loss: 3550.708963 \tValidation Loss: 902.419963\n","Epoch: 192 \tTraining Loss: 3584.889843 \tValidation Loss: 821.747359\n","Epoch: 193 \tTraining Loss: 3498.720190 \tValidation Loss: 930.884865\n","Epoch: 194 \tTraining Loss: 3649.535693 \tValidation Loss: 931.419733\n","Epoch: 195 \tTraining Loss: 3527.073101 \tValidation Loss: 885.525025\n","Epoch: 196 \tTraining Loss: 3572.973989 \tValidation Loss: 939.524022\n","Epoch: 197 \tTraining Loss: 3594.204518 \tValidation Loss: 906.663603\n","Epoch: 198 \tTraining Loss: 3506.296814 \tValidation Loss: 920.428877\n","Epoch: 199 \tTraining Loss: 3496.952554 \tValidation Loss: 930.290084\n","Epoch: 200 \tTraining Loss: 3609.267553 \tValidation Loss: 926.738022\n","Epoch: 201 \tTraining Loss: 3482.888938 \tValidation Loss: 940.611845\n","Epoch: 202 \tTraining Loss: 3562.759937 \tValidation Loss: 933.040490\n","Epoch: 203 \tTraining Loss: 3547.635124 \tValidation Loss: 855.807829\n","Epoch: 204 \tTraining Loss: 3517.822646 \tValidation Loss: 911.917468\n","Epoch: 205 \tTraining Loss: 3605.878745 \tValidation Loss: 909.614995\n","Epoch: 206 \tTraining Loss: 3546.537768 \tValidation Loss: 929.701839\n","Epoch: 207 \tTraining Loss: 3610.378766 \tValidation Loss: 791.962379\n","Epoch: 208 \tTraining Loss: 3622.022953 \tValidation Loss: 898.405739\n","Epoch: 209 \tTraining Loss: 3586.801438 \tValidation Loss: 918.236922\n","Epoch: 210 \tTraining Loss: 3588.997553 \tValidation Loss: 928.797434\n","Epoch: 211 \tTraining Loss: 3627.758803 \tValidation Loss: 920.746462\n","Epoch: 212 \tTraining Loss: 3522.937199 \tValidation Loss: 933.412466\n","Epoch: 213 \tTraining Loss: 3636.579459 \tValidation Loss: 911.787753\n","Epoch: 214 \tTraining Loss: 3515.274791 \tValidation Loss: 908.500341\n","Epoch: 215 \tTraining Loss: 3594.251850 \tValidation Loss: 916.395140\n","Epoch: 216 \tTraining Loss: 3576.062302 \tValidation Loss: 823.676336\n","Epoch: 217 \tTraining Loss: 3377.704178 \tValidation Loss: 902.183786\n","Epoch: 218 \tTraining Loss: 3522.446070 \tValidation Loss: 915.248374\n","Epoch: 219 \tTraining Loss: 2834.752525 \tValidation Loss: 933.683235\n","Epoch: 220 \tTraining Loss: 3530.744246 \tValidation Loss: 909.363780\n","Epoch: 221 \tTraining Loss: 3536.541333 \tValidation Loss: 863.491244\n","Epoch: 222 \tTraining Loss: 3587.639797 \tValidation Loss: 924.324057\n","Epoch: 223 \tTraining Loss: 3518.447180 \tValidation Loss: 830.877162\n","Epoch: 224 \tTraining Loss: 3529.526092 \tValidation Loss: 937.569886\n","Epoch: 225 \tTraining Loss: 3514.612096 \tValidation Loss: 922.162652\n","Epoch: 226 \tTraining Loss: 3534.272257 \tValidation Loss: 926.776176\n","Epoch: 227 \tTraining Loss: 3565.739612 \tValidation Loss: 917.345682\n","Epoch: 228 \tTraining Loss: 3558.906870 \tValidation Loss: 921.325753\n","Epoch: 229 \tTraining Loss: 3551.964952 \tValidation Loss: 926.156432\n","Epoch: 230 \tTraining Loss: 3562.573392 \tValidation Loss: 938.158328\n","Epoch: 231 \tTraining Loss: 3553.140888 \tValidation Loss: 934.311676\n","Epoch: 232 \tTraining Loss: 3477.513671 \tValidation Loss: 927.772301\n","Epoch: 233 \tTraining Loss: 3542.012683 \tValidation Loss: 907.787771\n","Epoch: 234 \tTraining Loss: 3531.008364 \tValidation Loss: 924.130086\n","Epoch: 235 \tTraining Loss: 3678.930397 \tValidation Loss: 920.835540\n","Epoch: 236 \tTraining Loss: 3286.235081 \tValidation Loss: 928.862446\n","Epoch: 237 \tTraining Loss: 3501.167850 \tValidation Loss: 841.492812\n","Epoch: 238 \tTraining Loss: 3545.530477 \tValidation Loss: 904.198208\n","Epoch: 239 \tTraining Loss: 3542.518191 \tValidation Loss: 938.501626\n","Epoch: 240 \tTraining Loss: 3556.692622 \tValidation Loss: 927.031418\n","Epoch: 241 \tTraining Loss: 2740.846567 \tValidation Loss: 829.684371\n","Epoch: 242 \tTraining Loss: 3553.666767 \tValidation Loss: 939.783813\n","Epoch: 243 \tTraining Loss: 3476.817890 \tValidation Loss: 932.856721\n","Epoch: 244 \tTraining Loss: 3518.572078 \tValidation Loss: 921.263461\n","Epoch: 245 \tTraining Loss: 3515.197388 \tValidation Loss: 750.961822\n","Epoch: 246 \tTraining Loss: 3570.095749 \tValidation Loss: 925.542868\n","Epoch: 247 \tTraining Loss: 3514.677663 \tValidation Loss: 870.150265\n","Epoch: 248 \tTraining Loss: 3619.256531 \tValidation Loss: 939.872545\n","Epoch: 249 \tTraining Loss: 3578.544484 \tValidation Loss: 934.387380\n","Epoch: 250 \tTraining Loss: 3515.034420 \tValidation Loss: 929.320966\n","Epoch: 251 \tTraining Loss: 3433.462959 \tValidation Loss: 740.507774\n","Epoch: 252 \tTraining Loss: 2747.110172 \tValidation Loss: 926.653432\n","Epoch: 253 \tTraining Loss: 3538.146007 \tValidation Loss: 918.906383\n","Epoch: 254 \tTraining Loss: 3555.055538 \tValidation Loss: 888.230886\n","Epoch: 255 \tTraining Loss: 3668.477126 \tValidation Loss: 923.010669\n","Epoch: 256 \tTraining Loss: 3541.869734 \tValidation Loss: 945.703774\n","Epoch: 257 \tTraining Loss: 3532.875641 \tValidation Loss: 939.784248\n","Epoch: 258 \tTraining Loss: 3472.040553 \tValidation Loss: 747.790288\n","Epoch: 259 \tTraining Loss: 3554.999422 \tValidation Loss: 930.836487\n","Epoch: 260 \tTraining Loss: 3613.652263 \tValidation Loss: 932.918034\n","Epoch: 261 \tTraining Loss: 3584.654187 \tValidation Loss: 922.068555\n","Epoch: 262 \tTraining Loss: 3575.788616 \tValidation Loss: 918.153573\n","Epoch: 263 \tTraining Loss: 3533.476848 \tValidation Loss: 914.379650\n","Epoch: 264 \tTraining Loss: 3544.640355 \tValidation Loss: 909.547566\n","Epoch: 265 \tTraining Loss: 3350.425888 \tValidation Loss: 866.630635\n","Epoch: 266 \tTraining Loss: 3588.750375 \tValidation Loss: 933.638776\n","Epoch: 267 \tTraining Loss: 3201.513763 \tValidation Loss: 897.476934\n","Epoch: 268 \tTraining Loss: 3551.588865 \tValidation Loss: 928.833422\n","Epoch: 269 \tTraining Loss: 3486.201001 \tValidation Loss: 741.001543\n","Epoch: 270 \tTraining Loss: 3590.045775 \tValidation Loss: 723.780760\n","Epoch: 271 \tTraining Loss: 3502.415014 \tValidation Loss: 905.486182\n","Epoch: 272 \tTraining Loss: 3430.421707 \tValidation Loss: 934.792705\n","Epoch: 273 \tTraining Loss: 3537.707358 \tValidation Loss: 913.049911\n","Epoch: 274 \tTraining Loss: 3537.698226 \tValidation Loss: 942.249296\n","Epoch: 275 \tTraining Loss: 3503.560276 \tValidation Loss: 942.318570\n","Epoch: 276 \tTraining Loss: 3523.651075 \tValidation Loss: 908.575048\n","Epoch: 277 \tTraining Loss: 3529.852860 \tValidation Loss: 829.860839\n","Epoch: 278 \tTraining Loss: 3490.733716 \tValidation Loss: 900.897260\n","Epoch: 279 \tTraining Loss: 3588.211141 \tValidation Loss: 914.338661\n","Epoch: 280 \tTraining Loss: 3538.920874 \tValidation Loss: 910.549541\n","Epoch: 281 \tTraining Loss: 3504.293069 \tValidation Loss: 777.971103\n","Epoch: 282 \tTraining Loss: 3523.391863 \tValidation Loss: 905.322845\n","Epoch: 283 \tTraining Loss: 3471.704860 \tValidation Loss: 917.096929\n","Epoch: 284 \tTraining Loss: 3595.753612 \tValidation Loss: 876.856372\n","Epoch: 285 \tTraining Loss: 3522.998770 \tValidation Loss: 817.716830\n","Epoch: 286 \tTraining Loss: 3490.091545 \tValidation Loss: 924.462879\n","Epoch: 287 \tTraining Loss: 3554.481975 \tValidation Loss: 919.131029\n","Epoch: 288 \tTraining Loss: 3493.958041 \tValidation Loss: 890.117692\n","Epoch: 289 \tTraining Loss: 3495.433080 \tValidation Loss: 910.050931\n","Epoch: 290 \tTraining Loss: 3544.212359 \tValidation Loss: 919.063558\n","Epoch: 291 \tTraining Loss: 3486.703439 \tValidation Loss: 809.806686\n","Epoch: 292 \tTraining Loss: 3520.248590 \tValidation Loss: 905.598703\n","Epoch: 293 \tTraining Loss: 3346.872143 \tValidation Loss: 906.315243\n","Epoch: 294 \tTraining Loss: 3447.168624 \tValidation Loss: 912.813842\n","Epoch: 295 \tTraining Loss: 3569.588119 \tValidation Loss: 816.960743\n","Epoch: 296 \tTraining Loss: 3459.574438 \tValidation Loss: 913.128430\n","Epoch: 297 \tTraining Loss: 3518.641665 \tValidation Loss: 918.355897\n","Epoch: 298 \tTraining Loss: 3537.335137 \tValidation Loss: 825.234842\n","Epoch: 299 \tTraining Loss: 3530.889179 \tValidation Loss: 924.485149\n","Epoch: 300 \tTraining Loss: 3564.688755 \tValidation Loss: 898.419040\n","Epoch: 301 \tTraining Loss: 3535.211194 \tValidation Loss: 923.120488\n","Epoch: 302 \tTraining Loss: 3504.374916 \tValidation Loss: 928.047094\n","Epoch: 303 \tTraining Loss: 3569.550665 \tValidation Loss: 819.967022\n","Epoch: 304 \tTraining Loss: 3545.622057 \tValidation Loss: 918.052864\n","Epoch: 305 \tTraining Loss: 3533.018181 \tValidation Loss: 833.914965\n","Epoch: 306 \tTraining Loss: 3558.082347 \tValidation Loss: 926.355819\n","Epoch: 307 \tTraining Loss: 3334.821604 \tValidation Loss: 923.851172\n","Epoch: 308 \tTraining Loss: 3498.325347 \tValidation Loss: 915.341330\n","Epoch: 309 \tTraining Loss: 3463.961515 \tValidation Loss: 921.301985\n","Epoch: 310 \tTraining Loss: 3498.165682 \tValidation Loss: 771.130929\n","Epoch: 311 \tTraining Loss: 3617.923402 \tValidation Loss: 955.411444\n","Epoch: 312 \tTraining Loss: 3496.519169 \tValidation Loss: 848.998602\n","Epoch: 313 \tTraining Loss: 3519.588391 \tValidation Loss: 858.932856\n","Epoch: 314 \tTraining Loss: 3501.547808 \tValidation Loss: 934.711627\n","Epoch: 315 \tTraining Loss: 3445.668074 \tValidation Loss: 934.173167\n","Epoch: 316 \tTraining Loss: 3486.893839 \tValidation Loss: 920.493497\n","Epoch: 317 \tTraining Loss: 3418.969370 \tValidation Loss: 903.518924\n","Epoch: 318 \tTraining Loss: 3523.646911 \tValidation Loss: 920.367432\n","Epoch: 319 \tTraining Loss: 3538.592512 \tValidation Loss: 934.101044\n","Epoch: 320 \tTraining Loss: 3448.212166 \tValidation Loss: 938.258820\n","Epoch: 321 \tTraining Loss: 3492.122049 \tValidation Loss: 906.767192\n","Epoch: 322 \tTraining Loss: 3557.818696 \tValidation Loss: 814.374997\n","Epoch: 323 \tTraining Loss: 3584.544465 \tValidation Loss: 919.780658\n","Epoch: 324 \tTraining Loss: 3512.137365 \tValidation Loss: 844.148900\n","Epoch: 325 \tTraining Loss: 3495.675602 \tValidation Loss: 841.232426\n","Epoch: 326 \tTraining Loss: 3577.145614 \tValidation Loss: 917.169824\n","Epoch: 327 \tTraining Loss: 3538.884040 \tValidation Loss: 918.113174\n","Epoch: 328 \tTraining Loss: 3483.751905 \tValidation Loss: 910.373114\n","Epoch: 329 \tTraining Loss: 3478.914015 \tValidation Loss: 806.017214\n","Epoch: 330 \tTraining Loss: 3422.263017 \tValidation Loss: 888.532506\n","Epoch: 331 \tTraining Loss: 3220.005841 \tValidation Loss: 929.866173\n","Epoch: 332 \tTraining Loss: 3494.197872 \tValidation Loss: 938.046882\n","Epoch: 333 \tTraining Loss: 3438.821524 \tValidation Loss: 929.140040\n","Epoch: 334 \tTraining Loss: 3574.169910 \tValidation Loss: 928.314885\n","Epoch: 335 \tTraining Loss: 3488.687022 \tValidation Loss: 876.733389\n","Epoch: 336 \tTraining Loss: 3509.607013 \tValidation Loss: 928.233190\n","Epoch: 337 \tTraining Loss: 3460.922671 \tValidation Loss: 921.168419\n","Epoch: 338 \tTraining Loss: 3559.480618 \tValidation Loss: 940.733136\n","Epoch: 339 \tTraining Loss: 3593.068153 \tValidation Loss: 939.881101\n","Epoch: 340 \tTraining Loss: 3459.179738 \tValidation Loss: 930.674709\n","Epoch: 341 \tTraining Loss: 3361.750096 \tValidation Loss: 923.521361\n","Epoch: 342 \tTraining Loss: 3483.828239 \tValidation Loss: 919.942749\n","Epoch: 343 \tTraining Loss: 3549.401489 \tValidation Loss: 718.280966\n","Epoch: 344 \tTraining Loss: 3500.541889 \tValidation Loss: 938.551370\n","Epoch: 345 \tTraining Loss: 3532.771786 \tValidation Loss: 941.187888\n","Epoch: 346 \tTraining Loss: 3325.356657 \tValidation Loss: 770.190774\n","Epoch: 347 \tTraining Loss: 3550.352535 \tValidation Loss: 894.227854\n","Epoch: 348 \tTraining Loss: 3553.895247 \tValidation Loss: 906.681354\n","Epoch: 349 \tTraining Loss: 3589.353726 \tValidation Loss: 911.820081\n","Epoch: 350 \tTraining Loss: 3542.324086 \tValidation Loss: 901.751745\n","Epoch: 351 \tTraining Loss: 3358.239558 \tValidation Loss: 910.155222\n","Epoch: 352 \tTraining Loss: 3509.928012 \tValidation Loss: 917.133789\n","Epoch: 353 \tTraining Loss: 3568.927917 \tValidation Loss: 907.523000\n","Epoch: 354 \tTraining Loss: 3501.294112 \tValidation Loss: 880.015038\n","Epoch: 355 \tTraining Loss: 3458.937976 \tValidation Loss: 920.821282\n","Epoch: 356 \tTraining Loss: 3553.756964 \tValidation Loss: 899.216732\n","Epoch: 357 \tTraining Loss: 3410.294760 \tValidation Loss: 914.248715\n","Epoch: 358 \tTraining Loss: 3431.141519 \tValidation Loss: 919.492042\n","Epoch: 359 \tTraining Loss: 3584.627949 \tValidation Loss: 941.714132\n","Epoch: 360 \tTraining Loss: 3231.339148 \tValidation Loss: 811.287557\n","Epoch: 361 \tTraining Loss: 3559.523538 \tValidation Loss: 920.717265\n","Epoch: 362 \tTraining Loss: 3473.241519 \tValidation Loss: 928.438809\n","Epoch: 363 \tTraining Loss: 3563.504891 \tValidation Loss: 919.146927\n","Epoch: 364 \tTraining Loss: 3533.274045 \tValidation Loss: 917.761629\n","Epoch: 365 \tTraining Loss: 3497.469975 \tValidation Loss: 773.244192\n","Epoch: 366 \tTraining Loss: 3561.932573 \tValidation Loss: 926.564127\n","Epoch: 367 \tTraining Loss: 3513.555005 \tValidation Loss: 917.128508\n","Epoch: 368 \tTraining Loss: 3515.954560 \tValidation Loss: 934.565924\n","Epoch: 369 \tTraining Loss: 3515.256230 \tValidation Loss: 907.526021\n","Epoch: 370 \tTraining Loss: 3478.500462 \tValidation Loss: 899.024908\n","Epoch: 371 \tTraining Loss: 3481.789714 \tValidation Loss: 739.918334\n","Epoch: 372 \tTraining Loss: 3451.255990 \tValidation Loss: 928.195087\n","Epoch: 373 \tTraining Loss: 3441.543987 \tValidation Loss: 930.693787\n","Epoch: 374 \tTraining Loss: 3421.679936 \tValidation Loss: 914.088260\n","Epoch: 375 \tTraining Loss: 3417.513519 \tValidation Loss: 935.774387\n","Epoch: 376 \tTraining Loss: 3570.929799 \tValidation Loss: 892.317568\n","Epoch: 377 \tTraining Loss: 3565.762984 \tValidation Loss: 940.310238\n","Epoch: 378 \tTraining Loss: 3547.892386 \tValidation Loss: 939.861884\n","Epoch: 379 \tTraining Loss: 3487.577420 \tValidation Loss: 927.196007\n","Epoch: 380 \tTraining Loss: 3589.891445 \tValidation Loss: 907.528062\n","Epoch: 381 \tTraining Loss: 3435.696842 \tValidation Loss: 822.765490\n","Epoch: 382 \tTraining Loss: 3537.284392 \tValidation Loss: 946.509277\n","Epoch: 383 \tTraining Loss: 3571.371333 \tValidation Loss: 921.371467\n","Epoch: 384 \tTraining Loss: 3568.422438 \tValidation Loss: 885.416856\n","Epoch: 385 \tTraining Loss: 3531.806174 \tValidation Loss: 910.453249\n","Epoch: 386 \tTraining Loss: 3558.343727 \tValidation Loss: 921.700055\n","Epoch: 387 \tTraining Loss: 3440.433331 \tValidation Loss: 882.471776\n","Epoch: 388 \tTraining Loss: 3497.936126 \tValidation Loss: 919.756853\n","Epoch: 389 \tTraining Loss: 3565.650736 \tValidation Loss: 920.192272\n","Epoch: 390 \tTraining Loss: 3472.358243 \tValidation Loss: 930.493050\n","Epoch: 391 \tTraining Loss: 3510.644250 \tValidation Loss: 908.892687\n","Epoch: 392 \tTraining Loss: 3535.662079 \tValidation Loss: 891.944155\n","Epoch: 393 \tTraining Loss: 3544.798003 \tValidation Loss: 924.106674\n","Epoch: 394 \tTraining Loss: 3607.565091 \tValidation Loss: 919.231493\n","Epoch: 395 \tTraining Loss: 3499.620817 \tValidation Loss: 910.985127\n","Epoch: 396 \tTraining Loss: 3518.209177 \tValidation Loss: 893.146823\n","Epoch: 397 \tTraining Loss: 3442.841378 \tValidation Loss: 912.532562\n","Epoch: 398 \tTraining Loss: 3451.407067 \tValidation Loss: 917.585555\n","Epoch: 399 \tTraining Loss: 3526.182022 \tValidation Loss: 915.413056\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MPiYWv9UoFP_","colab_type":"code","outputId":"78b8212d-d385-4687-cd0e-34973134cfd3","executionInfo":{"status":"ok","timestamp":1586923766564,"user_tz":300,"elapsed":622,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["net.load_state_dict(torch.load('lstm_test.pt'))"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":381}]},{"cell_type":"code","metadata":{"id":"LOYJ_iJ0gCED","colab_type":"code","outputId":"74a43145-6f7b-4ab0-b710-9eb8993ed03e","executionInfo":{"status":"ok","timestamp":1586923766881,"user_tz":300,"elapsed":592,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":199}},"source":["\n","\n","# track test loss\n","test_loss = 0.0\n","mean_abs = 0.0\n","\n","net.eval()\n","# iterate over test data\n","#bin_op.binarization()\n","test_h = net.init_hidden(batch_size)\n","\n","for batch_idx, (X_temp, X_num, X_cat, labels) in enumerate(test_loader):\n","    if(train_on_gpu):\n","        X_temp, X_num, X_cat, labels = X_temp.cuda(), X_num.cuda(), X_cat.type(torch.LongTensor).cuda(), labels.cuda()\n","    \n","    test_h = tuple([each.data for each in test_h])\n","\n","    # get the output from the model\n","    lstm_output, h, num_output, cat_output = net(X_temp, h, X_num, X_cat)\n","\n","    output = (weight_lstm*lstm_output + weight_num*num_output + weight_cat*cat_output) \n","    # / (weight_lstm + weight_num + weight_cat)\n","\n","    # calculate the loss and perform backprop\n","    loss = criterion(output.squeeze(), labels.double())\n","\n","    out_np = output.detach().squeeze().cpu().numpy().astype(int).T\n","    target_np = labels.detach().cpu().numpy().astype(int).T\n","\n","    print([(out, tar) for out, tar in zip(out_np, target_np)])\n","\n","    mean_abs += mean_absolute_error(out_np, target_np) * batch_size\n","        \n","    test_loss += loss.item() * batch_size\n","\n","# calculate average losses\n","test_loss = np.sqrt(test_loss / len(test_loader.dataset))\n","mean_abs /= len(test_loader.dataset)\n","print('Test Loss: {:.6f}\\n'.format(test_loss))\n","print('Mean Abs Loss: {:.6f}\\n'.format(mean_abs))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[(4, 3), (4, 3), (3, 2), (6, 2), (5, 2), (5, 3), (6, 3), (2, 3), (2, 2), (2, 6), (2, 4), (2, 4), (2, 5), (2, 3), (2, 2), (5, 16)]\n","[(4, 4), (2, 10), (2, 5), (2, 3), (2, 8), (9, 2), (6, 9), (2, 66), (7, 3), (5, 9), (5, 11), (5, 8), (4, 25), (4, 4), (8, 3), (7, 2)]\n","[(7, 3), (6, 3), (7, 6), (8, 3), (8, 8), (8, 8), (7, 6), (7, 9), (8, 12), (8, 18), (7, 24), (7, 19), (7, 3), (7, 15), (8, 12), (8, 12)]\n","[(7, 17), (7, 9), (7, 3), (7, 12), (7, 33), (7, 11), (7, 6), (7, 2), (7, 7), (7, 5), (7, 2), (8, 11), (8, 23), (8, 4), (7, 7), (7, 5)]\n","[(6, 6), (7, 10), (7, 12), (6, 5), (7, 10), (5, 2), (7, 3), (7, 32), (7, 13), (5, 27), (8, 9), (8, 12), (5, 16), (8, 6), (5, 3), (7, 7)]\n","[(5, 2), (5, 6), (7, 10), (7, 6), (7, 2), (4, 3), (5, 5), (7, 2), (7, 5), (4, 17), (7, 18), (7, 3), (4, 8), (7, 2), (7, 15), (4, 5)]\n","Test Loss: 8.987268\n","\n","Mean Abs Loss: 4.809524\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fdmJvO0ZNXDu","colab_type":"text"},"source":["## Mixed Model"]},{"cell_type":"code","metadata":{"id":"iALvtBlwNZc0","colab_type":"code","colab":{}},"source":["class MosquitoMixed(nn.Module):\n","    def __init__(self, \n","                 lstm_columns, lstm_hidden_dim, lstm_n_layers, lstm_output,\n","                 num_columns, num_hidden_dim,\n","                 embedding_sizes, cat_hidden_dim,\n","                 combined_dim):\n","        super(MosquitoMixed, self).__init__()\n","\n","        self.lstm_n_layers = lstm_n_layers\n","        self.lstm_hidden_dim = lstm_hidden_dim\n","\n","        self.num_hidden_dim = num_hidden_dim\n","        self.cat_hidden_dim = cat_hidden_dim\n","\n","        embed_size = 0\n","        for embed in embedding_sizes:\n","            embed_size += embed[1]\n","\n","        self.all_embeddings = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in embedding_sizes])\n","\n","        # For LSTM\n","        self.lstm = nn.LSTM(lstm_columns, lstm_hidden_dim, lstm_n_layers,\n","                            dropout=0.3, batch_first=True)\n","        \n","        # LSTM dense layer\n","        self.lstm_dense = nn.Sequential(\n","            nn.BatchNorm1d(lstm_hidden_dim),\n","            nn.Dropout(0.3),\n","            nn.Linear(lstm_hidden_dim, lstm_output),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm1d(lstm_output)\n","        )\n","        \n","        self.num_dense = nn.Sequential(\n","            nn.Dropout(0.3),\n","            nn.Linear(num_columns, num_hidden_dim),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm1d(num_hidden_dim),\n","            nn.Dropout(0.3),\n","            nn.Linear(num_hidden_dim, num_hidden_dim),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm1d(num_hidden_dim)\n","        )\n","\n","        self.cat_dense = nn.Sequential(\n","            nn.Dropout(0.3),\n","            nn.Linear(embed_size, cat_hidden_dim),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm1d(cat_hidden_dim),\n","            nn.Dropout(0.3),\n","            nn.Linear(cat_hidden_dim, cat_hidden_dim),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm1d(cat_hidden_dim)\n","        )\n","\n","        combined_output = lstm_output + num_hidden_dim + cat_hidden_dim\n","\n","        self.combined_dense = nn.Sequential(\n","            nn.Dropout(0.3),\n","            nn.Linear(combined_output, combined_dim),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm1d(combined_dim),\n","            nn.Dropout(0.3),\n","            nn.Linear(combined_dim, combined_dim),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm1d(combined_dim),\n","            nn.Linear(combined_dim, 1),\n","            nn.ReLU()\n","        )\n","\n","    def forward(self, X_temp, hidden, X_num, X_cat):\n","        # Forward lstm\n","        lstm_out, hidden = self.lstm(X_temp, hidden)\n","        # stack up lstm outputs\n","        lstm_out = lstm_out.contiguous().view(-1, self.lstm_hidden_dim)\n","        # dropout and fully-connected layer\n","        lstm_out = self.lstm_dense(lstm_out)\n","      \n","        # reshape to be batch_size first\n","        lstm_out = lstm_out.view(batch_size, SEQ_LENGTH, -1)\n","        lstm_out = lstm_out[:, -1, :]\n","\n","        # Forward num\n","        num_out = self.num_dense(X_num)\n","\n","        # Forward cat\n","        embeddings = []\n","        for i, e in enumerate(self.all_embeddings):\n","            embed = e(X_cat[:, i])\n","            embeddings.append(embed)\n","\n","        cat_out = torch.cat(embeddings, 1)\n","        cat_out = self.cat_dense(cat_out)\n","\n","        combined_out = torch.cat((lstm_out, num_out, cat_out), dim=1)\n","        combined_out = self.combined_dense(combined_out)\n","\n","        return combined_out, hidden\n","\n","    def init_hidden(self, batch_size):\n","        ''' Initializes hidden state '''\n","        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n","        # initialized to zero, for hidden state and cell state of LSTM\n","        weight = next(self.parameters()).data\n","        \n","        if (train_on_gpu):\n","            hidden = (weight.new(self.lstm_n_layers, batch_size, self.lstm_hidden_dim).zero_().cuda(),\n","                  weight.new(self.lstm_n_layers, batch_size, self.lstm_hidden_dim).zero_().cuda())\n","        else:\n","            hidden = (weight.new(self.lstm_n_layers, batch_size, self.lstm_hidden_dim).zero_(),\n","                      weight.new(self.lstm_n_layers, batch_size, self.lstm_hidden_dim).zero_())\n","        \n","        return hidden"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nG0TTenANiCE","colab_type":"code","outputId":"7256b60c-7ff9-40c5-c3d4-a28f79c6604a","executionInfo":{"status":"ok","timestamp":1586927566063,"user_tz":300,"elapsed":623,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Instantiate the model w/ hyperparams\n","a_h = 3\n","\n","lstm_hidden_dim = np.int(len(train_idx) / (a_h * len(numerical_columns) + 1))\n","print(lstm_hidden_dim)\n","lstm_n_layers = 2\n","\n","num_hidden_dim = 15\n","cat_hidden_dim = 15\n","lstm_output = 15\n","combined_dim = 30\n","\n","#net = MosquitoLSTM(len(numerical_columns), hidden_dim, n_layers).double()\n","net = MosquitoMixed(\n","    len(numerical_columns), lstm_hidden_dim, lstm_n_layers, lstm_output,\n","    len(non_num_columns), num_hidden_dim,\n","    embedding_sizes, cat_hidden_dim,\n","    combined_dim\n",").double()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["23\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ke9eeDONNieR","colab_type":"code","colab":{}},"source":["lr = 0.0005\n","\n","criterion = nn.MSELoss()\n","#optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n","optimizer = torch.optim.RMSprop(net.parameters(), lr=lr)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QmqNAusjNlqH","colab_type":"code","outputId":"bd3410bb-5f90-4d72-8c08-7d39445e09cd","executionInfo":{"status":"ok","timestamp":1586927743453,"user_tz":300,"elapsed":171250,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# training params\n","\n","epochs = 400 # 3-4 is approx where I noticed the validation loss stop decreasing\n","\n","counter = 0\n","print_every = 10\n","clip = 8 # gradient clipping\n","\n","# move model to GPU, if available\n","if(train_on_gpu):\n","    net.cuda()\n","\n","net.train()\n","# train for some number of epochs\n","valid_loss_min = np.Inf\n","\n","for epoch in range(epochs):\n","    # initialize hidden state\n","    h = net.init_hidden(batch_size)\n","\n","    train_loss = 0.0\n","    valid_loss = 0.0\n","\n","    net.train()\n","\n","    # batch loop\n","    for X_temp, X_num, X_cat, labels in train_loader:\n","        counter += 1\n","\n","        if(train_on_gpu):\n","            X_temp, X_num, X_cat, labels = X_temp.cuda(), X_num.cuda(), X_cat.type(torch.LongTensor).cuda(), labels.cuda()\n","\n","        # Creating new variables for the hidden state, otherwise\n","        # we'd backprop through the entire training history\n","        h = tuple([each.data for each in h])\n","        # zero accumulated gradients\n","        net.zero_grad()\n","\n","        # get the output from the model\n","        output, h = net(X_temp, h, X_num, X_cat)\n","\n","        # calculate the loss and perform backprop\n","        loss = criterion(output.squeeze(), labels.double())\n","        loss.backward()\n","        train_loss += loss.item()\n","        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","        nn.utils.clip_grad_norm_(net.parameters(), clip)\n","        optimizer.step()\n","\n","    net.eval()\n","\n","    val_h = net.init_hidden(batch_size)\n","\n","    for X_temp, X_num, X_cat, labels in valid_loader:\n","        \n","        \n","        if(train_on_gpu):\n","            X_temp, X_num, X_cat, labels = X_temp.cuda(), X_num.cuda(), X_cat.type(torch.LongTensor).cuda(), labels.cuda()\n","\n","        # Creating new variables for the hidden state, otherwise\n","        # we'd backprop through the entire training history\n","        val_h = tuple([each.data for each in val_h])\n","\n","        # get the output from the model\n","        output, h = net(X_temp, h, X_num, X_cat)\n","\n","        # calculate the loss and perform backprop\n","        loss = criterion(output.squeeze(), labels.double())\n","        valid_loss += loss.item()\n","\n","    # save model if validation loss has decreased\n","    if valid_loss <= valid_loss_min:\n","        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","        valid_loss_min,\n","        valid_loss))\n","        torch.save(net.state_dict(), 'mixed_model.pt')\n","        valid_loss_min = valid_loss\n","        \n","    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n","            epoch, train_loss, valid_loss))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Validation loss decreased (inf --> 1408.388703).  Saving model ...\n","Epoch: 0 \tTraining Loss: 5613.927077 \tValidation Loss: 1408.388703\n","Epoch: 1 \tTraining Loss: 5464.741120 \tValidation Loss: 1425.810636\n","Validation loss decreased (1408.388703 --> 1398.965393).  Saving model ...\n","Epoch: 2 \tTraining Loss: 5332.130202 \tValidation Loss: 1398.965393\n","Validation loss decreased (1398.965393 --> 1121.663717).  Saving model ...\n","Epoch: 3 \tTraining Loss: 5184.529398 \tValidation Loss: 1121.663717\n","Epoch: 4 \tTraining Loss: 5103.049155 \tValidation Loss: 1327.744298\n","Epoch: 5 \tTraining Loss: 4991.016889 \tValidation Loss: 1251.445485\n","Epoch: 6 \tTraining Loss: 4956.225093 \tValidation Loss: 1255.050812\n","Epoch: 7 \tTraining Loss: 3920.977041 \tValidation Loss: 1225.111348\n","Epoch: 8 \tTraining Loss: 4758.061543 \tValidation Loss: 1242.986504\n","Epoch: 9 \tTraining Loss: 4698.551310 \tValidation Loss: 1223.246880\n","Epoch: 10 \tTraining Loss: 4575.154572 \tValidation Loss: 1181.207521\n","Epoch: 11 \tTraining Loss: 4562.272252 \tValidation Loss: 1187.592221\n","Validation loss decreased (1121.663717 --> 1003.328698).  Saving model ...\n","Epoch: 12 \tTraining Loss: 4601.256597 \tValidation Loss: 1003.328698\n","Validation loss decreased (1003.328698 --> 943.293971).  Saving model ...\n","Epoch: 13 \tTraining Loss: 4549.384667 \tValidation Loss: 943.293971\n","Epoch: 14 \tTraining Loss: 4475.240220 \tValidation Loss: 1087.989385\n","Epoch: 15 \tTraining Loss: 4314.748794 \tValidation Loss: 1172.764157\n","Epoch: 16 \tTraining Loss: 3619.717063 \tValidation Loss: 1137.183387\n","Epoch: 17 \tTraining Loss: 4464.598506 \tValidation Loss: 1044.737271\n","Epoch: 18 \tTraining Loss: 4416.828679 \tValidation Loss: 1106.009552\n","Epoch: 19 \tTraining Loss: 4378.109719 \tValidation Loss: 1099.915399\n","Epoch: 20 \tTraining Loss: 4353.555397 \tValidation Loss: 1113.140434\n","Epoch: 21 \tTraining Loss: 4338.986533 \tValidation Loss: 1082.377342\n","Epoch: 22 \tTraining Loss: 4439.005950 \tValidation Loss: 1143.776270\n","Epoch: 23 \tTraining Loss: 4360.307087 \tValidation Loss: 1100.741529\n","Epoch: 24 \tTraining Loss: 4274.916120 \tValidation Loss: 1080.669816\n","Epoch: 25 \tTraining Loss: 4344.885421 \tValidation Loss: 1060.975055\n","Epoch: 26 \tTraining Loss: 4303.224271 \tValidation Loss: 1110.640613\n","Epoch: 27 \tTraining Loss: 4202.681283 \tValidation Loss: 1082.347487\n","Epoch: 28 \tTraining Loss: 4199.208450 \tValidation Loss: 1077.824932\n","Epoch: 29 \tTraining Loss: 4234.628685 \tValidation Loss: 1056.267335\n","Epoch: 30 \tTraining Loss: 4155.500540 \tValidation Loss: 1012.655533\n","Epoch: 31 \tTraining Loss: 4194.901412 \tValidation Loss: 1044.444875\n","Epoch: 32 \tTraining Loss: 4219.328675 \tValidation Loss: 1063.745791\n","Epoch: 33 \tTraining Loss: 4105.243827 \tValidation Loss: 1042.981109\n","Epoch: 34 \tTraining Loss: 4181.943315 \tValidation Loss: 999.559516\n","Epoch: 35 \tTraining Loss: 4118.503295 \tValidation Loss: 976.768237\n","Validation loss decreased (943.293971 --> 826.242358).  Saving model ...\n","Epoch: 36 \tTraining Loss: 4201.622486 \tValidation Loss: 826.242358\n","Epoch: 37 \tTraining Loss: 4181.090819 \tValidation Loss: 897.210557\n","Epoch: 38 \tTraining Loss: 4130.551755 \tValidation Loss: 992.202108\n","Epoch: 39 \tTraining Loss: 3807.274130 \tValidation Loss: 1011.601095\n","Epoch: 40 \tTraining Loss: 3250.264448 \tValidation Loss: 970.678512\n","Epoch: 41 \tTraining Loss: 4046.993914 \tValidation Loss: 938.389730\n","Epoch: 42 \tTraining Loss: 4244.871480 \tValidation Loss: 963.981151\n","Epoch: 43 \tTraining Loss: 4076.617916 \tValidation Loss: 1004.278110\n","Epoch: 44 \tTraining Loss: 4047.879851 \tValidation Loss: 989.865758\n","Epoch: 45 \tTraining Loss: 4091.001101 \tValidation Loss: 880.768318\n","Epoch: 46 \tTraining Loss: 4060.057595 \tValidation Loss: 902.455675\n","Epoch: 47 \tTraining Loss: 4072.207376 \tValidation Loss: 969.150708\n","Epoch: 48 \tTraining Loss: 4073.112569 \tValidation Loss: 987.304027\n","Epoch: 49 \tTraining Loss: 4047.838639 \tValidation Loss: 861.442664\n","Epoch: 50 \tTraining Loss: 4177.974648 \tValidation Loss: 863.570199\n","Epoch: 51 \tTraining Loss: 3970.239132 \tValidation Loss: 988.074335\n","Epoch: 52 \tTraining Loss: 4123.502215 \tValidation Loss: 985.662650\n","Epoch: 53 \tTraining Loss: 4092.470368 \tValidation Loss: 890.169742\n","Epoch: 54 \tTraining Loss: 4069.390346 \tValidation Loss: 987.543667\n","Epoch: 55 \tTraining Loss: 3962.132708 \tValidation Loss: 944.412945\n","Epoch: 56 \tTraining Loss: 4077.997454 \tValidation Loss: 1052.463173\n","Epoch: 57 \tTraining Loss: 4078.555309 \tValidation Loss: 1112.451932\n","Epoch: 58 \tTraining Loss: 4064.598502 \tValidation Loss: 854.911056\n","Epoch: 59 \tTraining Loss: 4056.738984 \tValidation Loss: 957.020979\n","Epoch: 60 \tTraining Loss: 4116.089262 \tValidation Loss: 1017.513853\n","Epoch: 61 \tTraining Loss: 3963.247367 \tValidation Loss: 1073.132700\n","Epoch: 62 \tTraining Loss: 4138.824250 \tValidation Loss: 1014.207854\n","Epoch: 63 \tTraining Loss: 4150.220491 \tValidation Loss: 906.632745\n","Epoch: 64 \tTraining Loss: 4048.727386 \tValidation Loss: 961.837957\n","Epoch: 65 \tTraining Loss: 4102.823576 \tValidation Loss: 962.172904\n","Epoch: 66 \tTraining Loss: 4090.797924 \tValidation Loss: 926.069977\n","Epoch: 67 \tTraining Loss: 3795.896220 \tValidation Loss: 977.148407\n","Epoch: 68 \tTraining Loss: 4147.295290 \tValidation Loss: 967.199216\n","Epoch: 69 \tTraining Loss: 4034.613794 \tValidation Loss: 962.481669\n","Epoch: 70 \tTraining Loss: 4035.007639 \tValidation Loss: 939.667193\n","Epoch: 71 \tTraining Loss: 4125.533224 \tValidation Loss: 933.221110\n","Epoch: 72 \tTraining Loss: 4069.902220 \tValidation Loss: 942.480131\n","Epoch: 73 \tTraining Loss: 4088.686642 \tValidation Loss: 942.801775\n","Epoch: 74 \tTraining Loss: 4055.526095 \tValidation Loss: 840.282099\n","Epoch: 75 \tTraining Loss: 3991.432550 \tValidation Loss: 1008.420676\n","Epoch: 76 \tTraining Loss: 4152.975843 \tValidation Loss: 938.096740\n","Epoch: 77 \tTraining Loss: 4020.895529 \tValidation Loss: 965.944157\n","Epoch: 78 \tTraining Loss: 3110.366464 \tValidation Loss: 952.431310\n","Epoch: 79 \tTraining Loss: 3930.716965 \tValidation Loss: 942.969531\n","Epoch: 80 \tTraining Loss: 3866.670220 \tValidation Loss: 959.970517\n","Epoch: 81 \tTraining Loss: 4018.208919 \tValidation Loss: 949.652657\n","Epoch: 82 \tTraining Loss: 3921.949018 \tValidation Loss: 944.501743\n","Epoch: 83 \tTraining Loss: 3961.154764 \tValidation Loss: 958.912900\n","Epoch: 84 \tTraining Loss: 3983.718756 \tValidation Loss: 933.232008\n","Epoch: 85 \tTraining Loss: 4027.256970 \tValidation Loss: 948.677360\n","Epoch: 86 \tTraining Loss: 3874.818439 \tValidation Loss: 956.284596\n","Validation loss decreased (826.242358 --> 821.236630).  Saving model ...\n","Epoch: 87 \tTraining Loss: 3884.709025 \tValidation Loss: 821.236630\n","Epoch: 88 \tTraining Loss: 4031.747770 \tValidation Loss: 842.030395\n","Epoch: 89 \tTraining Loss: 3881.810312 \tValidation Loss: 958.215557\n","Epoch: 90 \tTraining Loss: 3968.084079 \tValidation Loss: 905.333580\n","Epoch: 91 \tTraining Loss: 3956.481463 \tValidation Loss: 954.600212\n","Epoch: 92 \tTraining Loss: 3788.388302 \tValidation Loss: 960.617429\n","Epoch: 93 \tTraining Loss: 3970.818295 \tValidation Loss: 933.132775\n","Epoch: 94 \tTraining Loss: 3976.993080 \tValidation Loss: 909.227254\n","Epoch: 95 \tTraining Loss: 3985.898502 \tValidation Loss: 918.199320\n","Epoch: 96 \tTraining Loss: 3948.008072 \tValidation Loss: 839.805573\n","Epoch: 97 \tTraining Loss: 3938.150116 \tValidation Loss: 960.602816\n","Epoch: 98 \tTraining Loss: 3959.165266 \tValidation Loss: 950.772389\n","Epoch: 99 \tTraining Loss: 3986.261632 \tValidation Loss: 925.595149\n","Epoch: 100 \tTraining Loss: 3696.849486 \tValidation Loss: 938.048900\n","Epoch: 101 \tTraining Loss: 3831.556138 \tValidation Loss: 955.273490\n","Epoch: 102 \tTraining Loss: 3899.697022 \tValidation Loss: 970.195425\n","Epoch: 103 \tTraining Loss: 3584.741736 \tValidation Loss: 960.626984\n","Epoch: 104 \tTraining Loss: 3852.817641 \tValidation Loss: 954.021551\n","Validation loss decreased (821.236630 --> 686.134851).  Saving model ...\n","Epoch: 105 \tTraining Loss: 3898.904234 \tValidation Loss: 686.134851\n","Epoch: 106 \tTraining Loss: 3927.646464 \tValidation Loss: 817.860457\n","Epoch: 107 \tTraining Loss: 3883.992979 \tValidation Loss: 866.125686\n","Epoch: 108 \tTraining Loss: 3982.654794 \tValidation Loss: 959.192565\n","Epoch: 109 \tTraining Loss: 3933.059689 \tValidation Loss: 960.835640\n","Epoch: 110 \tTraining Loss: 3839.016282 \tValidation Loss: 951.876734\n","Epoch: 111 \tTraining Loss: 3927.581441 \tValidation Loss: 951.600701\n","Epoch: 112 \tTraining Loss: 3953.218749 \tValidation Loss: 951.169358\n","Epoch: 113 \tTraining Loss: 3875.593121 \tValidation Loss: 946.823861\n","Epoch: 114 \tTraining Loss: 3909.860712 \tValidation Loss: 960.533286\n","Epoch: 115 \tTraining Loss: 3935.103749 \tValidation Loss: 973.840727\n","Epoch: 116 \tTraining Loss: 3824.950579 \tValidation Loss: 935.004408\n","Epoch: 117 \tTraining Loss: 4003.172553 \tValidation Loss: 903.461645\n","Epoch: 118 \tTraining Loss: 3943.298866 \tValidation Loss: 951.794443\n","Epoch: 119 \tTraining Loss: 3947.015909 \tValidation Loss: 947.644554\n","Epoch: 120 \tTraining Loss: 3900.040645 \tValidation Loss: 900.445399\n","Epoch: 121 \tTraining Loss: 3919.212613 \tValidation Loss: 916.726483\n","Epoch: 122 \tTraining Loss: 3890.693921 \tValidation Loss: 922.839198\n","Epoch: 123 \tTraining Loss: 3903.416673 \tValidation Loss: 922.226310\n","Epoch: 124 \tTraining Loss: 3575.283223 \tValidation Loss: 941.171741\n","Epoch: 125 \tTraining Loss: 3436.880896 \tValidation Loss: 937.806189\n","Epoch: 126 \tTraining Loss: 3895.681430 \tValidation Loss: 956.774494\n","Epoch: 127 \tTraining Loss: 3888.372303 \tValidation Loss: 750.035700\n","Epoch: 128 \tTraining Loss: 3919.007015 \tValidation Loss: 951.480005\n","Epoch: 129 \tTraining Loss: 3949.808791 \tValidation Loss: 952.989084\n","Epoch: 130 \tTraining Loss: 3888.520019 \tValidation Loss: 944.213088\n","Epoch: 131 \tTraining Loss: 3965.543775 \tValidation Loss: 949.972197\n","Epoch: 132 \tTraining Loss: 3905.122032 \tValidation Loss: 933.998634\n","Epoch: 133 \tTraining Loss: 3935.252929 \tValidation Loss: 934.713846\n","Epoch: 134 \tTraining Loss: 3512.434405 \tValidation Loss: 953.519575\n","Epoch: 135 \tTraining Loss: 3955.216780 \tValidation Loss: 950.607637\n","Epoch: 136 \tTraining Loss: 3970.025690 \tValidation Loss: 957.236112\n","Epoch: 137 \tTraining Loss: 3928.170874 \tValidation Loss: 831.009051\n","Epoch: 138 \tTraining Loss: 3900.873248 \tValidation Loss: 954.246059\n","Epoch: 139 \tTraining Loss: 3874.931531 \tValidation Loss: 950.273338\n","Epoch: 140 \tTraining Loss: 3797.565418 \tValidation Loss: 896.843971\n","Epoch: 141 \tTraining Loss: 3923.733034 \tValidation Loss: 736.944530\n","Epoch: 142 \tTraining Loss: 3879.206722 \tValidation Loss: 912.266101\n","Epoch: 143 \tTraining Loss: 3833.029541 \tValidation Loss: 932.609358\n","Epoch: 144 \tTraining Loss: 3703.453633 \tValidation Loss: 805.974954\n","Epoch: 145 \tTraining Loss: 3902.962707 \tValidation Loss: 897.659189\n","Epoch: 146 \tTraining Loss: 3827.417131 \tValidation Loss: 927.317976\n","Epoch: 147 \tTraining Loss: 3942.422723 \tValidation Loss: 956.230874\n","Epoch: 148 \tTraining Loss: 3872.581927 \tValidation Loss: 944.296371\n","Epoch: 149 \tTraining Loss: 3853.202416 \tValidation Loss: 867.030378\n","Epoch: 150 \tTraining Loss: 3852.535661 \tValidation Loss: 922.425064\n","Epoch: 151 \tTraining Loss: 3859.447919 \tValidation Loss: 943.816486\n","Epoch: 152 \tTraining Loss: 3852.204243 \tValidation Loss: 938.464315\n","Epoch: 153 \tTraining Loss: 3916.001362 \tValidation Loss: 790.163682\n","Epoch: 154 \tTraining Loss: 3861.550000 \tValidation Loss: 950.334446\n","Epoch: 155 \tTraining Loss: 3895.096539 \tValidation Loss: 919.476355\n","Epoch: 156 \tTraining Loss: 3853.199730 \tValidation Loss: 954.340687\n","Epoch: 157 \tTraining Loss: 3825.232060 \tValidation Loss: 917.653944\n","Epoch: 158 \tTraining Loss: 3877.212665 \tValidation Loss: 901.807944\n","Epoch: 159 \tTraining Loss: 3868.545157 \tValidation Loss: 917.903498\n","Epoch: 160 \tTraining Loss: 3957.919905 \tValidation Loss: 930.019592\n","Epoch: 161 \tTraining Loss: 3881.980869 \tValidation Loss: 898.005400\n","Epoch: 162 \tTraining Loss: 3768.268926 \tValidation Loss: 949.861536\n","Epoch: 163 \tTraining Loss: 3833.436163 \tValidation Loss: 928.268745\n","Epoch: 164 \tTraining Loss: 3830.257455 \tValidation Loss: 915.828100\n","Epoch: 165 \tTraining Loss: 3780.652626 \tValidation Loss: 919.847624\n","Epoch: 166 \tTraining Loss: 3924.305331 \tValidation Loss: 924.877051\n","Epoch: 167 \tTraining Loss: 3931.622856 \tValidation Loss: 929.500151\n","Epoch: 168 \tTraining Loss: 3875.610258 \tValidation Loss: 873.928296\n","Epoch: 169 \tTraining Loss: 3684.575731 \tValidation Loss: 906.166183\n","Epoch: 170 \tTraining Loss: 3800.564803 \tValidation Loss: 930.821030\n","Epoch: 171 \tTraining Loss: 3915.773582 \tValidation Loss: 912.480801\n","Epoch: 172 \tTraining Loss: 3823.694937 \tValidation Loss: 937.771662\n","Epoch: 173 \tTraining Loss: 3772.325674 \tValidation Loss: 920.159569\n","Epoch: 174 \tTraining Loss: 3845.612172 \tValidation Loss: 913.369449\n","Epoch: 175 \tTraining Loss: 3849.356631 \tValidation Loss: 947.002919\n","Epoch: 176 \tTraining Loss: 3819.344641 \tValidation Loss: 782.258159\n","Epoch: 177 \tTraining Loss: 3808.115069 \tValidation Loss: 922.582336\n","Epoch: 178 \tTraining Loss: 3819.560194 \tValidation Loss: 965.064566\n","Epoch: 179 \tTraining Loss: 3837.922943 \tValidation Loss: 920.221827\n","Epoch: 180 \tTraining Loss: 3362.176390 \tValidation Loss: 912.777815\n","Epoch: 181 \tTraining Loss: 3766.848251 \tValidation Loss: 924.658643\n","Epoch: 182 \tTraining Loss: 3967.251417 \tValidation Loss: 915.362785\n","Epoch: 183 \tTraining Loss: 3880.084442 \tValidation Loss: 940.009726\n","Epoch: 184 \tTraining Loss: 3847.809949 \tValidation Loss: 927.084925\n","Epoch: 185 \tTraining Loss: 3906.048935 \tValidation Loss: 915.692598\n","Epoch: 186 \tTraining Loss: 3843.228275 \tValidation Loss: 928.114793\n","Epoch: 187 \tTraining Loss: 3830.575471 \tValidation Loss: 922.377654\n","Epoch: 188 \tTraining Loss: 3735.196999 \tValidation Loss: 912.658345\n","Epoch: 189 \tTraining Loss: 3831.968903 \tValidation Loss: 924.206181\n","Epoch: 190 \tTraining Loss: 3835.144896 \tValidation Loss: 902.491770\n","Epoch: 191 \tTraining Loss: 3762.894109 \tValidation Loss: 943.978877\n","Epoch: 192 \tTraining Loss: 3832.422348 \tValidation Loss: 911.461522\n","Epoch: 193 \tTraining Loss: 3747.773779 \tValidation Loss: 911.405008\n","Epoch: 194 \tTraining Loss: 3738.890977 \tValidation Loss: 902.295315\n","Epoch: 195 \tTraining Loss: 3844.341465 \tValidation Loss: 946.767855\n","Epoch: 196 \tTraining Loss: 3798.921984 \tValidation Loss: 920.480511\n","Epoch: 197 \tTraining Loss: 3849.063571 \tValidation Loss: 897.145026\n","Epoch: 198 \tTraining Loss: 3800.395512 \tValidation Loss: 912.792409\n","Epoch: 199 \tTraining Loss: 3753.565685 \tValidation Loss: 901.423561\n","Epoch: 200 \tTraining Loss: 3748.681098 \tValidation Loss: 914.606780\n","Epoch: 201 \tTraining Loss: 3844.419911 \tValidation Loss: 817.747479\n","Epoch: 202 \tTraining Loss: 3723.378840 \tValidation Loss: 944.524930\n","Epoch: 203 \tTraining Loss: 3826.671275 \tValidation Loss: 945.589676\n","Epoch: 204 \tTraining Loss: 3803.181716 \tValidation Loss: 967.205486\n","Epoch: 205 \tTraining Loss: 3769.301497 \tValidation Loss: 928.394589\n","Epoch: 206 \tTraining Loss: 3743.715853 \tValidation Loss: 952.945636\n","Epoch: 207 \tTraining Loss: 3797.273146 \tValidation Loss: 909.741581\n","Epoch: 208 \tTraining Loss: 3836.973163 \tValidation Loss: 950.227410\n","Epoch: 209 \tTraining Loss: 3868.776414 \tValidation Loss: 923.699465\n","Epoch: 210 \tTraining Loss: 3824.230854 \tValidation Loss: 924.784710\n","Epoch: 211 \tTraining Loss: 3886.651748 \tValidation Loss: 801.195239\n","Epoch: 212 \tTraining Loss: 3799.177593 \tValidation Loss: 946.494225\n","Epoch: 213 \tTraining Loss: 3755.015146 \tValidation Loss: 919.091041\n","Epoch: 214 \tTraining Loss: 3661.819589 \tValidation Loss: 900.549385\n","Epoch: 215 \tTraining Loss: 3730.251688 \tValidation Loss: 924.899511\n","Epoch: 216 \tTraining Loss: 3704.551580 \tValidation Loss: 924.785896\n","Epoch: 217 \tTraining Loss: 3872.717923 \tValidation Loss: 899.341553\n","Epoch: 218 \tTraining Loss: 3788.624575 \tValidation Loss: 919.218371\n","Epoch: 219 \tTraining Loss: 3653.707480 \tValidation Loss: 936.479872\n","Epoch: 220 \tTraining Loss: 3758.059357 \tValidation Loss: 935.698545\n","Epoch: 221 \tTraining Loss: 3696.128934 \tValidation Loss: 872.213008\n","Epoch: 222 \tTraining Loss: 3762.379577 \tValidation Loss: 910.536020\n","Epoch: 223 \tTraining Loss: 3750.986974 \tValidation Loss: 946.954234\n","Epoch: 224 \tTraining Loss: 3876.110835 \tValidation Loss: 905.507466\n","Epoch: 225 \tTraining Loss: 3781.948660 \tValidation Loss: 909.104217\n","Epoch: 226 \tTraining Loss: 3764.025657 \tValidation Loss: 933.698592\n","Epoch: 227 \tTraining Loss: 3770.007466 \tValidation Loss: 923.582065\n","Epoch: 228 \tTraining Loss: 3778.922956 \tValidation Loss: 930.563756\n","Epoch: 229 \tTraining Loss: 3879.790756 \tValidation Loss: 925.103676\n","Epoch: 230 \tTraining Loss: 3797.248303 \tValidation Loss: 915.051266\n","Epoch: 231 \tTraining Loss: 3815.678995 \tValidation Loss: 955.173301\n","Epoch: 232 \tTraining Loss: 3750.440319 \tValidation Loss: 920.617474\n","Epoch: 233 \tTraining Loss: 3684.074432 \tValidation Loss: 921.784806\n","Epoch: 234 \tTraining Loss: 3788.805537 \tValidation Loss: 902.408601\n","Epoch: 235 \tTraining Loss: 3735.200918 \tValidation Loss: 918.567759\n","Epoch: 236 \tTraining Loss: 3773.427219 \tValidation Loss: 923.013997\n","Epoch: 237 \tTraining Loss: 3889.533553 \tValidation Loss: 948.607115\n","Epoch: 238 \tTraining Loss: 3757.647529 \tValidation Loss: 913.951501\n","Epoch: 239 \tTraining Loss: 3789.239673 \tValidation Loss: 934.209030\n","Epoch: 240 \tTraining Loss: 3706.989941 \tValidation Loss: 931.148379\n","Epoch: 241 \tTraining Loss: 3815.842160 \tValidation Loss: 928.580767\n","Epoch: 242 \tTraining Loss: 3816.048969 \tValidation Loss: 915.863639\n","Epoch: 243 \tTraining Loss: 3820.138243 \tValidation Loss: 905.954802\n","Epoch: 244 \tTraining Loss: 3734.986200 \tValidation Loss: 908.491021\n","Epoch: 245 \tTraining Loss: 3818.606003 \tValidation Loss: 924.686195\n","Epoch: 246 \tTraining Loss: 3680.113107 \tValidation Loss: 922.002814\n","Epoch: 247 \tTraining Loss: 3462.258657 \tValidation Loss: 918.306382\n","Epoch: 248 \tTraining Loss: 3728.460006 \tValidation Loss: 916.841065\n","Epoch: 249 \tTraining Loss: 3679.036001 \tValidation Loss: 892.376744\n","Epoch: 250 \tTraining Loss: 3762.547817 \tValidation Loss: 918.589541\n","Epoch: 251 \tTraining Loss: 3637.022793 \tValidation Loss: 902.661486\n","Epoch: 252 \tTraining Loss: 3861.122695 \tValidation Loss: 729.228480\n","Epoch: 253 \tTraining Loss: 3699.554810 \tValidation Loss: 942.083565\n","Epoch: 254 \tTraining Loss: 3695.628547 \tValidation Loss: 937.522185\n","Epoch: 255 \tTraining Loss: 3790.194696 \tValidation Loss: 904.472672\n","Epoch: 256 \tTraining Loss: 3732.437416 \tValidation Loss: 922.775010\n","Epoch: 257 \tTraining Loss: 3695.565250 \tValidation Loss: 812.663682\n","Epoch: 258 \tTraining Loss: 3761.283512 \tValidation Loss: 921.812306\n","Epoch: 259 \tTraining Loss: 3699.254148 \tValidation Loss: 933.780725\n","Epoch: 260 \tTraining Loss: 3700.274415 \tValidation Loss: 914.012227\n","Epoch: 261 \tTraining Loss: 3680.133822 \tValidation Loss: 925.591527\n","Epoch: 262 \tTraining Loss: 3731.064180 \tValidation Loss: 907.980712\n","Epoch: 263 \tTraining Loss: 3692.006306 \tValidation Loss: 909.872060\n","Epoch: 264 \tTraining Loss: 3639.899494 \tValidation Loss: 940.339260\n","Epoch: 265 \tTraining Loss: 3705.156692 \tValidation Loss: 926.021073\n","Epoch: 266 \tTraining Loss: 3743.393503 \tValidation Loss: 919.245300\n","Epoch: 267 \tTraining Loss: 3625.899208 \tValidation Loss: 922.753665\n","Epoch: 268 \tTraining Loss: 3803.634394 \tValidation Loss: 939.460525\n","Epoch: 269 \tTraining Loss: 3823.560459 \tValidation Loss: 944.228897\n","Epoch: 270 \tTraining Loss: 3830.936848 \tValidation Loss: 827.993083\n","Epoch: 271 \tTraining Loss: 3661.014280 \tValidation Loss: 923.334974\n","Epoch: 272 \tTraining Loss: 3709.638537 \tValidation Loss: 928.008484\n","Epoch: 273 \tTraining Loss: 3696.330117 \tValidation Loss: 915.795919\n","Epoch: 274 \tTraining Loss: 3651.518793 \tValidation Loss: 936.751548\n","Epoch: 275 \tTraining Loss: 3665.296900 \tValidation Loss: 919.178017\n","Epoch: 276 \tTraining Loss: 3721.737938 \tValidation Loss: 918.969718\n","Epoch: 277 \tTraining Loss: 3763.114536 \tValidation Loss: 936.609512\n","Epoch: 278 \tTraining Loss: 3749.182575 \tValidation Loss: 929.988302\n","Epoch: 279 \tTraining Loss: 3705.182624 \tValidation Loss: 882.299340\n","Epoch: 280 \tTraining Loss: 3700.958997 \tValidation Loss: 815.379020\n","Epoch: 281 \tTraining Loss: 3686.878452 \tValidation Loss: 922.983289\n","Epoch: 282 \tTraining Loss: 3676.419016 \tValidation Loss: 909.661958\n","Epoch: 283 \tTraining Loss: 3717.519057 \tValidation Loss: 900.386841\n","Epoch: 284 \tTraining Loss: 3743.263624 \tValidation Loss: 896.064700\n","Epoch: 285 \tTraining Loss: 3724.817435 \tValidation Loss: 896.412044\n","Epoch: 286 \tTraining Loss: 3742.891617 \tValidation Loss: 878.915560\n","Epoch: 287 \tTraining Loss: 3690.676383 \tValidation Loss: 918.817867\n","Epoch: 288 \tTraining Loss: 3726.984467 \tValidation Loss: 904.573050\n","Epoch: 289 \tTraining Loss: 3764.097214 \tValidation Loss: 917.216106\n","Epoch: 290 \tTraining Loss: 3620.335370 \tValidation Loss: 885.505562\n","Epoch: 291 \tTraining Loss: 3319.061270 \tValidation Loss: 924.236584\n","Epoch: 292 \tTraining Loss: 3704.670572 \tValidation Loss: 852.623878\n","Epoch: 293 \tTraining Loss: 3629.589723 \tValidation Loss: 917.505723\n","Epoch: 294 \tTraining Loss: 3683.929185 \tValidation Loss: 914.921870\n","Epoch: 295 \tTraining Loss: 3681.928711 \tValidation Loss: 911.757383\n","Epoch: 296 \tTraining Loss: 3711.304542 \tValidation Loss: 845.687477\n","Epoch: 297 \tTraining Loss: 3715.595707 \tValidation Loss: 720.178986\n","Epoch: 298 \tTraining Loss: 3696.431609 \tValidation Loss: 913.321040\n","Epoch: 299 \tTraining Loss: 3719.108494 \tValidation Loss: 911.335388\n","Epoch: 300 \tTraining Loss: 3722.749558 \tValidation Loss: 920.178476\n","Epoch: 301 \tTraining Loss: 3700.509603 \tValidation Loss: 907.460433\n","Epoch: 302 \tTraining Loss: 3635.004743 \tValidation Loss: 914.119037\n","Epoch: 303 \tTraining Loss: 3781.916682 \tValidation Loss: 887.992085\n","Epoch: 304 \tTraining Loss: 3723.388119 \tValidation Loss: 902.640530\n","Epoch: 305 \tTraining Loss: 3571.313864 \tValidation Loss: 904.305974\n","Epoch: 306 \tTraining Loss: 3720.946837 \tValidation Loss: 897.840889\n","Epoch: 307 \tTraining Loss: 3620.203721 \tValidation Loss: 909.825163\n","Epoch: 308 \tTraining Loss: 3695.958697 \tValidation Loss: 889.926603\n","Epoch: 309 \tTraining Loss: 3643.468317 \tValidation Loss: 923.606199\n","Epoch: 310 \tTraining Loss: 3624.363649 \tValidation Loss: 834.705686\n","Epoch: 311 \tTraining Loss: 3644.678652 \tValidation Loss: 908.341729\n","Epoch: 312 \tTraining Loss: 3695.859920 \tValidation Loss: 910.413485\n","Epoch: 313 \tTraining Loss: 3686.892223 \tValidation Loss: 908.533307\n","Epoch: 314 \tTraining Loss: 3738.948034 \tValidation Loss: 914.821337\n","Epoch: 315 \tTraining Loss: 3689.882222 \tValidation Loss: 826.285792\n","Epoch: 316 \tTraining Loss: 3603.098903 \tValidation Loss: 865.772555\n","Epoch: 317 \tTraining Loss: 3598.400232 \tValidation Loss: 875.939756\n","Epoch: 318 \tTraining Loss: 3645.571340 \tValidation Loss: 895.992620\n","Epoch: 319 \tTraining Loss: 3538.561468 \tValidation Loss: 878.592392\n","Epoch: 320 \tTraining Loss: 3653.619101 \tValidation Loss: 815.363574\n","Epoch: 321 \tTraining Loss: 3715.472194 \tValidation Loss: 904.582350\n","Epoch: 322 \tTraining Loss: 3662.085245 \tValidation Loss: 907.526916\n","Epoch: 323 \tTraining Loss: 3603.759044 \tValidation Loss: 896.470000\n","Epoch: 324 \tTraining Loss: 3634.384235 \tValidation Loss: 918.491019\n","Epoch: 325 \tTraining Loss: 3623.975691 \tValidation Loss: 912.525388\n","Epoch: 326 \tTraining Loss: 3638.333999 \tValidation Loss: 887.723099\n","Epoch: 327 \tTraining Loss: 3655.164717 \tValidation Loss: 899.188293\n","Epoch: 328 \tTraining Loss: 3633.419444 \tValidation Loss: 907.135501\n","Epoch: 329 \tTraining Loss: 3613.469670 \tValidation Loss: 914.032909\n","Epoch: 330 \tTraining Loss: 3671.764994 \tValidation Loss: 886.130451\n","Epoch: 331 \tTraining Loss: 3596.235153 \tValidation Loss: 924.061351\n","Epoch: 332 \tTraining Loss: 3231.448865 \tValidation Loss: 907.778500\n","Epoch: 333 \tTraining Loss: 3618.138761 \tValidation Loss: 906.401760\n","Epoch: 334 \tTraining Loss: 3637.511331 \tValidation Loss: 920.759065\n","Epoch: 335 \tTraining Loss: 3708.677004 \tValidation Loss: 897.351336\n","Epoch: 336 \tTraining Loss: 3526.368447 \tValidation Loss: 888.777114\n","Epoch: 337 \tTraining Loss: 3695.375858 \tValidation Loss: 913.669659\n","Epoch: 338 \tTraining Loss: 3619.215191 \tValidation Loss: 808.767154\n","Epoch: 339 \tTraining Loss: 3620.029985 \tValidation Loss: 893.013295\n","Epoch: 340 \tTraining Loss: 3658.735051 \tValidation Loss: 807.569277\n","Epoch: 341 \tTraining Loss: 3617.560179 \tValidation Loss: 877.467420\n","Epoch: 342 \tTraining Loss: 3665.527699 \tValidation Loss: 870.433130\n","Epoch: 343 \tTraining Loss: 3715.464195 \tValidation Loss: 809.103321\n","Epoch: 344 \tTraining Loss: 3602.539033 \tValidation Loss: 768.286710\n","Epoch: 345 \tTraining Loss: 3603.557882 \tValidation Loss: 914.527720\n","Epoch: 346 \tTraining Loss: 3697.382652 \tValidation Loss: 888.470800\n","Epoch: 347 \tTraining Loss: 3499.850502 \tValidation Loss: 851.982441\n","Epoch: 348 \tTraining Loss: 3415.281069 \tValidation Loss: 878.602393\n","Epoch: 349 \tTraining Loss: 3705.555717 \tValidation Loss: 831.990592\n","Epoch: 350 \tTraining Loss: 3739.105594 \tValidation Loss: 895.131010\n","Epoch: 351 \tTraining Loss: 3743.740381 \tValidation Loss: 902.849679\n","Epoch: 352 \tTraining Loss: 3667.187548 \tValidation Loss: 906.121432\n","Epoch: 353 \tTraining Loss: 3664.419269 \tValidation Loss: 869.856925\n","Epoch: 354 \tTraining Loss: 3649.063638 \tValidation Loss: 893.097467\n","Epoch: 355 \tTraining Loss: 3676.212546 \tValidation Loss: 902.600960\n","Epoch: 356 \tTraining Loss: 3564.971145 \tValidation Loss: 908.073553\n","Epoch: 357 \tTraining Loss: 3663.679838 \tValidation Loss: 894.257983\n","Epoch: 358 \tTraining Loss: 3597.727959 \tValidation Loss: 909.350221\n","Epoch: 359 \tTraining Loss: 3572.416625 \tValidation Loss: 725.021012\n","Epoch: 360 \tTraining Loss: 3637.247662 \tValidation Loss: 898.810764\n","Epoch: 361 \tTraining Loss: 3638.638407 \tValidation Loss: 908.563472\n","Epoch: 362 \tTraining Loss: 3727.776871 \tValidation Loss: 915.457281\n","Epoch: 363 \tTraining Loss: 3630.815946 \tValidation Loss: 913.432116\n","Epoch: 364 \tTraining Loss: 3701.397388 \tValidation Loss: 853.950177\n","Epoch: 365 \tTraining Loss: 3637.707670 \tValidation Loss: 873.731833\n","Epoch: 366 \tTraining Loss: 3685.653671 \tValidation Loss: 919.199127\n","Epoch: 367 \tTraining Loss: 3618.260614 \tValidation Loss: 916.626279\n","Epoch: 368 \tTraining Loss: 3614.663822 \tValidation Loss: 919.913056\n","Epoch: 369 \tTraining Loss: 3593.233540 \tValidation Loss: 781.144918\n","Epoch: 370 \tTraining Loss: 3559.338765 \tValidation Loss: 917.752149\n","Epoch: 371 \tTraining Loss: 3587.870673 \tValidation Loss: 892.425512\n","Epoch: 372 \tTraining Loss: 3570.636379 \tValidation Loss: 897.147509\n","Epoch: 373 \tTraining Loss: 3632.019557 \tValidation Loss: 912.851209\n","Epoch: 374 \tTraining Loss: 3617.449277 \tValidation Loss: 907.092779\n","Epoch: 375 \tTraining Loss: 3635.448285 \tValidation Loss: 910.513249\n","Epoch: 376 \tTraining Loss: 3615.824289 \tValidation Loss: 905.232423\n","Epoch: 377 \tTraining Loss: 3663.072034 \tValidation Loss: 911.484791\n","Epoch: 378 \tTraining Loss: 3473.488199 \tValidation Loss: 907.817486\n","Epoch: 379 \tTraining Loss: 3553.029615 \tValidation Loss: 912.962773\n","Epoch: 380 \tTraining Loss: 3508.819844 \tValidation Loss: 703.749281\n","Epoch: 381 \tTraining Loss: 3533.888509 \tValidation Loss: 924.317793\n","Epoch: 382 \tTraining Loss: 3629.560503 \tValidation Loss: 924.718387\n","Epoch: 383 \tTraining Loss: 3695.077498 \tValidation Loss: 925.001631\n","Epoch: 384 \tTraining Loss: 3575.649250 \tValidation Loss: 714.510530\n","Epoch: 385 \tTraining Loss: 3566.378588 \tValidation Loss: 916.527340\n","Epoch: 386 \tTraining Loss: 3533.722657 \tValidation Loss: 933.183346\n","Validation loss decreased (686.134851 --> 642.829318).  Saving model ...\n","Epoch: 387 \tTraining Loss: 3573.038820 \tValidation Loss: 642.829318\n","Epoch: 388 \tTraining Loss: 3692.292549 \tValidation Loss: 933.492989\n","Epoch: 389 \tTraining Loss: 3697.063977 \tValidation Loss: 923.881933\n","Epoch: 390 \tTraining Loss: 3640.119638 \tValidation Loss: 922.359502\n","Epoch: 391 \tTraining Loss: 3634.812868 \tValidation Loss: 854.638008\n","Epoch: 392 \tTraining Loss: 3721.460966 \tValidation Loss: 927.238286\n","Epoch: 393 \tTraining Loss: 3588.703511 \tValidation Loss: 928.550975\n","Epoch: 394 \tTraining Loss: 3633.251668 \tValidation Loss: 923.112673\n","Epoch: 395 \tTraining Loss: 3578.653287 \tValidation Loss: 892.294006\n","Epoch: 396 \tTraining Loss: 3476.178192 \tValidation Loss: 907.180144\n","Epoch: 397 \tTraining Loss: 3612.604861 \tValidation Loss: 907.409999\n","Epoch: 398 \tTraining Loss: 3567.475261 \tValidation Loss: 906.384060\n","Epoch: 399 \tTraining Loss: 3607.108436 \tValidation Loss: 894.684867\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OqtVdqkZP21K","colab_type":"code","outputId":"f79edf13-0848-4e19-9654-6b472476c90e","executionInfo":{"status":"ok","timestamp":1586929265664,"user_tz":300,"elapsed":809,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["net.load_state_dict(torch.load('mixed_model.pt'))"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":440}]},{"cell_type":"code","metadata":{"id":"iRCN4REPP93Y","colab_type":"code","outputId":"f5aa18ba-328b-40d6-d252-4df6ee482872","executionInfo":{"status":"ok","timestamp":1586929260594,"user_tz":300,"elapsed":1369,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":199}},"source":["from sklearn.metrics import mean_absolute_error\n","\n","# track test loss\n","test_loss = 0.0\n","mean_abs = 0.0\n","\n","net.eval()\n","# iterate over test data\n","#bin_op.binarization()\n","test_h = net.init_hidden(batch_size)\n","\n","for batch_idx, (X_temp, X_num, X_cat, labels) in enumerate(test_loader):\n","    if(train_on_gpu):\n","        X_temp, X_num, X_cat, labels = X_temp.cuda(), X_num.cuda(), X_cat.type(torch.LongTensor).cuda(), labels.cuda()\n","    \n","    test_h = tuple([each.data for each in test_h])\n","\n","    # get the output from the model\n","    output, h = net(X_temp, h, X_num, X_cat)\n","    # / (weight_lstm + weight_num + weight_cat)\n","\n","    # calculate the loss and perform backprop\n","    loss = criterion(output.squeeze(), labels.double())\n","\n","    out_np = output.detach().squeeze().cpu().numpy().astype(int).T\n","    target_np = labels.detach().cpu().numpy().astype(int).T\n","\n","    print([(out, tar) for out, tar in zip(out_np, target_np)])\n","\n","    mean_abs += mean_absolute_error(out_np, target_np) * batch_size\n","        \n","    test_loss += loss.item() * batch_size\n","\n","# calculate average losses\n","test_loss = np.sqrt(test_loss / len(test_loader.dataset))\n","mean_abs /= len(test_loader.dataset)\n","print('Test Loss: {:.6f}\\n'.format(test_loss))\n","print('Mean Abs Loss: {:.6f}\\n'.format(mean_abs))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[(4, 3), (4, 3), (4, 2), (7, 2), (4, 2), (4, 3), (8, 3), (3, 3), (3, 2), (3, 6), (3, 4), (3, 4), (3, 5), (3, 3), (3, 2), (3, 16)]\n","[(3, 4), (3, 10), (3, 5), (4, 3), (4, 8), (8, 2), (6, 9), (4, 66), (6, 3), (7, 9), (7, 11), (7, 8), (5, 25), (5, 4), (6, 3), (5, 2)]\n","[(10, 3), (5, 3), (6, 6), (5, 3), (6, 8), (6, 8), (5, 6), (5, 9), (6, 12), (6, 18), (5, 24), (5, 19), (8, 3), (5, 15), (7, 12), (7, 12)]\n","[(5, 17), (5, 9), (7, 3), (5, 12), (6, 33), (6, 11), (5, 6), (6, 2), (5, 7), (5, 5), (5, 2), (8, 11), (6, 23), (6, 4), (5, 7), (7, 5)]\n","[(5, 6), (7, 10), (7, 12), (6, 5), (7, 10), (4, 2), (8, 3), (7, 32), (7, 13), (4, 27), (8, 9), (8, 12), (5, 16), (8, 6), (4, 3), (7, 7)]\n","[(4, 2), (4, 6), (8, 10), (7, 6), (7, 2), (4, 3), (5, 5), (8, 2), (8, 5), (5, 17), (7, 18), (6, 3), (5, 8), (7, 2), (6, 15), (5, 5)]\n","Test Loss: 8.977377\n","\n","Mean Abs Loss: 4.866667\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5GSpw9ExoD6B","colab_type":"text"},"source":["## Deluxe Mixed Model"]},{"cell_type":"code","metadata":{"id":"98-Fe0zNWv-v","colab_type":"code","colab":{}},"source":["def train(model, criterion, optimizer, epochs):\n","    # training params\n","\n","    #epochs = 00 # 3-4 is approx where I noticed the validation loss stop decreasing\n","\n","    counter = 0\n","    print_every = 10\n","    clip = 8 # gradient clipping\n","\n","    # move model to GPU, if available\n","    if(train_on_gpu):\n","        model.cuda()\n","\n","    model.train()\n","    # train for some number of epochs\n","    valid_loss_min = np.Inf\n","    train_loss_min = np.Inf\n","\n","    best_train_epoch = 0\n","    best_valid_epoch = 0\n","\n","    for epoch in range(1, epochs + 1):\n","        # initialize hidden state\n","        h = model.init_hidden(batch_size)\n","\n","        train_loss = 0.0\n","        valid_loss = 0.0\n","\n","        model.train()\n","\n","        # batch loop\n","        for X_temp, X_num, X_cat, labels in train_loader:\n","            counter += 1\n","\n","            if(train_on_gpu):\n","                X_temp, X_num, X_cat, labels = X_temp.cuda(), X_num.cuda(), X_cat.type(torch.LongTensor).cuda(), labels.cuda()\n","\n","            # Creating new variables for the hidden state, otherwise\n","            # we'd backprop through the entire training history\n","            h = tuple([each.data for each in h])\n","            # zero accumulated gradients\n","            model.zero_grad()\n","\n","            # get the output from the model\n","            output, h = model(X_temp, h, X_num, X_cat)\n","\n","            # calculate the loss and perform backprop\n","            loss = criterion(output.squeeze(), labels.double())\n","            loss.backward()\n","            train_loss += loss.item()\n","            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","            nn.utils.clip_grad_norm_(model.parameters(), clip)\n","            optimizer.step()\n","\n","        if train_loss <= train_loss_min:\n","            torch.save(model.state_dict(), 'train_mixed_model2.pt')\n","            train_loss_min = train_loss\n","            best_train_epoch = epoch\n","\n","        model.eval()\n","\n","        val_h = model.init_hidden(batch_size)\n","\n","        for X_temp, X_num, X_cat, labels in valid_loader:\n","            \n","            \n","            if(train_on_gpu):\n","                X_temp, X_num, X_cat, labels = X_temp.cuda(), X_num.cuda(), X_cat.type(torch.LongTensor).cuda(), labels.cuda()\n","\n","            # Creating new variables for the hidden state, otherwise\n","            # we'd backprop through the entire training history\n","            val_h = tuple([each.data for each in val_h])\n","\n","            # get the output from the model\n","            output, h = model(X_temp, h, X_num, X_cat)\n","\n","            # calculate the loss and perform backprop\n","            loss = criterion(output.squeeze(), labels.double())\n","            valid_loss += loss.item()\n","\n","        # save model if validation loss has decreased\n","        if valid_loss <= valid_loss_min:\n","            torch.save(model.state_dict(), 'valid_mixed_model2.pt')\n","            valid_loss_min = valid_loss\n","            best_valid_epoch = epoch\n","            \n","        if epoch % 50 == 0:\n","            print('  Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n","                epoch, train_loss, valid_loss))\n","            \n","    print(\"Best Validation Loss: {:.6f} in epoch {}\".format(\n","        valid_loss_min, str(best_valid_epoch)\n","    ))\n","\n","    print(\"Best Train Loss: {:.6f} in epoch {}\".format(\n","        train_loss_min, str(best_train_epoch)\n","    ))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CNlwIrKlXixG","colab_type":"code","colab":{}},"source":["def test(model, criterion):\n","    # track test loss\n","    test_loss = 0.0\n","    mean_abs = 0.0\n","\n","    model.eval()\n","    # iterate over test data\n","    #bin_op.binarization()\n","    test_h = model.init_hidden(batch_size)\n","\n","    for batch_idx, (X_temp, X_num, X_cat, labels) in enumerate(test_loader):\n","        if(train_on_gpu):\n","            X_temp, X_num, X_cat, labels = X_temp.cuda(), X_num.cuda(), X_cat.type(torch.LongTensor).cuda(), labels.cuda()\n","        \n","        test_h = tuple([each.data for each in test_h])\n","\n","        # get the output from the model\n","        output, test_h = model(X_temp, test_h, X_num, X_cat)\n","        # / (weight_lstm + weight_num + weight_cat)\n","\n","        # calculate the loss and perform backprop\n","        loss = criterion(output.squeeze(), labels.double())\n","\n","        out_np = output.detach().squeeze().cpu().numpy().astype(int).T\n","        target_np = labels.detach().cpu().numpy().astype(int).T\n","\n","        #print([(out, tar) for out, tar in zip(out_np, target_np)])\n","\n","        mean_abs += mean_absolute_error(out_np, target_np) * batch_size\n","            \n","        test_loss += loss.item() * batch_size\n","\n","    # calculate average losses\n","    test_loss = np.sqrt(test_loss / len(test_loader.dataset))\n","    mean_abs /= len(test_loader.dataset)\n","    print('  Test Loss: {:.6f}\\n'.format(test_loss))\n","    print('  Mean Abs Loss: {:.6f}\\n'.format(mean_abs))\n","\n","    return mean_abs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"48KzkMyMXrt7","colab_type":"code","outputId":"7d376a67-91ec-4b4b-954a-9d9f0856f1f7","executionInfo":{"status":"ok","timestamp":1587025806875,"user_tz":300,"elapsed":14320415,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["learning_rates = [0.001, 0.0001]\n","a_hs = [2, 3, 4]\n","num_hidden_dims = [10, 15, 20]\n","cat_hidden_dims = [10, 15, 20]\n","lstm_outputs = [5, 10, 15]\n","combined_dims = [40]\n","\n","criterion = nn.MSELoss()\n","lstm_n_layers = 2\n","\n","#overall_test_square_error = np.Inf\n","overall_test_mean_error = np.Inf\n","\n","for combined_dim in combined_dims:\n","    for lstm_output in lstm_outputs:\n","        for cat_hidden_dim in cat_hidden_dims:\n","            for num_hidden_dim in num_hidden_dims:\n","                for a_h in a_hs:\n","                    for lr in learning_rates:\n","                        lstm_hidden_dim = np.int(len(train_idx) / (a_h * len(numerical_columns) + 1))\n","                        \n","                        net = MosquitoMixed(\n","                            len(numerical_columns), lstm_hidden_dim, lstm_n_layers, lstm_output,\n","                            len(non_num_columns), num_hidden_dim,\n","                            embedding_sizes, cat_hidden_dim,\n","                            combined_dim\n","                        ).double()\n","\n","                        \n","                        epochs = 300\n","\n","                        optimizer = torch.optim.RMSprop(net.parameters(), lr=lr)\n","\n","                        print('\\nCombined Dimension: ' + str(combined_dim))\n","                        print('LSTM Output Dimension: ' + str(lstm_output))\n","                        print('Categorical Dimension: ' + str(cat_hidden_dim))\n","                        print('Numerical Dimension: ' + str(num_hidden_dim))\n","                        print('LSTM Dimension: ' + str(lstm_hidden_dim))\n","                        print('Learning Rate: ' + str(lr))\n","                        \n","                        train(net, criterion, optimizer, epochs)\n","\n","                        net.load_state_dict(torch.load('train_mixed_model2.pt'))\n","\n","                        otme = test(net, criterion)\n","                        \n","\n","                        if otme <= overall_test_mean_error:\n","                            print(' Test loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","                            overall_test_mean_error,\n","                            otme))\n","                            torch.save(net.state_dict(), 'best_mixed_model2.pt')\n","                            overall_test_mean_error = otme\n","\n","                        net.load_state_dict(torch.load('valid_mixed_model2.pt'))\n","\n","                        otme = test(net, criterion)\n","\n","                        if otme <= overall_test_mean_error:\n","                            print(' Test loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","                            overall_test_mean_error,\n","                            otme))\n","                            torch.save(net.state_dict(), 'best_mixed_model2.pt')\n","                            overall_test_mean_error = otme\n","\n","                        \n","\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3822.925468 \tValidation Loss: 1009.383622\n","  Epoch: 100 \tTraining Loss: 3817.743481 \tValidation Loss: 983.396817\n","  Epoch: 150 \tTraining Loss: 3645.321342 \tValidation Loss: 988.775240\n","  Epoch: 200 \tTraining Loss: 3676.005698 \tValidation Loss: 610.692957\n","  Epoch: 250 \tTraining Loss: 3469.527166 \tValidation Loss: 998.100238\n","  Epoch: 300 \tTraining Loss: 3594.455835 \tValidation Loss: 908.388196\n","Best Validation Loss: 566.613660 in epoch 223\n","Best Train Loss: 2771.707119 in epoch 246\n","  Test Loss: 8.870554\n","\n","  Mean Abs Loss: 4.695238\n","\n"," Test loss decreased (inf --> 4.695238).  Saving model ...\n","  Test Loss: 8.829969\n","\n","  Mean Abs Loss: 4.752381\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4722.988360 \tValidation Loss: 1192.735820\n","  Epoch: 100 \tTraining Loss: 4224.571840 \tValidation Loss: 1079.070751\n","  Epoch: 150 \tTraining Loss: 4050.387671 \tValidation Loss: 1054.485208\n","  Epoch: 200 \tTraining Loss: 4059.128295 \tValidation Loss: 1050.182345\n","  Epoch: 250 \tTraining Loss: 3962.963773 \tValidation Loss: 1049.152169\n","  Epoch: 300 \tTraining Loss: 3933.955721 \tValidation Loss: 1031.406817\n","Best Validation Loss: 609.778485 in epoch 105\n","Best Train Loss: 3579.363921 in epoch 197\n","  Test Loss: 9.213162\n","\n","  Mean Abs Loss: 5.038095\n","\n","  Test Loss: 9.384985\n","\n","  Mean Abs Loss: 5.219048\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3796.389664 \tValidation Loss: 1012.039914\n","  Epoch: 100 \tTraining Loss: 3775.354090 \tValidation Loss: 980.804754\n","  Epoch: 150 \tTraining Loss: 3633.661882 \tValidation Loss: 954.444034\n","  Epoch: 200 \tTraining Loss: 3636.126962 \tValidation Loss: 955.808975\n","  Epoch: 250 \tTraining Loss: 3474.975220 \tValidation Loss: 936.765231\n","  Epoch: 300 \tTraining Loss: 3452.867266 \tValidation Loss: 934.526587\n","Best Validation Loss: 536.389296 in epoch 216\n","Best Train Loss: 2761.302234 in epoch 152\n","  Test Loss: 8.631638\n","\n","  Mean Abs Loss: 4.704762\n","\n","  Test Loss: 8.639550\n","\n","  Mean Abs Loss: 4.809524\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4560.328133 \tValidation Loss: 1196.133964\n","  Epoch: 100 \tTraining Loss: 4239.747539 \tValidation Loss: 905.606431\n","  Epoch: 150 \tTraining Loss: 4079.486481 \tValidation Loss: 1050.844759\n","  Epoch: 200 \tTraining Loss: 4052.712357 \tValidation Loss: 1019.805168\n","  Epoch: 250 \tTraining Loss: 3854.148487 \tValidation Loss: 911.955353\n","  Epoch: 300 \tTraining Loss: 3943.346979 \tValidation Loss: 840.400403\n","Best Validation Loss: 524.761056 in epoch 73\n","Best Train Loss: 3179.411178 in epoch 167\n","  Test Loss: 8.773616\n","\n","  Mean Abs Loss: 4.685714\n","\n"," Test loss decreased (4.695238 --> 4.685714).  Saving model ...\n","  Test Loss: 9.491656\n","\n","  Mean Abs Loss: 5.504762\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3812.074525 \tValidation Loss: 997.881879\n","  Epoch: 100 \tTraining Loss: 3751.807337 \tValidation Loss: 998.900467\n","  Epoch: 150 \tTraining Loss: 3642.268409 \tValidation Loss: 995.128890\n","  Epoch: 200 \tTraining Loss: 3712.582870 \tValidation Loss: 1016.296896\n","  Epoch: 250 \tTraining Loss: 3708.676353 \tValidation Loss: 988.199854\n","  Epoch: 300 \tTraining Loss: 3650.338760 \tValidation Loss: 785.964261\n","Best Validation Loss: 603.804280 in epoch 240\n","Best Train Loss: 2793.760318 in epoch 223\n","  Test Loss: 8.711863\n","\n","  Mean Abs Loss: 4.761905\n","\n","  Test Loss: 8.680422\n","\n","  Mean Abs Loss: 4.780952\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4605.142548 \tValidation Loss: 1275.286506\n","  Epoch: 100 \tTraining Loss: 3361.691067 \tValidation Loss: 1151.494025\n","  Epoch: 150 \tTraining Loss: 3973.381065 \tValidation Loss: 1118.078681\n","  Epoch: 200 \tTraining Loss: 4072.121241 \tValidation Loss: 1102.645590\n","  Epoch: 250 \tTraining Loss: 3999.249532 \tValidation Loss: 1061.105648\n","  Epoch: 300 \tTraining Loss: 3914.539705 \tValidation Loss: 1040.250986\n","Best Validation Loss: 670.586047 in epoch 281\n","Best Train Loss: 3180.848340 in epoch 248\n","  Test Loss: 9.266223\n","\n","  Mean Abs Loss: 5.257143\n","\n","  Test Loss: 9.056984\n","\n","  Mean Abs Loss: 5.247619\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3807.209541 \tValidation Loss: 1005.004331\n","  Epoch: 100 \tTraining Loss: 3585.595372 \tValidation Loss: 958.019949\n","  Epoch: 150 \tTraining Loss: 3583.160190 \tValidation Loss: 947.885515\n","  Epoch: 200 \tTraining Loss: 3568.628816 \tValidation Loss: 942.556469\n","  Epoch: 250 \tTraining Loss: 3512.282543 \tValidation Loss: 946.110236\n","  Epoch: 300 \tTraining Loss: 3500.461524 \tValidation Loss: 932.036475\n","Best Validation Loss: 531.587665 in epoch 67\n","Best Train Loss: 2713.322651 in epoch 269\n","  Test Loss: 8.730891\n","\n","  Mean Abs Loss: 4.847619\n","\n","  Test Loss: 8.897490\n","\n","  Mean Abs Loss: 5.057143\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4456.666701 \tValidation Loss: 973.094655\n","  Epoch: 100 \tTraining Loss: 4154.411427 \tValidation Loss: 1073.999041\n","  Epoch: 150 \tTraining Loss: 3947.708527 \tValidation Loss: 1066.156562\n","  Epoch: 200 \tTraining Loss: 3935.266955 \tValidation Loss: 1058.423273\n","  Epoch: 250 \tTraining Loss: 3918.632851 \tValidation Loss: 1051.067245\n","  Epoch: 300 \tTraining Loss: 3779.925930 \tValidation Loss: 1034.800755\n","Best Validation Loss: 496.474052 in epoch 82\n","Best Train Loss: 2988.813551 in epoch 252\n","  Test Loss: 8.697952\n","\n","  Mean Abs Loss: 4.733333\n","\n","  Test Loss: 8.571061\n","\n","  Mean Abs Loss: 4.742857\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3826.234997 \tValidation Loss: 1035.684796\n","  Epoch: 100 \tTraining Loss: 3825.305977 \tValidation Loss: 1013.420634\n","  Epoch: 150 \tTraining Loss: 3772.172845 \tValidation Loss: 981.653504\n","  Epoch: 200 \tTraining Loss: 3708.290728 \tValidation Loss: 972.339527\n","  Epoch: 250 \tTraining Loss: 3601.796134 \tValidation Loss: 980.880163\n","  Epoch: 300 \tTraining Loss: 3482.686162 \tValidation Loss: 977.712793\n","Best Validation Loss: 591.770944 in epoch 104\n","Best Train Loss: 2806.585508 in epoch 244\n","  Test Loss: 8.489195\n","\n","  Mean Abs Loss: 4.647619\n","\n"," Test loss decreased (4.685714 --> 4.647619).  Saving model ...\n","  Test Loss: 8.540882\n","\n","  Mean Abs Loss: 4.780952\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4558.579307 \tValidation Loss: 1263.425836\n","  Epoch: 100 \tTraining Loss: 4253.597105 \tValidation Loss: 1178.051851\n","  Epoch: 150 \tTraining Loss: 4054.285383 \tValidation Loss: 1116.441112\n","  Epoch: 200 \tTraining Loss: 3905.087689 \tValidation Loss: 1088.085206\n","  Epoch: 250 \tTraining Loss: 3873.753912 \tValidation Loss: 1149.281696\n","  Epoch: 300 \tTraining Loss: 4001.182913 \tValidation Loss: 1075.524887\n","Best Validation Loss: 673.063077 in epoch 219\n","Best Train Loss: 3059.063845 in epoch 210\n","  Test Loss: 9.409262\n","\n","  Mean Abs Loss: 5.361905\n","\n","  Test Loss: 9.692289\n","\n","  Mean Abs Loss: 5.561905\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3988.942580 \tValidation Loss: 979.655683\n","  Epoch: 100 \tTraining Loss: 3814.663466 \tValidation Loss: 999.750954\n","  Epoch: 150 \tTraining Loss: 3684.870753 \tValidation Loss: 972.652275\n","  Epoch: 200 \tTraining Loss: 3583.847822 \tValidation Loss: 976.307698\n","  Epoch: 250 \tTraining Loss: 3631.343179 \tValidation Loss: 964.358477\n","  Epoch: 300 \tTraining Loss: 3591.390854 \tValidation Loss: 967.918573\n","Best Validation Loss: 557.181329 in epoch 81\n","Best Train Loss: 3193.152394 in epoch 266\n","  Test Loss: 8.676283\n","\n","  Mean Abs Loss: 4.790476\n","\n","  Test Loss: 9.073190\n","\n","  Mean Abs Loss: 5.333333\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4584.363761 \tValidation Loss: 1224.565271\n","  Epoch: 100 \tTraining Loss: 4127.910441 \tValidation Loss: 1129.581631\n","  Epoch: 150 \tTraining Loss: 3994.840614 \tValidation Loss: 1100.139054\n","  Epoch: 200 \tTraining Loss: 3976.865933 \tValidation Loss: 1089.460459\n","  Epoch: 250 \tTraining Loss: 3930.133234 \tValidation Loss: 668.134881\n","  Epoch: 300 \tTraining Loss: 3934.279493 \tValidation Loss: 1049.414158\n","Best Validation Loss: 547.851002 in epoch 287\n","Best Train Loss: 3685.769061 in epoch 167\n","  Test Loss: 9.062054\n","\n","  Mean Abs Loss: 5.257143\n","\n","  Test Loss: 8.975783\n","\n","  Mean Abs Loss: 5.076190\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3824.695228 \tValidation Loss: 1020.467281\n","  Epoch: 100 \tTraining Loss: 3774.364072 \tValidation Loss: 990.290219\n","  Epoch: 150 \tTraining Loss: 3548.073637 \tValidation Loss: 954.403770\n","  Epoch: 200 \tTraining Loss: 3647.207684 \tValidation Loss: 868.157542\n","  Epoch: 250 \tTraining Loss: 3500.906181 \tValidation Loss: 962.117128\n","  Epoch: 300 \tTraining Loss: 3313.052159 \tValidation Loss: 935.374299\n","Best Validation Loss: 583.211496 in epoch 174\n","Best Train Loss: 2981.182752 in epoch 44\n","  Test Loss: 8.640276\n","\n","  Mean Abs Loss: 4.752381\n","\n","  Test Loss: 8.585840\n","\n","  Mean Abs Loss: 4.647619\n","\n"," Test loss decreased (4.647619 --> 4.647619).  Saving model ...\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4676.325804 \tValidation Loss: 1132.124770\n","  Epoch: 100 \tTraining Loss: 4285.300070 \tValidation Loss: 1135.861460\n","  Epoch: 150 \tTraining Loss: 4148.071196 \tValidation Loss: 1095.351929\n","  Epoch: 200 \tTraining Loss: 4046.798245 \tValidation Loss: 1063.932215\n","  Epoch: 250 \tTraining Loss: 3977.401859 \tValidation Loss: 1065.429913\n","  Epoch: 300 \tTraining Loss: 3966.703674 \tValidation Loss: 1028.985885\n","Best Validation Loss: 675.958348 in epoch 193\n","Best Train Loss: 3110.560502 in epoch 229\n","  Test Loss: 9.202434\n","\n","  Mean Abs Loss: 4.961905\n","\n","  Test Loss: 9.556775\n","\n","  Mean Abs Loss: 5.314286\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3868.029173 \tValidation Loss: 1022.711959\n","  Epoch: 100 \tTraining Loss: 3692.691250 \tValidation Loss: 979.422854\n","  Epoch: 150 \tTraining Loss: 3721.484713 \tValidation Loss: 622.347597\n","  Epoch: 200 \tTraining Loss: 3621.789330 \tValidation Loss: 970.170507\n","  Epoch: 250 \tTraining Loss: 3548.204666 \tValidation Loss: 965.840161\n","  Epoch: 300 \tTraining Loss: 3598.202435 \tValidation Loss: 938.818544\n","Best Validation Loss: 591.844175 in epoch 253\n","Best Train Loss: 2754.595200 in epoch 282\n","  Test Loss: 8.689333\n","\n","  Mean Abs Loss: 4.723810\n","\n","  Test Loss: 8.693337\n","\n","  Mean Abs Loss: 4.771429\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4282.589536 \tValidation Loss: 1184.413870\n","  Epoch: 100 \tTraining Loss: 4063.659377 \tValidation Loss: 1117.619105\n","  Epoch: 150 \tTraining Loss: 3881.168991 \tValidation Loss: 1043.576202\n","  Epoch: 200 \tTraining Loss: 3853.696592 \tValidation Loss: 1051.579088\n","  Epoch: 250 \tTraining Loss: 4035.918139 \tValidation Loss: 1032.426973\n","  Epoch: 300 \tTraining Loss: 3798.030957 \tValidation Loss: 1023.761259\n","Best Validation Loss: 640.371069 in epoch 161\n","Best Train Loss: 2833.619355 in epoch 254\n","  Test Loss: 9.039483\n","\n","  Mean Abs Loss: 5.142857\n","\n","  Test Loss: 8.777490\n","\n","  Mean Abs Loss: 5.085714\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3816.583128 \tValidation Loss: 999.544815\n","  Epoch: 100 \tTraining Loss: 3842.255819 \tValidation Loss: 966.563449\n","  Epoch: 150 \tTraining Loss: 3622.042007 \tValidation Loss: 955.193387\n","  Epoch: 200 \tTraining Loss: 3609.689495 \tValidation Loss: 954.935377\n","  Epoch: 250 \tTraining Loss: 3593.187960 \tValidation Loss: 953.993469\n","  Epoch: 300 \tTraining Loss: 3604.191852 \tValidation Loss: 960.691146\n","Best Validation Loss: 560.754138 in epoch 266\n","Best Train Loss: 2807.092481 in epoch 225\n","  Test Loss: 8.591594\n","\n","  Mean Abs Loss: 4.666667\n","\n","  Test Loss: 8.673419\n","\n","  Mean Abs Loss: 4.590476\n","\n"," Test loss decreased (4.647619 --> 4.590476).  Saving model ...\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4724.082509 \tValidation Loss: 1333.820050\n","  Epoch: 100 \tTraining Loss: 4213.186286 \tValidation Loss: 1137.346546\n","  Epoch: 150 \tTraining Loss: 4144.983115 \tValidation Loss: 1151.402877\n","  Epoch: 200 \tTraining Loss: 4017.212550 \tValidation Loss: 1121.865170\n","  Epoch: 250 \tTraining Loss: 3949.176346 \tValidation Loss: 1124.206445\n","  Epoch: 300 \tTraining Loss: 4024.494807 \tValidation Loss: 885.558673\n","Best Validation Loss: 510.721888 in epoch 223\n","Best Train Loss: 3313.088373 in epoch 142\n","  Test Loss: 9.918289\n","\n","  Mean Abs Loss: 5.723810\n","\n","  Test Loss: 9.485746\n","\n","  Mean Abs Loss: 5.285714\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3893.053984 \tValidation Loss: 1030.773108\n","  Epoch: 100 \tTraining Loss: 3814.901231 \tValidation Loss: 1013.236481\n","  Epoch: 150 \tTraining Loss: 3737.422009 \tValidation Loss: 969.429174\n","  Epoch: 200 \tTraining Loss: 3614.615674 \tValidation Loss: 954.449849\n","  Epoch: 250 \tTraining Loss: 3410.719052 \tValidation Loss: 902.016007\n","  Epoch: 300 \tTraining Loss: 3559.017280 \tValidation Loss: 398.974147\n","Best Validation Loss: 398.974147 in epoch 300\n","Best Train Loss: 2854.785376 in epoch 141\n","  Test Loss: 8.950199\n","\n","  Mean Abs Loss: 4.800000\n","\n","  Test Loss: 8.834425\n","\n","  Mean Abs Loss: 4.876190\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4673.268752 \tValidation Loss: 1295.503244\n","  Epoch: 100 \tTraining Loss: 4228.794629 \tValidation Loss: 1177.998615\n","  Epoch: 150 \tTraining Loss: 4224.328835 \tValidation Loss: 1140.529728\n","  Epoch: 200 \tTraining Loss: 4014.207410 \tValidation Loss: 1098.149407\n","  Epoch: 250 \tTraining Loss: 4058.070971 \tValidation Loss: 1109.962096\n","  Epoch: 300 \tTraining Loss: 3896.022411 \tValidation Loss: 1105.832857\n","Best Validation Loss: 697.426277 in epoch 147\n","Best Train Loss: 3026.764868 in epoch 291\n","  Test Loss: 9.368395\n","\n","  Mean Abs Loss: 5.123810\n","\n","  Test Loss: 9.656105\n","\n","  Mean Abs Loss: 5.466667\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3816.823636 \tValidation Loss: 1037.718820\n","  Epoch: 100 \tTraining Loss: 3769.868319 \tValidation Loss: 1017.698622\n","  Epoch: 150 \tTraining Loss: 3718.007933 \tValidation Loss: 989.972745\n","  Epoch: 200 \tTraining Loss: 3590.259187 \tValidation Loss: 975.938998\n","  Epoch: 250 \tTraining Loss: 3568.660700 \tValidation Loss: 969.699408\n","  Epoch: 300 \tTraining Loss: 3542.576222 \tValidation Loss: 746.814732\n","Best Validation Loss: 429.500456 in epoch 109\n","Best Train Loss: 2927.151185 in epoch 111\n","  Test Loss: 9.529608\n","\n","  Mean Abs Loss: 5.333333\n","\n","  Test Loss: 9.589998\n","\n","  Mean Abs Loss: 5.371429\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4502.411731 \tValidation Loss: 1179.060685\n","  Epoch: 100 \tTraining Loss: 4009.077549 \tValidation Loss: 1098.466006\n","  Epoch: 150 \tTraining Loss: 4047.232957 \tValidation Loss: 893.723329\n","  Epoch: 200 \tTraining Loss: 3888.874938 \tValidation Loss: 1072.486710\n","  Epoch: 250 \tTraining Loss: 3911.231528 \tValidation Loss: 878.712255\n","  Epoch: 300 \tTraining Loss: 3809.627205 \tValidation Loss: 1056.490101\n","Best Validation Loss: 488.779206 in epoch 137\n","Best Train Loss: 3027.671288 in epoch 63\n","  Test Loss: 9.842726\n","\n","  Mean Abs Loss: 5.542857\n","\n","  Test Loss: 9.705865\n","\n","  Mean Abs Loss: 5.390476\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3946.626435 \tValidation Loss: 1016.456075\n","  Epoch: 100 \tTraining Loss: 3751.916546 \tValidation Loss: 987.314783\n","  Epoch: 150 \tTraining Loss: 3910.610207 \tValidation Loss: 859.522754\n","  Epoch: 200 \tTraining Loss: 3641.410610 \tValidation Loss: 967.500495\n","  Epoch: 250 \tTraining Loss: 3602.944350 \tValidation Loss: 970.490328\n","  Epoch: 300 \tTraining Loss: 3542.140708 \tValidation Loss: 948.498563\n","Best Validation Loss: 593.500976 in epoch 175\n","Best Train Loss: 2850.580225 in epoch 167\n","  Test Loss: 8.701417\n","\n","  Mean Abs Loss: 4.695238\n","\n","  Test Loss: 8.840718\n","\n","  Mean Abs Loss: 4.752381\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4571.723405 \tValidation Loss: 1282.139323\n","  Epoch: 100 \tTraining Loss: 4314.722189 \tValidation Loss: 1103.754813\n","  Epoch: 150 \tTraining Loss: 4397.059751 \tValidation Loss: 1078.806217\n","  Epoch: 200 \tTraining Loss: 4167.772902 \tValidation Loss: 1041.406757\n","  Epoch: 250 \tTraining Loss: 4108.472087 \tValidation Loss: 1050.155873\n","  Epoch: 300 \tTraining Loss: 4020.475666 \tValidation Loss: 1045.824703\n","Best Validation Loss: 619.758179 in epoch 235\n","Best Train Loss: 3753.255884 in epoch 162\n","  Test Loss: 8.926383\n","\n","  Mean Abs Loss: 4.914286\n","\n","  Test Loss: 8.899496\n","\n","  Mean Abs Loss: 4.828571\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3773.327841 \tValidation Loss: 1026.595509\n","  Epoch: 100 \tTraining Loss: 3681.986907 \tValidation Loss: 1010.795389\n","  Epoch: 150 \tTraining Loss: 3504.086732 \tValidation Loss: 977.364779\n","  Epoch: 200 \tTraining Loss: 3686.392845 \tValidation Loss: 758.516665\n","  Epoch: 250 \tTraining Loss: 3583.869991 \tValidation Loss: 943.476274\n","  Epoch: 300 \tTraining Loss: 3384.501405 \tValidation Loss: 976.835688\n","Best Validation Loss: 529.406800 in epoch 102\n","Best Train Loss: 2755.281417 in epoch 218\n","  Test Loss: 8.688759\n","\n","  Mean Abs Loss: 4.790476\n","\n","  Test Loss: 8.729792\n","\n","  Mean Abs Loss: 4.923810\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4533.048420 \tValidation Loss: 1148.019369\n","  Epoch: 100 \tTraining Loss: 4019.488074 \tValidation Loss: 1053.383625\n","  Epoch: 150 \tTraining Loss: 3938.355036 \tValidation Loss: 1050.526962\n","  Epoch: 200 \tTraining Loss: 3648.883808 \tValidation Loss: 1010.384966\n","  Epoch: 250 \tTraining Loss: 3858.576916 \tValidation Loss: 1034.238326\n","  Epoch: 300 \tTraining Loss: 3785.041728 \tValidation Loss: 1008.403207\n","Best Validation Loss: 544.575806 in epoch 186\n","Best Train Loss: 3023.856873 in epoch 264\n","  Test Loss: 9.485231\n","\n","  Mean Abs Loss: 5.228571\n","\n","  Test Loss: 9.533977\n","\n","  Mean Abs Loss: 5.314286\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3832.774778 \tValidation Loss: 1034.313647\n","  Epoch: 100 \tTraining Loss: 3813.465278 \tValidation Loss: 1037.104980\n","  Epoch: 150 \tTraining Loss: 3678.965281 \tValidation Loss: 976.518058\n","  Epoch: 200 \tTraining Loss: 3724.721752 \tValidation Loss: 965.963910\n","  Epoch: 250 \tTraining Loss: 3623.276614 \tValidation Loss: 987.627136\n","  Epoch: 300 \tTraining Loss: 3580.602351 \tValidation Loss: 939.941960\n","Best Validation Loss: 570.144106 in epoch 157\n","Best Train Loss: 2946.723814 in epoch 111\n","  Test Loss: 9.444604\n","\n","  Mean Abs Loss: 5.295238\n","\n","  Test Loss: 8.723831\n","\n","  Mean Abs Loss: 4.742857\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4655.403903 \tValidation Loss: 1274.109699\n","  Epoch: 100 \tTraining Loss: 4227.483897 \tValidation Loss: 1119.869669\n","  Epoch: 150 \tTraining Loss: 4102.907638 \tValidation Loss: 1092.640529\n","  Epoch: 200 \tTraining Loss: 3978.195337 \tValidation Loss: 1099.772993\n","  Epoch: 250 \tTraining Loss: 3829.241678 \tValidation Loss: 1094.299328\n","  Epoch: 300 \tTraining Loss: 3935.714394 \tValidation Loss: 1070.214097\n","Best Validation Loss: 577.340153 in epoch 134\n","Best Train Loss: 3067.813823 in epoch 229\n","  Test Loss: 8.865939\n","\n","  Mean Abs Loss: 4.819048\n","\n","  Test Loss: 9.074325\n","\n","  Mean Abs Loss: 4.885714\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3523.817565 \tValidation Loss: 1020.875533\n","  Epoch: 100 \tTraining Loss: 3712.082261 \tValidation Loss: 997.159993\n","  Epoch: 150 \tTraining Loss: 3688.335738 \tValidation Loss: 956.074677\n","  Epoch: 200 \tTraining Loss: 3552.377233 \tValidation Loss: 950.199258\n","  Epoch: 250 \tTraining Loss: 3580.562134 \tValidation Loss: 948.276462\n","  Epoch: 300 \tTraining Loss: 3543.412317 \tValidation Loss: 940.549550\n","Best Validation Loss: 478.636269 in epoch 268\n","Best Train Loss: 2690.672009 in epoch 230\n","  Test Loss: 8.717229\n","\n","  Mean Abs Loss: 4.752381\n","\n","  Test Loss: 8.578224\n","\n","  Mean Abs Loss: 4.571429\n","\n"," Test loss decreased (4.590476 --> 4.571429).  Saving model ...\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4515.540331 \tValidation Loss: 806.268763\n","  Epoch: 100 \tTraining Loss: 4092.540071 \tValidation Loss: 1085.220109\n","  Epoch: 150 \tTraining Loss: 4078.705404 \tValidation Loss: 1075.677359\n","  Epoch: 200 \tTraining Loss: 3891.906546 \tValidation Loss: 1061.195366\n","  Epoch: 250 \tTraining Loss: 4025.569222 \tValidation Loss: 1062.503981\n","  Epoch: 300 \tTraining Loss: 3937.988766 \tValidation Loss: 1024.511204\n","Best Validation Loss: 570.622023 in epoch 137\n","Best Train Loss: 3161.463095 in epoch 129\n","  Test Loss: 8.984284\n","\n","  Mean Abs Loss: 4.800000\n","\n","  Test Loss: 9.042712\n","\n","  Mean Abs Loss: 4.790476\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3829.414611 \tValidation Loss: 981.145171\n","  Epoch: 100 \tTraining Loss: 3591.653743 \tValidation Loss: 965.993741\n","  Epoch: 150 \tTraining Loss: 3697.376377 \tValidation Loss: 831.158163\n","  Epoch: 200 \tTraining Loss: 3505.001474 \tValidation Loss: 937.988620\n","  Epoch: 250 \tTraining Loss: 3460.345709 \tValidation Loss: 925.370314\n","  Epoch: 300 \tTraining Loss: 3484.408225 \tValidation Loss: 931.608730\n","Best Validation Loss: 568.518765 in epoch 282\n","Best Train Loss: 2526.845364 in epoch 290\n","  Test Loss: 8.847289\n","\n","  Mean Abs Loss: 4.800000\n","\n","  Test Loss: 8.839607\n","\n","  Mean Abs Loss: 4.895238\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4509.852592 \tValidation Loss: 1201.938433\n","  Epoch: 100 \tTraining Loss: 4253.644366 \tValidation Loss: 1111.582408\n","  Epoch: 150 \tTraining Loss: 3960.854914 \tValidation Loss: 1083.101394\n","  Epoch: 200 \tTraining Loss: 3963.740482 \tValidation Loss: 1093.201493\n","  Epoch: 250 \tTraining Loss: 3956.456733 \tValidation Loss: 1050.816852\n","  Epoch: 300 \tTraining Loss: 3862.318695 \tValidation Loss: 1045.790130\n","Best Validation Loss: 566.090708 in epoch 267\n","Best Train Loss: 2948.332080 in epoch 254\n","  Test Loss: 9.624191\n","\n","  Mean Abs Loss: 5.523810\n","\n","  Test Loss: 9.659660\n","\n","  Mean Abs Loss: 5.571429\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3763.323307 \tValidation Loss: 987.225669\n","  Epoch: 100 \tTraining Loss: 3739.864742 \tValidation Loss: 961.626462\n","  Epoch: 150 \tTraining Loss: 3628.077836 \tValidation Loss: 949.960105\n","  Epoch: 200 \tTraining Loss: 3651.978746 \tValidation Loss: 954.986950\n","  Epoch: 250 \tTraining Loss: 3527.846630 \tValidation Loss: 939.363156\n","  Epoch: 300 \tTraining Loss: 3429.656470 \tValidation Loss: 980.353177\n","Best Validation Loss: 461.230678 in epoch 16\n","Best Train Loss: 2880.287548 in epoch 109\n","  Test Loss: 8.807247\n","\n","  Mean Abs Loss: 4.819048\n","\n","  Test Loss: 8.652248\n","\n","  Mean Abs Loss: 4.704762\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4448.582570 \tValidation Loss: 1147.615477\n","  Epoch: 100 \tTraining Loss: 4025.365484 \tValidation Loss: 1094.254109\n","  Epoch: 150 \tTraining Loss: 3194.867416 \tValidation Loss: 1048.317513\n","  Epoch: 200 \tTraining Loss: 3795.779128 \tValidation Loss: 1044.120938\n","  Epoch: 250 \tTraining Loss: 3971.761589 \tValidation Loss: 1028.422749\n","  Epoch: 300 \tTraining Loss: 3853.991696 \tValidation Loss: 1044.934721\n","Best Validation Loss: 541.550862 in epoch 217\n","Best Train Loss: 2991.567477 in epoch 298\n","  Test Loss: 8.840652\n","\n","  Mean Abs Loss: 4.895238\n","\n","  Test Loss: 8.717139\n","\n","  Mean Abs Loss: 4.704762\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3894.533204 \tValidation Loss: 1007.846864\n","  Epoch: 100 \tTraining Loss: 3643.898659 \tValidation Loss: 991.801825\n","  Epoch: 150 \tTraining Loss: 3638.579906 \tValidation Loss: 968.771507\n","  Epoch: 200 \tTraining Loss: 3432.625521 \tValidation Loss: 965.315763\n","  Epoch: 250 \tTraining Loss: 3566.608908 \tValidation Loss: 932.426999\n","  Epoch: 300 \tTraining Loss: 3531.190078 \tValidation Loss: 938.034895\n","Best Validation Loss: 500.356773 in epoch 173\n","Best Train Loss: 2696.488219 in epoch 287\n","  Test Loss: 8.809638\n","\n","  Mean Abs Loss: 4.866667\n","\n","  Test Loss: 9.013807\n","\n","  Mean Abs Loss: 4.952381\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4499.527620 \tValidation Loss: 1178.660772\n","  Epoch: 100 \tTraining Loss: 4118.922519 \tValidation Loss: 1059.298156\n","  Epoch: 150 \tTraining Loss: 4019.487026 \tValidation Loss: 1067.711069\n","  Epoch: 200 \tTraining Loss: 3916.663117 \tValidation Loss: 1059.088314\n","  Epoch: 250 \tTraining Loss: 3905.703993 \tValidation Loss: 842.347683\n","  Epoch: 300 \tTraining Loss: 3630.399739 \tValidation Loss: 1016.692975\n","Best Validation Loss: 652.521595 in epoch 269\n","Best Train Loss: 2852.711362 in epoch 298\n","  Test Loss: 8.907179\n","\n","  Mean Abs Loss: 4.876190\n","\n","  Test Loss: 8.932887\n","\n","  Mean Abs Loss: 4.838095\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3950.987117 \tValidation Loss: 1012.571841\n","  Epoch: 100 \tTraining Loss: 3740.996147 \tValidation Loss: 1000.147606\n","  Epoch: 150 \tTraining Loss: 3716.825213 \tValidation Loss: 962.872768\n","  Epoch: 200 \tTraining Loss: 3597.289612 \tValidation Loss: 972.537164\n","  Epoch: 250 \tTraining Loss: 3563.861316 \tValidation Loss: 962.257201\n","  Epoch: 300 \tTraining Loss: 3574.559326 \tValidation Loss: 957.917383\n","Best Validation Loss: 562.729252 in epoch 234\n","Best Train Loss: 2812.456154 in epoch 204\n","  Test Loss: 8.693692\n","\n","  Mean Abs Loss: 4.771429\n","\n","  Test Loss: 8.588288\n","\n","  Mean Abs Loss: 4.819048\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4693.483260 \tValidation Loss: 1152.191408\n","  Epoch: 100 \tTraining Loss: 4141.979397 \tValidation Loss: 1054.427138\n","  Epoch: 150 \tTraining Loss: 4117.522910 \tValidation Loss: 1040.676485\n","  Epoch: 200 \tTraining Loss: 3975.906258 \tValidation Loss: 1029.654449\n","  Epoch: 250 \tTraining Loss: 3877.837950 \tValidation Loss: 1021.964473\n","  Epoch: 300 \tTraining Loss: 3893.670200 \tValidation Loss: 1010.581051\n","Best Validation Loss: 613.087853 in epoch 153\n","Best Train Loss: 2983.522907 in epoch 283\n","  Test Loss: 8.704964\n","\n","  Mean Abs Loss: 4.752381\n","\n","  Test Loss: 8.737465\n","\n","  Mean Abs Loss: 4.819048\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3893.064441 \tValidation Loss: 996.511220\n","  Epoch: 100 \tTraining Loss: 3859.807020 \tValidation Loss: 1008.486578\n","  Epoch: 150 \tTraining Loss: 3625.027763 \tValidation Loss: 968.977773\n","  Epoch: 200 \tTraining Loss: 3629.401940 \tValidation Loss: 938.455094\n","  Epoch: 250 \tTraining Loss: 3681.576188 \tValidation Loss: 571.227363\n","  Epoch: 300 \tTraining Loss: 3555.703935 \tValidation Loss: 946.913616\n","Best Validation Loss: 570.899503 in epoch 177\n","Best Train Loss: 2954.217678 in epoch 118\n","  Test Loss: 8.545488\n","\n","  Mean Abs Loss: 4.495238\n","\n"," Test loss decreased (4.571429 --> 4.495238).  Saving model ...\n","  Test Loss: 8.552046\n","\n","  Mean Abs Loss: 4.552381\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4627.650260 \tValidation Loss: 1255.794864\n","  Epoch: 100 \tTraining Loss: 4390.180660 \tValidation Loss: 1190.726384\n","  Epoch: 150 \tTraining Loss: 4237.341853 \tValidation Loss: 907.312426\n","  Epoch: 200 \tTraining Loss: 3722.214330 \tValidation Loss: 1055.089782\n","  Epoch: 250 \tTraining Loss: 3982.137909 \tValidation Loss: 996.921286\n","  Epoch: 300 \tTraining Loss: 4028.677251 \tValidation Loss: 840.225190\n","Best Validation Loss: 634.654499 in epoch 279\n","Best Train Loss: 3270.686600 in epoch 142\n","  Test Loss: 9.678710\n","\n","  Mean Abs Loss: 5.533333\n","\n","  Test Loss: 9.025978\n","\n","  Mean Abs Loss: 4.847619\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3724.493878 \tValidation Loss: 608.448964\n","  Epoch: 100 \tTraining Loss: 3879.777370 \tValidation Loss: 994.682924\n","  Epoch: 150 \tTraining Loss: 3682.769152 \tValidation Loss: 994.527282\n","  Epoch: 200 \tTraining Loss: 3724.971490 \tValidation Loss: 955.144513\n","  Epoch: 250 \tTraining Loss: 3712.939702 \tValidation Loss: 955.309014\n","  Epoch: 300 \tTraining Loss: 3779.854545 \tValidation Loss: 944.389498\n","Best Validation Loss: 408.253336 in epoch 287\n","Best Train Loss: 2986.698799 in epoch 201\n","  Test Loss: 8.863763\n","\n","  Mean Abs Loss: 4.780952\n","\n","  Test Loss: 8.817233\n","\n","  Mean Abs Loss: 4.790476\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4778.399828 \tValidation Loss: 1346.183486\n","  Epoch: 100 \tTraining Loss: 4217.606012 \tValidation Loss: 1045.801877\n","  Epoch: 150 \tTraining Loss: 4088.412810 \tValidation Loss: 1059.918060\n","  Epoch: 200 \tTraining Loss: 3955.061511 \tValidation Loss: 1067.218770\n","  Epoch: 250 \tTraining Loss: 4007.759065 \tValidation Loss: 1066.635039\n","  Epoch: 300 \tTraining Loss: 3901.698074 \tValidation Loss: 1052.101037\n","Best Validation Loss: 619.640018 in epoch 268\n","Best Train Loss: 3283.445508 in epoch 135\n","  Test Loss: 8.990007\n","\n","  Mean Abs Loss: 4.847619\n","\n","  Test Loss: 8.774787\n","\n","  Mean Abs Loss: 4.714286\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3875.180144 \tValidation Loss: 1001.520821\n","  Epoch: 100 \tTraining Loss: 3366.812956 \tValidation Loss: 880.460531\n","  Epoch: 150 \tTraining Loss: 3571.911100 \tValidation Loss: 899.746085\n","  Epoch: 200 \tTraining Loss: 3590.419508 \tValidation Loss: 920.527435\n","  Epoch: 250 \tTraining Loss: 3603.667705 \tValidation Loss: 945.440456\n","  Epoch: 300 \tTraining Loss: 3575.362497 \tValidation Loss: 953.082727\n","Best Validation Loss: 579.243626 in epoch 272\n","Best Train Loss: 2691.366536 in epoch 242\n","  Test Loss: 8.642135\n","\n","  Mean Abs Loss: 4.733333\n","\n","  Test Loss: 8.962448\n","\n","  Mean Abs Loss: 4.819048\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4680.085831 \tValidation Loss: 1240.273292\n","  Epoch: 100 \tTraining Loss: 4248.466613 \tValidation Loss: 1141.473311\n","  Epoch: 150 \tTraining Loss: 4049.411543 \tValidation Loss: 1078.153062\n","  Epoch: 200 \tTraining Loss: 3958.215912 \tValidation Loss: 856.578612\n","  Epoch: 250 \tTraining Loss: 4008.161964 \tValidation Loss: 1041.852532\n","  Epoch: 300 \tTraining Loss: 3965.103674 \tValidation Loss: 1031.541637\n","Best Validation Loss: 463.214309 in epoch 228\n","Best Train Loss: 3222.550029 in epoch 198\n","  Test Loss: 9.150285\n","\n","  Mean Abs Loss: 4.990476\n","\n","  Test Loss: 9.080150\n","\n","  Mean Abs Loss: 4.952381\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 4010.720786 \tValidation Loss: 994.484438\n","  Epoch: 100 \tTraining Loss: 3825.086695 \tValidation Loss: 946.442820\n","  Epoch: 150 \tTraining Loss: 3720.043065 \tValidation Loss: 955.426131\n","  Epoch: 200 \tTraining Loss: 3554.723054 \tValidation Loss: 949.670226\n","  Epoch: 250 \tTraining Loss: 3538.378029 \tValidation Loss: 941.474248\n","  Epoch: 300 \tTraining Loss: 3636.165958 \tValidation Loss: 948.883184\n","Best Validation Loss: 583.166229 in epoch 277\n","Best Train Loss: 2661.120820 in epoch 215\n","  Test Loss: 8.908132\n","\n","  Mean Abs Loss: 4.828571\n","\n","  Test Loss: 8.882075\n","\n","  Mean Abs Loss: 4.857143\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4365.911803 \tValidation Loss: 1198.131100\n","  Epoch: 100 \tTraining Loss: 4067.135321 \tValidation Loss: 1062.663224\n","  Epoch: 150 \tTraining Loss: 4014.325424 \tValidation Loss: 1070.041483\n","  Epoch: 200 \tTraining Loss: 3847.778303 \tValidation Loss: 1033.878683\n","  Epoch: 250 \tTraining Loss: 3970.838279 \tValidation Loss: 664.367486\n","  Epoch: 300 \tTraining Loss: 3926.089716 \tValidation Loss: 1029.682665\n","Best Validation Loss: 659.097505 in epoch 209\n","Best Train Loss: 3054.155454 in epoch 253\n","  Test Loss: 9.085614\n","\n","  Mean Abs Loss: 5.066667\n","\n","  Test Loss: 9.417980\n","\n","  Mean Abs Loss: 5.238095\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3890.574739 \tValidation Loss: 974.700698\n","  Epoch: 100 \tTraining Loss: 3707.835801 \tValidation Loss: 1004.788669\n","  Epoch: 150 \tTraining Loss: 3721.585025 \tValidation Loss: 868.860845\n","  Epoch: 200 \tTraining Loss: 3653.761349 \tValidation Loss: 977.946618\n","  Epoch: 250 \tTraining Loss: 3623.440874 \tValidation Loss: 975.977566\n","  Epoch: 300 \tTraining Loss: 3556.132726 \tValidation Loss: 979.184373\n","Best Validation Loss: 418.466391 in epoch 271\n","Best Train Loss: 2765.193509 in epoch 287\n","  Test Loss: 8.559180\n","\n","  Mean Abs Loss: 4.685714\n","\n","  Test Loss: 8.496468\n","\n","  Mean Abs Loss: 4.685714\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4175.900075 \tValidation Loss: 1137.595501\n","  Epoch: 100 \tTraining Loss: 3934.763390 \tValidation Loss: 1074.832670\n","  Epoch: 150 \tTraining Loss: 3822.220312 \tValidation Loss: 941.294708\n","  Epoch: 200 \tTraining Loss: 4024.395973 \tValidation Loss: 999.326456\n","  Epoch: 250 \tTraining Loss: 4168.792855 \tValidation Loss: 1042.745138\n","  Epoch: 300 \tTraining Loss: 3874.971014 \tValidation Loss: 979.603693\n","Best Validation Loss: 588.484590 in epoch 173\n","Best Train Loss: 3658.126490 in epoch 249\n","  Test Loss: 8.757289\n","\n","  Mean Abs Loss: 4.771429\n","\n","  Test Loss: 8.769592\n","\n","  Mean Abs Loss: 4.771429\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3823.093818 \tValidation Loss: 1064.466502\n","  Epoch: 100 \tTraining Loss: 3644.073624 \tValidation Loss: 799.268635\n","  Epoch: 150 \tTraining Loss: 3851.096802 \tValidation Loss: 622.211723\n","  Epoch: 200 \tTraining Loss: 3635.527383 \tValidation Loss: 954.395246\n","  Epoch: 250 \tTraining Loss: 3479.177745 \tValidation Loss: 983.721425\n","  Epoch: 300 \tTraining Loss: 3435.052374 \tValidation Loss: 850.701099\n","Best Validation Loss: 480.250260 in epoch 294\n","Best Train Loss: 2665.853288 in epoch 290\n","  Test Loss: 8.800918\n","\n","  Mean Abs Loss: 4.876190\n","\n","  Test Loss: 8.739635\n","\n","  Mean Abs Loss: 4.819048\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4587.083191 \tValidation Loss: 1154.391032\n","  Epoch: 100 \tTraining Loss: 4145.291444 \tValidation Loss: 1038.097359\n","  Epoch: 150 \tTraining Loss: 4052.636615 \tValidation Loss: 1037.373614\n","  Epoch: 200 \tTraining Loss: 3950.959907 \tValidation Loss: 1025.275646\n","  Epoch: 250 \tTraining Loss: 4003.150841 \tValidation Loss: 1031.544843\n","  Epoch: 300 \tTraining Loss: 3888.597316 \tValidation Loss: 1018.630065\n","Best Validation Loss: 450.732157 in epoch 102\n","Best Train Loss: 3031.230660 in epoch 205\n","  Test Loss: 8.804943\n","\n","  Mean Abs Loss: 4.866667\n","\n","  Test Loss: 8.807989\n","\n","  Mean Abs Loss: 4.847619\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3795.081087 \tValidation Loss: 1006.941254\n","  Epoch: 100 \tTraining Loss: 3846.227227 \tValidation Loss: 960.282687\n","  Epoch: 150 \tTraining Loss: 3668.395328 \tValidation Loss: 977.847888\n","  Epoch: 200 \tTraining Loss: 3509.444028 \tValidation Loss: 949.748507\n","  Epoch: 250 \tTraining Loss: 3553.598024 \tValidation Loss: 918.064380\n","  Epoch: 300 \tTraining Loss: 3487.308561 \tValidation Loss: 948.883461\n","Best Validation Loss: 407.116981 in epoch 139\n","Best Train Loss: 2696.373821 in epoch 185\n","  Test Loss: 8.717237\n","\n","  Mean Abs Loss: 4.742857\n","\n","  Test Loss: 8.977541\n","\n","  Mean Abs Loss: 4.847619\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4505.303925 \tValidation Loss: 1141.081602\n","  Epoch: 100 \tTraining Loss: 4157.631450 \tValidation Loss: 1000.752871\n","  Epoch: 150 \tTraining Loss: 4028.720873 \tValidation Loss: 1000.719396\n","  Epoch: 200 \tTraining Loss: 3933.871950 \tValidation Loss: 835.603519\n","  Epoch: 250 \tTraining Loss: 3860.020025 \tValidation Loss: 1024.498481\n","  Epoch: 300 \tTraining Loss: 3880.067316 \tValidation Loss: 989.544418\n","Best Validation Loss: 451.478672 in epoch 156\n","Best Train Loss: 3019.720591 in epoch 236\n","  Test Loss: 8.727803\n","\n","  Mean Abs Loss: 4.742857\n","\n","  Test Loss: 9.082600\n","\n","  Mean Abs Loss: 4.828571\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3838.913235 \tValidation Loss: 1023.289270\n","  Epoch: 100 \tTraining Loss: 3743.709455 \tValidation Loss: 606.111241\n","  Epoch: 150 \tTraining Loss: 3702.743576 \tValidation Loss: 988.722186\n","  Epoch: 200 \tTraining Loss: 3647.472649 \tValidation Loss: 940.549463\n","  Epoch: 250 \tTraining Loss: 3593.014776 \tValidation Loss: 932.038764\n","  Epoch: 300 \tTraining Loss: 3529.333096 \tValidation Loss: 906.040292\n","Best Validation Loss: 479.905987 in epoch 80\n","Best Train Loss: 2712.489374 in epoch 297\n","  Test Loss: 8.736650\n","\n","  Mean Abs Loss: 4.714286\n","\n","  Test Loss: 8.815665\n","\n","  Mean Abs Loss: 4.733333\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4697.665855 \tValidation Loss: 1276.066922\n","  Epoch: 100 \tTraining Loss: 4329.693401 \tValidation Loss: 1152.814571\n","  Epoch: 150 \tTraining Loss: 4097.217520 \tValidation Loss: 1155.472468\n","  Epoch: 200 \tTraining Loss: 4122.916491 \tValidation Loss: 1149.192180\n","  Epoch: 250 \tTraining Loss: 4080.179729 \tValidation Loss: 1011.106554\n","  Epoch: 300 \tTraining Loss: 4012.851103 \tValidation Loss: 1062.042309\n","Best Validation Loss: 457.268926 in epoch 282\n","Best Train Loss: 3086.545838 in epoch 251\n","  Test Loss: 8.955948\n","\n","  Mean Abs Loss: 4.933333\n","\n","  Test Loss: 8.865212\n","\n","  Mean Abs Loss: 4.809524\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3816.450439 \tValidation Loss: 1017.333638\n","  Epoch: 100 \tTraining Loss: 3702.069447 \tValidation Loss: 1002.425075\n","  Epoch: 150 \tTraining Loss: 3637.878137 \tValidation Loss: 954.080363\n","  Epoch: 200 \tTraining Loss: 3536.579236 \tValidation Loss: 945.545808\n","  Epoch: 250 \tTraining Loss: 3460.923670 \tValidation Loss: 890.596615\n","  Epoch: 300 \tTraining Loss: 3458.621708 \tValidation Loss: 942.405490\n","Best Validation Loss: 475.065892 in epoch 204\n","Best Train Loss: 3230.574565 in epoch 205\n","  Test Loss: 8.610977\n","\n","  Mean Abs Loss: 4.771429\n","\n","  Test Loss: 8.581875\n","\n","  Mean Abs Loss: 4.733333\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4647.986998 \tValidation Loss: 1297.796502\n","  Epoch: 100 \tTraining Loss: 3956.406420 \tValidation Loss: 1155.126918\n","  Epoch: 150 \tTraining Loss: 4084.470587 \tValidation Loss: 1118.150251\n","  Epoch: 200 \tTraining Loss: 3928.713989 \tValidation Loss: 1058.007779\n","  Epoch: 250 \tTraining Loss: 3829.127252 \tValidation Loss: 1067.939401\n","  Epoch: 300 \tTraining Loss: 3836.798244 \tValidation Loss: 1035.387834\n","Best Validation Loss: 624.081392 in epoch 295\n","Best Train Loss: 3029.378859 in epoch 239\n","  Test Loss: 8.982447\n","\n","  Mean Abs Loss: 5.180952\n","\n","  Test Loss: 8.946133\n","\n","  Mean Abs Loss: 5.019048\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3932.559207 \tValidation Loss: 1011.984558\n","  Epoch: 100 \tTraining Loss: 3802.449219 \tValidation Loss: 1007.603103\n","  Epoch: 150 \tTraining Loss: 3795.781818 \tValidation Loss: 854.832448\n","  Epoch: 200 \tTraining Loss: 3685.277564 \tValidation Loss: 983.465737\n","  Epoch: 250 \tTraining Loss: 3593.407147 \tValidation Loss: 969.669364\n","  Epoch: 300 \tTraining Loss: 3659.265133 \tValidation Loss: 931.689594\n","Best Validation Loss: 392.962760 in epoch 285\n","Best Train Loss: 2917.300786 in epoch 65\n","  Test Loss: 8.570290\n","\n","  Mean Abs Loss: 4.666667\n","\n","  Test Loss: 8.763337\n","\n","  Mean Abs Loss: 4.704762\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4482.857389 \tValidation Loss: 1213.861650\n","  Epoch: 100 \tTraining Loss: 4068.244667 \tValidation Loss: 697.140311\n","  Epoch: 150 \tTraining Loss: 4097.354688 \tValidation Loss: 1087.073016\n","  Epoch: 200 \tTraining Loss: 3915.747797 \tValidation Loss: 1050.787314\n","  Epoch: 250 \tTraining Loss: 4014.118635 \tValidation Loss: 1051.137608\n","  Epoch: 300 \tTraining Loss: 3971.291982 \tValidation Loss: 1039.461923\n","Best Validation Loss: 630.033611 in epoch 202\n","Best Train Loss: 3160.139257 in epoch 173\n","  Test Loss: 8.729291\n","\n","  Mean Abs Loss: 4.895238\n","\n","  Test Loss: 8.737424\n","\n","  Mean Abs Loss: 4.809524\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3916.684050 \tValidation Loss: 1053.596632\n","  Epoch: 100 \tTraining Loss: 3956.339991 \tValidation Loss: 1025.275102\n","  Epoch: 150 \tTraining Loss: 3654.361307 \tValidation Loss: 939.599084\n","  Epoch: 200 \tTraining Loss: 3570.322489 \tValidation Loss: 963.626626\n","  Epoch: 250 \tTraining Loss: 3585.116013 \tValidation Loss: 963.852592\n","  Epoch: 300 \tTraining Loss: 3499.379183 \tValidation Loss: 946.841379\n","Best Validation Loss: 455.750414 in epoch 43\n","Best Train Loss: 2767.085413 in epoch 141\n","  Test Loss: 8.574039\n","\n","  Mean Abs Loss: 4.819048\n","\n","  Test Loss: 8.959189\n","\n","  Mean Abs Loss: 4.933333\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4669.963084 \tValidation Loss: 1226.458029\n","  Epoch: 100 \tTraining Loss: 4116.526532 \tValidation Loss: 1095.940649\n","  Epoch: 150 \tTraining Loss: 3914.008627 \tValidation Loss: 1078.585019\n","  Epoch: 200 \tTraining Loss: 3838.918571 \tValidation Loss: 1054.320641\n","  Epoch: 250 \tTraining Loss: 3872.270204 \tValidation Loss: 1057.283216\n","  Epoch: 300 \tTraining Loss: 3867.735509 \tValidation Loss: 1052.863734\n","Best Validation Loss: 417.375784 in epoch 252\n","Best Train Loss: 3010.524332 in epoch 290\n","  Test Loss: 8.690845\n","\n","  Mean Abs Loss: 4.771429\n","\n","  Test Loss: 8.771204\n","\n","  Mean Abs Loss: 4.809524\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3911.445200 \tValidation Loss: 1001.402411\n","  Epoch: 100 \tTraining Loss: 3720.482207 \tValidation Loss: 990.019809\n","  Epoch: 150 \tTraining Loss: 3671.521132 \tValidation Loss: 997.970538\n","  Epoch: 200 \tTraining Loss: 3657.372809 \tValidation Loss: 982.392015\n","  Epoch: 250 \tTraining Loss: 3521.895162 \tValidation Loss: 969.528483\n","  Epoch: 300 \tTraining Loss: 3442.395382 \tValidation Loss: 954.616396\n","Best Validation Loss: 579.445089 in epoch 66\n","Best Train Loss: 3098.895768 in epoch 30\n","  Test Loss: 9.030672\n","\n","  Mean Abs Loss: 5.123810\n","\n","  Test Loss: 8.781926\n","\n","  Mean Abs Loss: 4.819048\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4545.409858 \tValidation Loss: 1203.717048\n","  Epoch: 100 \tTraining Loss: 4081.456128 \tValidation Loss: 1090.575735\n","  Epoch: 150 \tTraining Loss: 4043.890748 \tValidation Loss: 1075.973792\n","  Epoch: 200 \tTraining Loss: 4042.749727 \tValidation Loss: 1079.021496\n","  Epoch: 250 \tTraining Loss: 3909.663695 \tValidation Loss: 1077.234161\n","  Epoch: 300 \tTraining Loss: 3855.916166 \tValidation Loss: 1048.985403\n","Best Validation Loss: 692.777138 in epoch 162\n","Best Train Loss: 2904.330461 in epoch 266\n","  Test Loss: 9.009843\n","\n","  Mean Abs Loss: 4.971429\n","\n","  Test Loss: 9.032913\n","\n","  Mean Abs Loss: 4.980952\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3761.941119 \tValidation Loss: 1015.268854\n","  Epoch: 100 \tTraining Loss: 3551.752600 \tValidation Loss: 939.986130\n","  Epoch: 150 \tTraining Loss: 3716.385373 \tValidation Loss: 978.893365\n","  Epoch: 200 \tTraining Loss: 3534.687738 \tValidation Loss: 949.366728\n","  Epoch: 250 \tTraining Loss: 3620.197799 \tValidation Loss: 961.193035\n","  Epoch: 300 \tTraining Loss: 3468.040114 \tValidation Loss: 932.280669\n","Best Validation Loss: 395.214399 in epoch 70\n","Best Train Loss: 2836.912019 in epoch 77\n","  Test Loss: 8.759310\n","\n","  Mean Abs Loss: 4.695238\n","\n","  Test Loss: 8.587463\n","\n","  Mean Abs Loss: 4.685714\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4647.478288 \tValidation Loss: 1252.295991\n","  Epoch: 100 \tTraining Loss: 4418.944489 \tValidation Loss: 1130.890343\n","  Epoch: 150 \tTraining Loss: 3973.149364 \tValidation Loss: 1137.535678\n","  Epoch: 200 \tTraining Loss: 3939.676574 \tValidation Loss: 1127.760624\n","  Epoch: 250 \tTraining Loss: 3834.967162 \tValidation Loss: 984.057385\n","  Epoch: 300 \tTraining Loss: 3771.818011 \tValidation Loss: 708.191959\n","Best Validation Loss: 477.575461 in epoch 187\n","Best Train Loss: 2845.243659 in epoch 293\n","  Test Loss: 10.008100\n","\n","  Mean Abs Loss: 5.695238\n","\n","  Test Loss: 9.821194\n","\n","  Mean Abs Loss: 5.561905\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3860.643044 \tValidation Loss: 1019.989234\n","  Epoch: 100 \tTraining Loss: 3756.125394 \tValidation Loss: 979.303811\n","  Epoch: 150 \tTraining Loss: 3643.261006 \tValidation Loss: 984.259175\n","  Epoch: 200 \tTraining Loss: 3678.124855 \tValidation Loss: 977.988892\n","  Epoch: 250 \tTraining Loss: 3670.632154 \tValidation Loss: 972.765549\n","  Epoch: 300 \tTraining Loss: 3562.286242 \tValidation Loss: 972.612494\n","Best Validation Loss: 592.131172 in epoch 83\n","Best Train Loss: 2826.707911 in epoch 167\n","  Test Loss: 9.533195\n","\n","  Mean Abs Loss: 5.238095\n","\n","  Test Loss: 9.485444\n","\n","  Mean Abs Loss: 5.228571\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4603.719720 \tValidation Loss: 1280.018196\n","  Epoch: 100 \tTraining Loss: 4198.705339 \tValidation Loss: 1166.396801\n","  Epoch: 150 \tTraining Loss: 4117.488644 \tValidation Loss: 1084.820095\n","  Epoch: 200 \tTraining Loss: 3955.460622 \tValidation Loss: 1078.984756\n","  Epoch: 250 \tTraining Loss: 3976.336898 \tValidation Loss: 1094.226927\n","  Epoch: 300 \tTraining Loss: 3917.440006 \tValidation Loss: 1051.693200\n","Best Validation Loss: 664.517296 in epoch 278\n","Best Train Loss: 3381.902022 in epoch 80\n","  Test Loss: 9.646806\n","\n","  Mean Abs Loss: 5.600000\n","\n","  Test Loss: 8.801200\n","\n","  Mean Abs Loss: 4.809524\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3773.910320 \tValidation Loss: 805.375075\n","  Epoch: 100 \tTraining Loss: 3781.993592 \tValidation Loss: 993.057931\n","  Epoch: 150 \tTraining Loss: 3733.649573 \tValidation Loss: 942.371623\n","  Epoch: 200 \tTraining Loss: 3623.514202 \tValidation Loss: 967.525154\n","  Epoch: 250 \tTraining Loss: 3439.938815 \tValidation Loss: 964.505129\n","  Epoch: 300 \tTraining Loss: 3503.505996 \tValidation Loss: 959.860851\n","Best Validation Loss: 575.719735 in epoch 179\n","Best Train Loss: 2809.344371 in epoch 195\n","  Test Loss: 8.782705\n","\n","  Mean Abs Loss: 4.885714\n","\n","  Test Loss: 8.748257\n","\n","  Mean Abs Loss: 4.809524\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4708.281442 \tValidation Loss: 1262.185821\n","  Epoch: 100 \tTraining Loss: 4170.877419 \tValidation Loss: 1129.346379\n","  Epoch: 150 \tTraining Loss: 4159.267637 \tValidation Loss: 1097.302915\n","  Epoch: 200 \tTraining Loss: 3830.074414 \tValidation Loss: 962.779717\n","  Epoch: 250 \tTraining Loss: 3831.904205 \tValidation Loss: 1054.310113\n","  Epoch: 300 \tTraining Loss: 3815.156156 \tValidation Loss: 1043.525856\n","Best Validation Loss: 636.883682 in epoch 283\n","Best Train Loss: 3624.666303 in epoch 290\n","  Test Loss: 8.827614\n","\n","  Mean Abs Loss: 5.019048\n","\n","  Test Loss: 8.824033\n","\n","  Mean Abs Loss: 4.980952\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3849.242866 \tValidation Loss: 1016.556800\n","  Epoch: 100 \tTraining Loss: 3772.938953 \tValidation Loss: 1006.557025\n","  Epoch: 150 \tTraining Loss: 3642.269286 \tValidation Loss: 987.111003\n","  Epoch: 200 \tTraining Loss: 3648.949372 \tValidation Loss: 962.943444\n","  Epoch: 250 \tTraining Loss: 3574.095839 \tValidation Loss: 968.342212\n","  Epoch: 300 \tTraining Loss: 3476.088871 \tValidation Loss: 959.166031\n","Best Validation Loss: 613.563965 in epoch 243\n","Best Train Loss: 2824.071708 in epoch 252\n","  Test Loss: 8.760097\n","\n","  Mean Abs Loss: 4.723810\n","\n","  Test Loss: 8.650637\n","\n","  Mean Abs Loss: 4.971429\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4438.164969 \tValidation Loss: 1151.558040\n","  Epoch: 100 \tTraining Loss: 4014.713696 \tValidation Loss: 693.056719\n","  Epoch: 150 \tTraining Loss: 4025.946096 \tValidation Loss: 1066.435171\n","  Epoch: 200 \tTraining Loss: 3830.633582 \tValidation Loss: 1040.954844\n","  Epoch: 250 \tTraining Loss: 3894.231810 \tValidation Loss: 1033.357483\n","  Epoch: 300 \tTraining Loss: 3817.912541 \tValidation Loss: 980.306456\n","Best Validation Loss: 455.292348 in epoch 227\n","Best Train Loss: 3042.341420 in epoch 278\n","  Test Loss: 8.778435\n","\n","  Mean Abs Loss: 4.971429\n","\n","  Test Loss: 8.852322\n","\n","  Mean Abs Loss: 5.133333\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3974.067588 \tValidation Loss: 637.670438\n","  Epoch: 100 \tTraining Loss: 3747.022784 \tValidation Loss: 992.555063\n","  Epoch: 150 \tTraining Loss: 3594.464426 \tValidation Loss: 952.031964\n","  Epoch: 200 \tTraining Loss: 3514.154048 \tValidation Loss: 1005.632081\n","  Epoch: 250 \tTraining Loss: 3548.080703 \tValidation Loss: 992.465951\n","  Epoch: 300 \tTraining Loss: 3418.520241 \tValidation Loss: 1010.607409\n","Best Validation Loss: 602.707253 in epoch 126\n","Best Train Loss: 2688.708953 in epoch 259\n","  Test Loss: 8.900870\n","\n","  Mean Abs Loss: 4.695238\n","\n","  Test Loss: 8.648056\n","\n","  Mean Abs Loss: 4.780952\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4069.416706 \tValidation Loss: 1128.151220\n","  Epoch: 100 \tTraining Loss: 4173.311293 \tValidation Loss: 715.326191\n","  Epoch: 150 \tTraining Loss: 4192.408494 \tValidation Loss: 1073.178145\n","  Epoch: 200 \tTraining Loss: 3993.732436 \tValidation Loss: 1065.936717\n","  Epoch: 250 \tTraining Loss: 3888.701546 \tValidation Loss: 1055.123158\n","  Epoch: 300 \tTraining Loss: 3964.547292 \tValidation Loss: 1023.983854\n","Best Validation Loss: 653.571355 in epoch 279\n","Best Train Loss: 3414.536093 in epoch 61\n","  Test Loss: 9.712264\n","\n","  Mean Abs Loss: 5.476190\n","\n","  Test Loss: 9.036792\n","\n","  Mean Abs Loss: 5.133333\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3828.897068 \tValidation Loss: 988.754858\n","  Epoch: 100 \tTraining Loss: 3698.658612 \tValidation Loss: 966.220342\n","  Epoch: 150 \tTraining Loss: 3517.311147 \tValidation Loss: 964.212396\n","  Epoch: 200 \tTraining Loss: 3462.088481 \tValidation Loss: 586.332531\n","  Epoch: 250 \tTraining Loss: 3478.553850 \tValidation Loss: 968.274050\n","  Epoch: 300 \tTraining Loss: 3211.658311 \tValidation Loss: 978.383079\n","Best Validation Loss: 486.485419 in epoch 258\n","Best Train Loss: 2845.336384 in epoch 159\n","  Test Loss: 8.521278\n","\n","  Mean Abs Loss: 4.676190\n","\n","  Test Loss: 8.569003\n","\n","  Mean Abs Loss: 4.800000\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4552.507259 \tValidation Loss: 1211.635032\n","  Epoch: 100 \tTraining Loss: 4104.552691 \tValidation Loss: 1060.510249\n","  Epoch: 150 \tTraining Loss: 4049.205183 \tValidation Loss: 1076.547321\n","  Epoch: 200 \tTraining Loss: 3981.903877 \tValidation Loss: 1035.516484\n","  Epoch: 250 \tTraining Loss: 3879.910134 \tValidation Loss: 1032.907340\n","  Epoch: 300 \tTraining Loss: 3798.731683 \tValidation Loss: 1011.913037\n","Best Validation Loss: 604.797172 in epoch 233\n","Best Train Loss: 3010.331170 in epoch 191\n","  Test Loss: 8.773158\n","\n","  Mean Abs Loss: 4.752381\n","\n","  Test Loss: 8.767748\n","\n","  Mean Abs Loss: 4.742857\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3832.791289 \tValidation Loss: 1028.854585\n","  Epoch: 100 \tTraining Loss: 3709.760253 \tValidation Loss: 1009.033874\n","  Epoch: 150 \tTraining Loss: 3701.807667 \tValidation Loss: 993.786618\n","  Epoch: 200 \tTraining Loss: 3651.808787 \tValidation Loss: 978.054178\n","  Epoch: 250 \tTraining Loss: 3486.980027 \tValidation Loss: 978.866498\n","  Epoch: 300 \tTraining Loss: 3653.208096 \tValidation Loss: 1012.478622\n","Best Validation Loss: 554.138527 in epoch 214\n","Best Train Loss: 2731.001007 in epoch 266\n","  Test Loss: 8.806843\n","\n","  Mean Abs Loss: 4.733333\n","\n","  Test Loss: 8.749286\n","\n","  Mean Abs Loss: 4.695238\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4440.107179 \tValidation Loss: 1189.292425\n","  Epoch: 100 \tTraining Loss: 4068.677076 \tValidation Loss: 1097.369257\n","  Epoch: 150 \tTraining Loss: 4023.389898 \tValidation Loss: 1081.220902\n","  Epoch: 200 \tTraining Loss: 3975.047519 \tValidation Loss: 1068.211516\n","  Epoch: 250 \tTraining Loss: 3862.127179 \tValidation Loss: 1070.803930\n","  Epoch: 300 \tTraining Loss: 3784.886782 \tValidation Loss: 1025.715138\n","Best Validation Loss: 659.468162 in epoch 284\n","Best Train Loss: 3039.969769 in epoch 194\n","  Test Loss: 9.159034\n","\n","  Mean Abs Loss: 5.066667\n","\n","  Test Loss: 9.080264\n","\n","  Mean Abs Loss: 5.095238\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3883.565555 \tValidation Loss: 825.233361\n","  Epoch: 100 \tTraining Loss: 3846.641036 \tValidation Loss: 1030.812096\n","  Epoch: 150 \tTraining Loss: 3670.400029 \tValidation Loss: 1007.331055\n","  Epoch: 200 \tTraining Loss: 3648.308530 \tValidation Loss: 966.439717\n","  Epoch: 250 \tTraining Loss: 3674.112442 \tValidation Loss: 946.758108\n","  Epoch: 300 \tTraining Loss: 3646.889172 \tValidation Loss: 942.213742\n","Best Validation Loss: 567.903222 in epoch 75\n","Best Train Loss: 2810.121045 in epoch 180\n","  Test Loss: 8.597226\n","\n","  Mean Abs Loss: 4.742857\n","\n","  Test Loss: 8.600663\n","\n","  Mean Abs Loss: 4.761905\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4518.291537 \tValidation Loss: 1151.781720\n","  Epoch: 100 \tTraining Loss: 4121.289849 \tValidation Loss: 1051.524329\n","  Epoch: 150 \tTraining Loss: 4042.405156 \tValidation Loss: 1037.139830\n","  Epoch: 200 \tTraining Loss: 4032.860724 \tValidation Loss: 1033.407977\n","  Epoch: 250 \tTraining Loss: 4058.893441 \tValidation Loss: 1028.180389\n","  Epoch: 300 \tTraining Loss: 3674.401726 \tValidation Loss: 1017.530141\n","Best Validation Loss: 527.033207 in epoch 201\n","Best Train Loss: 3615.096146 in epoch 254\n","  Test Loss: 8.677511\n","\n","  Mean Abs Loss: 4.800000\n","\n","  Test Loss: 8.701998\n","\n","  Mean Abs Loss: 4.828571\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3838.570073 \tValidation Loss: 1009.440668\n","  Epoch: 100 \tTraining Loss: 3621.790638 \tValidation Loss: 971.038118\n","  Epoch: 150 \tTraining Loss: 3538.991436 \tValidation Loss: 944.007839\n","  Epoch: 200 \tTraining Loss: 3453.929227 \tValidation Loss: 929.916830\n","  Epoch: 250 \tTraining Loss: 3532.477642 \tValidation Loss: 909.685643\n","  Epoch: 300 \tTraining Loss: 3533.680216 \tValidation Loss: 911.646406\n","Best Validation Loss: 387.344692 in epoch 199\n","Best Train Loss: 2965.197567 in epoch 41\n","  Test Loss: 8.889142\n","\n","  Mean Abs Loss: 5.038095\n","\n","  Test Loss: 8.740733\n","\n","  Mean Abs Loss: 4.714286\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4408.411117 \tValidation Loss: 1198.483611\n","  Epoch: 100 \tTraining Loss: 4262.468696 \tValidation Loss: 1125.709398\n","  Epoch: 150 \tTraining Loss: 4210.729481 \tValidation Loss: 1104.881779\n","  Epoch: 200 \tTraining Loss: 4004.518425 \tValidation Loss: 1106.553776\n","  Epoch: 250 \tTraining Loss: 4015.775062 \tValidation Loss: 1027.993945\n","  Epoch: 300 \tTraining Loss: 3866.759211 \tValidation Loss: 1055.757863\n","Best Validation Loss: 650.025232 in epoch 223\n","Best Train Loss: 3438.434360 in epoch 74\n","  Test Loss: 10.000059\n","\n","  Mean Abs Loss: 6.104762\n","\n","  Test Loss: 9.222906\n","\n","  Mean Abs Loss: 5.266667\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3747.366725 \tValidation Loss: 1022.858090\n","  Epoch: 100 \tTraining Loss: 3792.404410 \tValidation Loss: 583.393607\n","  Epoch: 150 \tTraining Loss: 3765.200762 \tValidation Loss: 981.099411\n","  Epoch: 200 \tTraining Loss: 3590.147591 \tValidation Loss: 967.752247\n","  Epoch: 250 \tTraining Loss: 3308.496530 \tValidation Loss: 960.090420\n","  Epoch: 300 \tTraining Loss: 3530.224166 \tValidation Loss: 981.227649\n","Best Validation Loss: 502.676622 in epoch 119\n","Best Train Loss: 2824.117710 in epoch 163\n","  Test Loss: 8.907032\n","\n","  Mean Abs Loss: 4.866667\n","\n","  Test Loss: 8.879223\n","\n","  Mean Abs Loss: 4.828571\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4605.523094 \tValidation Loss: 1209.748841\n","  Epoch: 100 \tTraining Loss: 4127.141540 \tValidation Loss: 1126.583685\n","  Epoch: 150 \tTraining Loss: 3977.710481 \tValidation Loss: 1153.007110\n","  Epoch: 200 \tTraining Loss: 3192.575215 \tValidation Loss: 1080.222485\n","  Epoch: 250 \tTraining Loss: 3954.162529 \tValidation Loss: 1061.289326\n","  Epoch: 300 \tTraining Loss: 3850.915641 \tValidation Loss: 1039.562466\n","Best Validation Loss: 606.877982 in epoch 257\n","Best Train Loss: 3192.575215 in epoch 200\n","  Test Loss: 9.759463\n","\n","  Mean Abs Loss: 5.590476\n","\n","  Test Loss: 9.293773\n","\n","  Mean Abs Loss: 4.990476\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3773.885964 \tValidation Loss: 1009.458585\n","  Epoch: 100 \tTraining Loss: 3705.390868 \tValidation Loss: 954.448291\n","  Epoch: 150 \tTraining Loss: 3647.204204 \tValidation Loss: 959.105021\n","  Epoch: 200 \tTraining Loss: 3592.027327 \tValidation Loss: 938.813745\n","  Epoch: 250 \tTraining Loss: 3520.224569 \tValidation Loss: 935.029864\n","  Epoch: 300 \tTraining Loss: 3499.674131 \tValidation Loss: 951.833120\n","Best Validation Loss: 565.744627 in epoch 284\n","Best Train Loss: 2676.691368 in epoch 281\n","  Test Loss: 8.657484\n","\n","  Mean Abs Loss: 4.742857\n","\n","  Test Loss: 8.693582\n","\n","  Mean Abs Loss: 4.771429\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4538.738827 \tValidation Loss: 1142.626696\n","  Epoch: 100 \tTraining Loss: 4304.007924 \tValidation Loss: 1046.063902\n","  Epoch: 150 \tTraining Loss: 4102.302917 \tValidation Loss: 1046.703329\n","  Epoch: 200 \tTraining Loss: 3970.469541 \tValidation Loss: 844.575950\n","  Epoch: 250 \tTraining Loss: 3997.322278 \tValidation Loss: 1040.818628\n","  Epoch: 300 \tTraining Loss: 3964.494630 \tValidation Loss: 1027.702111\n","Best Validation Loss: 640.964710 in epoch 122\n","Best Train Loss: 3657.194835 in epoch 223\n","  Test Loss: 9.322445\n","\n","  Mean Abs Loss: 5.085714\n","\n","  Test Loss: 8.979195\n","\n","  Mean Abs Loss: 4.780952\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3831.097532 \tValidation Loss: 817.990381\n","  Epoch: 100 \tTraining Loss: 3681.978670 \tValidation Loss: 809.447484\n","  Epoch: 150 \tTraining Loss: 3579.013208 \tValidation Loss: 994.457823\n","  Epoch: 200 \tTraining Loss: 3502.060764 \tValidation Loss: 976.551147\n","  Epoch: 250 \tTraining Loss: 3489.390383 \tValidation Loss: 890.830581\n","  Epoch: 300 \tTraining Loss: 3478.732523 \tValidation Loss: 910.101358\n","Best Validation Loss: 625.804334 in epoch 296\n","Best Train Loss: 2676.488140 in epoch 236\n","  Test Loss: 8.850019\n","\n","  Mean Abs Loss: 4.838095\n","\n","  Test Loss: 8.803700\n","\n","  Mean Abs Loss: 4.761905\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4476.993459 \tValidation Loss: 788.776683\n","  Epoch: 100 \tTraining Loss: 4175.455328 \tValidation Loss: 1063.149850\n","  Epoch: 150 \tTraining Loss: 3961.036208 \tValidation Loss: 1064.680899\n","  Epoch: 200 \tTraining Loss: 3911.855606 \tValidation Loss: 1040.174412\n","  Epoch: 250 \tTraining Loss: 3863.950859 \tValidation Loss: 1033.203387\n","  Epoch: 300 \tTraining Loss: 3850.217986 \tValidation Loss: 1029.650653\n","Best Validation Loss: 458.366125 in epoch 279\n","Best Train Loss: 3428.463585 in epoch 265\n","  Test Loss: 8.989150\n","\n","  Mean Abs Loss: 5.085714\n","\n","  Test Loss: 9.035845\n","\n","  Mean Abs Loss: 5.114286\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3929.619035 \tValidation Loss: 1019.506233\n","  Epoch: 100 \tTraining Loss: 3737.192275 \tValidation Loss: 1025.668887\n","  Epoch: 150 \tTraining Loss: 3640.483599 \tValidation Loss: 982.095622\n","  Epoch: 200 \tTraining Loss: 3567.382564 \tValidation Loss: 969.849648\n","  Epoch: 250 \tTraining Loss: 3597.393361 \tValidation Loss: 967.305860\n","  Epoch: 300 \tTraining Loss: 3526.107022 \tValidation Loss: 925.681511\n","Best Validation Loss: 480.113757 in epoch 23\n","Best Train Loss: 3254.833616 in epoch 211\n","  Test Loss: 8.778991\n","\n","  Mean Abs Loss: 4.695238\n","\n","  Test Loss: 9.119909\n","\n","  Mean Abs Loss: 4.952381\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4758.842719 \tValidation Loss: 1369.606088\n","  Epoch: 100 \tTraining Loss: 4428.029394 \tValidation Loss: 1189.282695\n","  Epoch: 150 \tTraining Loss: 4088.736598 \tValidation Loss: 1090.611525\n","  Epoch: 200 \tTraining Loss: 4183.632358 \tValidation Loss: 1066.696703\n","  Epoch: 250 \tTraining Loss: 3990.250882 \tValidation Loss: 1083.047331\n","  Epoch: 300 \tTraining Loss: 3895.772114 \tValidation Loss: 1071.496996\n","Best Validation Loss: 677.113059 in epoch 186\n","Best Train Loss: 3146.449149 in epoch 267\n","  Test Loss: 9.681262\n","\n","  Mean Abs Loss: 5.666667\n","\n","  Test Loss: 10.632368\n","\n","  Mean Abs Loss: 6.533333\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3865.662761 \tValidation Loss: 1023.293129\n","  Epoch: 100 \tTraining Loss: 3827.780878 \tValidation Loss: 1010.496489\n","  Epoch: 150 \tTraining Loss: 3657.597706 \tValidation Loss: 973.049313\n","  Epoch: 200 \tTraining Loss: 3640.078080 \tValidation Loss: 960.392417\n","  Epoch: 250 \tTraining Loss: 3502.295351 \tValidation Loss: 897.748130\n","  Epoch: 300 \tTraining Loss: 3500.546695 \tValidation Loss: 932.945277\n","Best Validation Loss: 561.582641 in epoch 261\n","Best Train Loss: 2705.371280 in epoch 254\n","  Test Loss: 8.843067\n","\n","  Mean Abs Loss: 4.847619\n","\n","  Test Loss: 8.630123\n","\n","  Mean Abs Loss: 4.876190\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4685.721110 \tValidation Loss: 1281.397028\n","  Epoch: 100 \tTraining Loss: 4377.880720 \tValidation Loss: 1191.654074\n","  Epoch: 150 \tTraining Loss: 4153.649509 \tValidation Loss: 1156.592904\n","  Epoch: 200 \tTraining Loss: 4041.189046 \tValidation Loss: 1086.481143\n","  Epoch: 250 \tTraining Loss: 3944.621306 \tValidation Loss: 1077.391358\n","  Epoch: 300 \tTraining Loss: 3986.392797 \tValidation Loss: 1049.305927\n","Best Validation Loss: 679.605813 in epoch 279\n","Best Train Loss: 3202.236483 in epoch 185\n","  Test Loss: 9.830809\n","\n","  Mean Abs Loss: 5.676190\n","\n","  Test Loss: 9.162897\n","\n","  Mean Abs Loss: 5.190476\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3953.106985 \tValidation Loss: 1002.510694\n","  Epoch: 100 \tTraining Loss: 3804.577581 \tValidation Loss: 1006.483530\n","  Epoch: 150 \tTraining Loss: 3840.172639 \tValidation Loss: 968.690317\n","  Epoch: 200 \tTraining Loss: 3555.433578 \tValidation Loss: 963.980757\n","  Epoch: 250 \tTraining Loss: 3592.934735 \tValidation Loss: 982.131519\n","  Epoch: 300 \tTraining Loss: 3444.617306 \tValidation Loss: 984.568496\n","Best Validation Loss: 420.352074 in epoch 77\n","Best Train Loss: 2714.869293 in epoch 233\n","  Test Loss: 8.953276\n","\n","  Mean Abs Loss: 5.285714\n","\n","  Test Loss: 8.663628\n","\n","  Mean Abs Loss: 4.780952\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4474.946603 \tValidation Loss: 1182.478575\n","  Epoch: 100 \tTraining Loss: 4018.787213 \tValidation Loss: 1094.513533\n","  Epoch: 150 \tTraining Loss: 3975.938344 \tValidation Loss: 1024.483843\n","  Epoch: 200 \tTraining Loss: 3877.661166 \tValidation Loss: 1062.966190\n","  Epoch: 250 \tTraining Loss: 3794.134411 \tValidation Loss: 1051.202184\n","  Epoch: 300 \tTraining Loss: 3756.555490 \tValidation Loss: 1044.151770\n","Best Validation Loss: 623.082611 in epoch 169\n","Best Train Loss: 3047.124578 in epoch 271\n","  Test Loss: 8.845621\n","\n","  Mean Abs Loss: 5.000000\n","\n","  Test Loss: 8.852417\n","\n","  Mean Abs Loss: 4.942857\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3913.772389 \tValidation Loss: 1030.313239\n","  Epoch: 100 \tTraining Loss: 3738.358766 \tValidation Loss: 977.252954\n","  Epoch: 150 \tTraining Loss: 3672.091426 \tValidation Loss: 973.905653\n","  Epoch: 200 \tTraining Loss: 3286.510412 \tValidation Loss: 958.849179\n","  Epoch: 250 \tTraining Loss: 3578.413203 \tValidation Loss: 987.623983\n","  Epoch: 300 \tTraining Loss: 3549.194547 \tValidation Loss: 947.119264\n","Best Validation Loss: 593.582079 in epoch 279\n","Best Train Loss: 2681.056492 in epoch 274\n","  Test Loss: 8.738862\n","\n","  Mean Abs Loss: 4.685714\n","\n","  Test Loss: 8.705206\n","\n","  Mean Abs Loss: 4.685714\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4492.678573 \tValidation Loss: 1119.997046\n","  Epoch: 100 \tTraining Loss: 4262.060060 \tValidation Loss: 638.951193\n","  Epoch: 150 \tTraining Loss: 4056.010658 \tValidation Loss: 1033.266728\n","  Epoch: 200 \tTraining Loss: 4151.846353 \tValidation Loss: 1033.117231\n","  Epoch: 250 \tTraining Loss: 3923.653289 \tValidation Loss: 1032.718106\n","  Epoch: 300 \tTraining Loss: 3848.224748 \tValidation Loss: 1026.537379\n","Best Validation Loss: 581.714428 in epoch 173\n","Best Train Loss: 3074.011806 in epoch 275\n","  Test Loss: 8.745533\n","\n","  Mean Abs Loss: 4.847619\n","\n","  Test Loss: 8.722805\n","\n","  Mean Abs Loss: 4.828571\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3772.803741 \tValidation Loss: 981.983028\n","  Epoch: 100 \tTraining Loss: 3725.896542 \tValidation Loss: 858.185729\n","  Epoch: 150 \tTraining Loss: 3538.978270 \tValidation Loss: 961.311874\n","  Epoch: 200 \tTraining Loss: 3502.951326 \tValidation Loss: 948.120586\n","  Epoch: 250 \tTraining Loss: 3494.068898 \tValidation Loss: 828.187594\n","  Epoch: 300 \tTraining Loss: 3537.914789 \tValidation Loss: 946.091828\n","Best Validation Loss: 569.934665 in epoch 252\n","Best Train Loss: 2768.292010 in epoch 127\n","  Test Loss: 8.758763\n","\n","  Mean Abs Loss: 4.809524\n","\n","  Test Loss: 8.827527\n","\n","  Mean Abs Loss: 4.800000\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4637.240894 \tValidation Loss: 1246.446687\n","  Epoch: 100 \tTraining Loss: 4221.346257 \tValidation Loss: 1160.839037\n","  Epoch: 150 \tTraining Loss: 4119.612816 \tValidation Loss: 1150.857006\n","  Epoch: 200 \tTraining Loss: 4053.051699 \tValidation Loss: 1070.491634\n","  Epoch: 250 \tTraining Loss: 3895.721068 \tValidation Loss: 1020.846262\n","  Epoch: 300 \tTraining Loss: 3923.637534 \tValidation Loss: 989.295894\n","Best Validation Loss: 629.973765 in epoch 267\n","Best Train Loss: 3001.149459 in epoch 281\n","  Test Loss: 8.925758\n","\n","  Mean Abs Loss: 4.819048\n","\n","  Test Loss: 8.854030\n","\n","  Mean Abs Loss: 4.876190\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3888.111557 \tValidation Loss: 1098.503486\n","  Epoch: 100 \tTraining Loss: 3774.027663 \tValidation Loss: 998.523857\n","  Epoch: 150 \tTraining Loss: 3648.086130 \tValidation Loss: 988.327443\n","  Epoch: 200 \tTraining Loss: 3558.191749 \tValidation Loss: 951.528397\n","  Epoch: 250 \tTraining Loss: 3621.481779 \tValidation Loss: 946.835502\n","  Epoch: 300 \tTraining Loss: 3499.474765 \tValidation Loss: 933.056538\n","Best Validation Loss: 588.493586 in epoch 63\n","Best Train Loss: 2839.746882 in epoch 123\n","  Test Loss: 8.644373\n","\n","  Mean Abs Loss: 4.695238\n","\n","  Test Loss: 8.872442\n","\n","  Mean Abs Loss: 4.742857\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4658.515399 \tValidation Loss: 1017.634978\n","  Epoch: 100 \tTraining Loss: 4172.110552 \tValidation Loss: 1125.873856\n","  Epoch: 150 \tTraining Loss: 4146.476628 \tValidation Loss: 1074.778098\n","  Epoch: 200 \tTraining Loss: 3819.174321 \tValidation Loss: 1069.253009\n","  Epoch: 250 \tTraining Loss: 3844.754409 \tValidation Loss: 1054.528152\n","  Epoch: 300 \tTraining Loss: 3875.877392 \tValidation Loss: 1036.849697\n","Best Validation Loss: 490.024261 in epoch 234\n","Best Train Loss: 3015.462597 in epoch 282\n","  Test Loss: 8.941040\n","\n","  Mean Abs Loss: 4.942857\n","\n","  Test Loss: 9.713779\n","\n","  Mean Abs Loss: 5.504762\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3791.477960 \tValidation Loss: 1020.035376\n","  Epoch: 100 \tTraining Loss: 3743.257682 \tValidation Loss: 890.727122\n","  Epoch: 150 \tTraining Loss: 3638.170298 \tValidation Loss: 947.485509\n","  Epoch: 200 \tTraining Loss: 3592.849709 \tValidation Loss: 980.893272\n","  Epoch: 250 \tTraining Loss: 3613.406131 \tValidation Loss: 979.839634\n","  Epoch: 300 \tTraining Loss: 3469.204407 \tValidation Loss: 946.630569\n","Best Validation Loss: 571.995159 in epoch 214\n","Best Train Loss: 2695.769246 in epoch 230\n","  Test Loss: 8.830035\n","\n","  Mean Abs Loss: 4.780952\n","\n","  Test Loss: 8.770107\n","\n","  Mean Abs Loss: 4.761905\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4466.989922 \tValidation Loss: 1165.792980\n","  Epoch: 100 \tTraining Loss: 4058.986588 \tValidation Loss: 1052.258742\n","  Epoch: 150 \tTraining Loss: 3953.134144 \tValidation Loss: 1049.620160\n","  Epoch: 200 \tTraining Loss: 3919.533373 \tValidation Loss: 1051.450228\n","  Epoch: 250 \tTraining Loss: 3848.612400 \tValidation Loss: 1030.553678\n","  Epoch: 300 \tTraining Loss: 3963.033978 \tValidation Loss: 992.458807\n","Best Validation Loss: 599.374344 in epoch 185\n","Best Train Loss: 3167.961940 in epoch 207\n","  Test Loss: 8.747132\n","\n","  Mean Abs Loss: 4.790476\n","\n","  Test Loss: 8.690793\n","\n","  Mean Abs Loss: 4.800000\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3827.557119 \tValidation Loss: 1028.693100\n","  Epoch: 100 \tTraining Loss: 3797.458663 \tValidation Loss: 1012.236684\n","  Epoch: 150 \tTraining Loss: 3623.156582 \tValidation Loss: 967.965006\n","  Epoch: 200 \tTraining Loss: 3644.990289 \tValidation Loss: 982.920657\n","  Epoch: 250 \tTraining Loss: 3540.766961 \tValidation Loss: 596.748566\n","  Epoch: 300 \tTraining Loss: 3575.117933 \tValidation Loss: 964.928520\n","Best Validation Loss: 574.321280 in epoch 260\n","Best Train Loss: 2703.591959 in epoch 282\n","  Test Loss: 8.689920\n","\n","  Mean Abs Loss: 4.771429\n","\n","  Test Loss: 8.690413\n","\n","  Mean Abs Loss: 4.790476\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4560.816361 \tValidation Loss: 1172.632171\n","  Epoch: 100 \tTraining Loss: 4217.637997 \tValidation Loss: 1046.276221\n","  Epoch: 150 \tTraining Loss: 3931.427792 \tValidation Loss: 1138.215155\n","  Epoch: 200 \tTraining Loss: 4074.266006 \tValidation Loss: 1088.428225\n","  Epoch: 250 \tTraining Loss: 3946.420659 \tValidation Loss: 1097.324311\n","  Epoch: 300 \tTraining Loss: 3864.617585 \tValidation Loss: 998.268040\n","Best Validation Loss: 460.093204 in epoch 289\n","Best Train Loss: 3313.615065 in epoch 157\n","  Test Loss: 9.942111\n","\n","  Mean Abs Loss: 5.895238\n","\n","  Test Loss: 9.227211\n","\n","  Mean Abs Loss: 5.123810\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3597.652157 \tValidation Loss: 1029.784111\n","  Epoch: 100 \tTraining Loss: 3612.338763 \tValidation Loss: 1009.294076\n","  Epoch: 150 \tTraining Loss: 3589.618430 \tValidation Loss: 992.206137\n","  Epoch: 200 \tTraining Loss: 3463.393030 \tValidation Loss: 980.504704\n","  Epoch: 250 \tTraining Loss: 3564.142006 \tValidation Loss: 624.478761\n","  Epoch: 300 \tTraining Loss: 3363.941956 \tValidation Loss: 1025.604946\n","Best Validation Loss: 597.688899 in epoch 166\n","Best Train Loss: 3160.442750 in epoch 243\n","  Test Loss: 8.666193\n","\n","  Mean Abs Loss: 4.761905\n","\n","  Test Loss: 8.894232\n","\n","  Mean Abs Loss: 4.819048\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4408.469968 \tValidation Loss: 1286.419489\n","  Epoch: 100 \tTraining Loss: 4278.928895 \tValidation Loss: 1139.724069\n","  Epoch: 150 \tTraining Loss: 3997.756914 \tValidation Loss: 1112.228093\n","  Epoch: 200 \tTraining Loss: 3931.422476 \tValidation Loss: 1043.719460\n","  Epoch: 250 \tTraining Loss: 3893.491355 \tValidation Loss: 1035.980397\n","  Epoch: 300 \tTraining Loss: 3798.822836 \tValidation Loss: 993.883222\n","Best Validation Loss: 625.002348 in epoch 297\n","Best Train Loss: 3033.243384 in epoch 198\n","  Test Loss: 8.947544\n","\n","  Mean Abs Loss: 4.790476\n","\n","  Test Loss: 8.838258\n","\n","  Mean Abs Loss: 4.723810\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3827.388871 \tValidation Loss: 997.422562\n","  Epoch: 100 \tTraining Loss: 3649.191281 \tValidation Loss: 593.082181\n","  Epoch: 150 \tTraining Loss: 3600.780466 \tValidation Loss: 963.116896\n","  Epoch: 200 \tTraining Loss: 3532.506037 \tValidation Loss: 979.608213\n","  Epoch: 250 \tTraining Loss: 3499.690093 \tValidation Loss: 966.678598\n","  Epoch: 300 \tTraining Loss: 3400.773805 \tValidation Loss: 995.701803\n","Best Validation Loss: 513.128024 in epoch 283\n","Best Train Loss: 2672.948880 in epoch 192\n","  Test Loss: 8.421829\n","\n","  Mean Abs Loss: 4.657143\n","\n","  Test Loss: 8.674351\n","\n","  Mean Abs Loss: 4.904762\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4461.566535 \tValidation Loss: 1131.866437\n","  Epoch: 100 \tTraining Loss: 4045.790919 \tValidation Loss: 932.007803\n","  Epoch: 150 \tTraining Loss: 3997.572134 \tValidation Loss: 1029.899437\n","  Epoch: 200 \tTraining Loss: 3887.040728 \tValidation Loss: 1014.299472\n","  Epoch: 250 \tTraining Loss: 4015.108032 \tValidation Loss: 820.619890\n","  Epoch: 300 \tTraining Loss: 3966.474190 \tValidation Loss: 1007.389668\n","Best Validation Loss: 625.011262 in epoch 230\n","Best Train Loss: 3560.300208 in epoch 178\n","  Test Loss: 8.835891\n","\n","  Mean Abs Loss: 4.752381\n","\n","  Test Loss: 8.728319\n","\n","  Mean Abs Loss: 4.695238\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3866.816222 \tValidation Loss: 823.039930\n","  Epoch: 100 \tTraining Loss: 3800.780607 \tValidation Loss: 1007.817120\n","  Epoch: 150 \tTraining Loss: 3632.718478 \tValidation Loss: 943.152364\n","  Epoch: 200 \tTraining Loss: 3632.878605 \tValidation Loss: 971.550274\n","  Epoch: 250 \tTraining Loss: 3535.260373 \tValidation Loss: 946.393992\n","  Epoch: 300 \tTraining Loss: 3451.145104 \tValidation Loss: 926.648507\n","Best Validation Loss: 594.917392 in epoch 251\n","Best Train Loss: 2925.794540 in epoch 84\n","  Test Loss: 8.666659\n","\n","  Mean Abs Loss: 4.676190\n","\n","  Test Loss: 8.641184\n","\n","  Mean Abs Loss: 4.752381\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4713.197938 \tValidation Loss: 1238.496866\n","  Epoch: 100 \tTraining Loss: 4438.707016 \tValidation Loss: 1114.451312\n","  Epoch: 150 \tTraining Loss: 4263.834151 \tValidation Loss: 1059.613199\n","  Epoch: 200 \tTraining Loss: 4065.009786 \tValidation Loss: 1062.555489\n","  Epoch: 250 \tTraining Loss: 3976.824919 \tValidation Loss: 859.303568\n","  Epoch: 300 \tTraining Loss: 3972.284383 \tValidation Loss: 1035.483807\n","Best Validation Loss: 652.872655 in epoch 245\n","Best Train Loss: 3279.929282 in epoch 159\n","  Test Loss: 10.511799\n","\n","  Mean Abs Loss: 6.571429\n","\n","  Test Loss: 10.114568\n","\n","  Mean Abs Loss: 6.295238\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3955.610812 \tValidation Loss: 1174.292423\n","  Epoch: 100 \tTraining Loss: 3761.672902 \tValidation Loss: 969.850450\n","  Epoch: 150 \tTraining Loss: 3571.502726 \tValidation Loss: 984.125750\n","  Epoch: 200 \tTraining Loss: 3540.945663 \tValidation Loss: 981.704175\n","  Epoch: 250 \tTraining Loss: 3547.509613 \tValidation Loss: 1006.416567\n","  Epoch: 300 \tTraining Loss: 3494.873906 \tValidation Loss: 1022.077000\n","Best Validation Loss: 607.293344 in epoch 167\n","Best Train Loss: 3266.971943 in epoch 292\n","  Test Loss: 8.888318\n","\n","  Mean Abs Loss: 4.704762\n","\n","  Test Loss: 8.682241\n","\n","  Mean Abs Loss: 4.542857\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4435.345280 \tValidation Loss: 1244.774700\n","  Epoch: 100 \tTraining Loss: 4203.672546 \tValidation Loss: 1182.861204\n","  Epoch: 150 \tTraining Loss: 3999.411613 \tValidation Loss: 1075.137192\n","  Epoch: 200 \tTraining Loss: 3992.711942 \tValidation Loss: 1152.048389\n","  Epoch: 250 \tTraining Loss: 3829.707097 \tValidation Loss: 1077.012737\n","  Epoch: 300 \tTraining Loss: 3906.008997 \tValidation Loss: 1059.253592\n","Best Validation Loss: 664.832647 in epoch 298\n","Best Train Loss: 3129.900497 in epoch 246\n","  Test Loss: 9.262323\n","\n","  Mean Abs Loss: 5.180952\n","\n","  Test Loss: 9.109871\n","\n","  Mean Abs Loss: 5.057143\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 4000.087296 \tValidation Loss: 1007.396133\n","  Epoch: 100 \tTraining Loss: 3791.454482 \tValidation Loss: 1013.068882\n","  Epoch: 150 \tTraining Loss: 3717.694915 \tValidation Loss: 987.188645\n","  Epoch: 200 \tTraining Loss: 3533.726055 \tValidation Loss: 966.623509\n","  Epoch: 250 \tTraining Loss: 3551.730110 \tValidation Loss: 611.055560\n","  Epoch: 300 \tTraining Loss: 3553.276718 \tValidation Loss: 997.373733\n","Best Validation Loss: 609.757615 in epoch 296\n","Best Train Loss: 2768.844837 in epoch 218\n","  Test Loss: 8.695360\n","\n","  Mean Abs Loss: 4.638095\n","\n","  Test Loss: 8.679949\n","\n","  Mean Abs Loss: 4.809524\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4515.748257 \tValidation Loss: 1236.173539\n","  Epoch: 100 \tTraining Loss: 4228.675145 \tValidation Loss: 1176.541217\n","  Epoch: 150 \tTraining Loss: 3942.035025 \tValidation Loss: 1104.174922\n","  Epoch: 200 \tTraining Loss: 4083.055270 \tValidation Loss: 1101.372114\n","  Epoch: 250 \tTraining Loss: 3885.832459 \tValidation Loss: 1094.712563\n","  Epoch: 300 \tTraining Loss: 3914.236838 \tValidation Loss: 1089.785733\n","Best Validation Loss: 747.175300 in epoch 162\n","Best Train Loss: 3080.258180 in epoch 245\n","  Test Loss: 9.355998\n","\n","  Mean Abs Loss: 5.400000\n","\n","  Test Loss: 9.839119\n","\n","  Mean Abs Loss: 5.876190\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3711.950941 \tValidation Loss: 1011.650874\n","  Epoch: 100 \tTraining Loss: 3676.579781 \tValidation Loss: 993.770874\n","  Epoch: 150 \tTraining Loss: 3655.203425 \tValidation Loss: 964.841286\n","  Epoch: 200 \tTraining Loss: 3528.617954 \tValidation Loss: 933.635124\n","  Epoch: 250 \tTraining Loss: 3544.722833 \tValidation Loss: 762.749114\n","  Epoch: 300 \tTraining Loss: 3514.407101 \tValidation Loss: 946.939665\n","Best Validation Loss: 574.982605 in epoch 207\n","Best Train Loss: 2821.267731 in epoch 172\n","  Test Loss: 8.885025\n","\n","  Mean Abs Loss: 4.790476\n","\n","  Test Loss: 8.960057\n","\n","  Mean Abs Loss: 4.866667\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4844.378793 \tValidation Loss: 1309.167839\n","  Epoch: 100 \tTraining Loss: 4213.862455 \tValidation Loss: 1063.923914\n","  Epoch: 150 \tTraining Loss: 4149.638806 \tValidation Loss: 1052.019184\n","  Epoch: 200 \tTraining Loss: 4226.094660 \tValidation Loss: 1038.413346\n","  Epoch: 250 \tTraining Loss: 4018.177101 \tValidation Loss: 1003.473814\n","  Epoch: 300 \tTraining Loss: 3971.785111 \tValidation Loss: 1026.104176\n","Best Validation Loss: 473.779145 in epoch 223\n","Best Train Loss: 3069.908579 in epoch 263\n","  Test Loss: 8.792775\n","\n","  Mean Abs Loss: 4.790476\n","\n","  Test Loss: 8.849383\n","\n","  Mean Abs Loss: 4.847619\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3776.044582 \tValidation Loss: 1006.060004\n","  Epoch: 100 \tTraining Loss: 3696.704143 \tValidation Loss: 974.019823\n","  Epoch: 150 \tTraining Loss: 3669.570022 \tValidation Loss: 1021.538830\n","  Epoch: 200 \tTraining Loss: 3517.737349 \tValidation Loss: 954.827029\n","  Epoch: 250 \tTraining Loss: 3531.569994 \tValidation Loss: 969.510904\n","  Epoch: 300 \tTraining Loss: 3446.602230 \tValidation Loss: 973.070389\n","Best Validation Loss: 580.239314 in epoch 89\n","Best Train Loss: 3304.068619 in epoch 172\n","  Test Loss: 8.718610\n","\n","  Mean Abs Loss: 4.790476\n","\n","  Test Loss: 8.712114\n","\n","  Mean Abs Loss: 4.742857\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4547.114234 \tValidation Loss: 1220.367916\n","  Epoch: 100 \tTraining Loss: 4152.529901 \tValidation Loss: 1168.336999\n","  Epoch: 150 \tTraining Loss: 4123.393272 \tValidation Loss: 1152.874545\n","  Epoch: 200 \tTraining Loss: 3954.218653 \tValidation Loss: 1105.882946\n","  Epoch: 250 \tTraining Loss: 4019.069692 \tValidation Loss: 1062.990927\n","  Epoch: 300 \tTraining Loss: 3867.609866 \tValidation Loss: 1066.486376\n","Best Validation Loss: 663.881727 in epoch 287\n","Best Train Loss: 3632.885708 in epoch 281\n","  Test Loss: 9.150467\n","\n","  Mean Abs Loss: 5.190476\n","\n","  Test Loss: 9.109303\n","\n","  Mean Abs Loss: 5.219048\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3803.940464 \tValidation Loss: 992.360393\n","  Epoch: 100 \tTraining Loss: 3767.586082 \tValidation Loss: 993.561099\n","  Epoch: 150 \tTraining Loss: 3588.302892 \tValidation Loss: 975.263590\n","  Epoch: 200 \tTraining Loss: 3532.508915 \tValidation Loss: 783.259683\n","  Epoch: 250 \tTraining Loss: 3567.654446 \tValidation Loss: 953.183091\n","  Epoch: 300 \tTraining Loss: 3566.469278 \tValidation Loss: 1007.376461\n","Best Validation Loss: 518.277493 in epoch 227\n","Best Train Loss: 2827.763051 in epoch 187\n","  Test Loss: 8.752575\n","\n","  Mean Abs Loss: 4.733333\n","\n","  Test Loss: 8.759474\n","\n","  Mean Abs Loss: 4.685714\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4462.198761 \tValidation Loss: 1131.330302\n","  Epoch: 100 \tTraining Loss: 4180.925694 \tValidation Loss: 948.909744\n","  Epoch: 150 \tTraining Loss: 4024.209345 \tValidation Loss: 1017.631235\n","  Epoch: 200 \tTraining Loss: 3938.495816 \tValidation Loss: 1012.488264\n","  Epoch: 250 \tTraining Loss: 3921.088219 \tValidation Loss: 1039.106906\n","  Epoch: 300 \tTraining Loss: 3972.496058 \tValidation Loss: 1015.464718\n","Best Validation Loss: 539.203931 in epoch 111\n","Best Train Loss: 3416.308910 in epoch 225\n","  Test Loss: 8.857241\n","\n","  Mean Abs Loss: 4.980952\n","\n","  Test Loss: 8.890937\n","\n","  Mean Abs Loss: 5.057143\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3860.627064 \tValidation Loss: 1014.779514\n","  Epoch: 100 \tTraining Loss: 3697.145688 \tValidation Loss: 984.907373\n","  Epoch: 150 \tTraining Loss: 3662.155976 \tValidation Loss: 973.105847\n","  Epoch: 200 \tTraining Loss: 3490.710487 \tValidation Loss: 961.523358\n","  Epoch: 250 \tTraining Loss: 3445.745328 \tValidation Loss: 967.317315\n","  Epoch: 300 \tTraining Loss: 3503.143070 \tValidation Loss: 828.209251\n","Best Validation Loss: 550.562215 in epoch 27\n","Best Train Loss: 2768.471552 in epoch 249\n","  Test Loss: 8.762906\n","\n","  Mean Abs Loss: 4.685714\n","\n","  Test Loss: 8.863340\n","\n","  Mean Abs Loss: 4.647619\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4545.213522 \tValidation Loss: 1263.443316\n","  Epoch: 100 \tTraining Loss: 4247.809955 \tValidation Loss: 1126.805666\n","  Epoch: 150 \tTraining Loss: 4027.425986 \tValidation Loss: 1055.262670\n","  Epoch: 200 \tTraining Loss: 3986.609026 \tValidation Loss: 1041.002964\n","  Epoch: 250 \tTraining Loss: 3992.022355 \tValidation Loss: 1014.386244\n","  Epoch: 300 \tTraining Loss: 3854.931105 \tValidation Loss: 1029.045336\n","Best Validation Loss: 632.838138 in epoch 224\n","Best Train Loss: 3139.632208 in epoch 180\n","  Test Loss: 9.747950\n","\n","  Mean Abs Loss: 5.457143\n","\n","  Test Loss: 9.638127\n","\n","  Mean Abs Loss: 5.400000\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3962.866928 \tValidation Loss: 860.021560\n","  Epoch: 100 \tTraining Loss: 3862.478820 \tValidation Loss: 853.509962\n","  Epoch: 150 \tTraining Loss: 3623.184575 \tValidation Loss: 978.155390\n","  Epoch: 200 \tTraining Loss: 3630.303684 \tValidation Loss: 946.335095\n","  Epoch: 250 \tTraining Loss: 3604.364042 \tValidation Loss: 1003.697280\n","  Epoch: 300 \tTraining Loss: 3493.683175 \tValidation Loss: 976.423110\n","Best Validation Loss: 573.793384 in epoch 118\n","Best Train Loss: 2707.393924 in epoch 228\n","  Test Loss: 8.943871\n","\n","  Mean Abs Loss: 4.828571\n","\n","  Test Loss: 8.983002\n","\n","  Mean Abs Loss: 4.980952\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4576.642439 \tValidation Loss: 1239.747557\n","  Epoch: 100 \tTraining Loss: 4153.162722 \tValidation Loss: 1155.906402\n","  Epoch: 150 \tTraining Loss: 4100.255770 \tValidation Loss: 1128.774231\n","  Epoch: 200 \tTraining Loss: 3981.449988 \tValidation Loss: 965.602232\n","  Epoch: 250 \tTraining Loss: 3891.446688 \tValidation Loss: 1063.201711\n","  Epoch: 300 \tTraining Loss: 3887.459202 \tValidation Loss: 630.066965\n","Best Validation Loss: 459.180641 in epoch 202\n","Best Train Loss: 3133.376093 in epoch 146\n","  Test Loss: 9.871113\n","\n","  Mean Abs Loss: 5.819048\n","\n","  Test Loss: 9.524693\n","\n","  Mean Abs Loss: 5.561905\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3893.421284 \tValidation Loss: 1045.811601\n","  Epoch: 100 \tTraining Loss: 3823.384750 \tValidation Loss: 801.220151\n","  Epoch: 150 \tTraining Loss: 3634.694921 \tValidation Loss: 987.757678\n","  Epoch: 200 \tTraining Loss: 3593.986613 \tValidation Loss: 956.085542\n","  Epoch: 250 \tTraining Loss: 3507.754163 \tValidation Loss: 941.612906\n","  Epoch: 300 \tTraining Loss: 3571.474875 \tValidation Loss: 941.487834\n","Best Validation Loss: 578.460719 in epoch 211\n","Best Train Loss: 2887.321319 in epoch 117\n","  Test Loss: 8.824204\n","\n","  Mean Abs Loss: 4.961905\n","\n","  Test Loss: 8.673954\n","\n","  Mean Abs Loss: 4.771429\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4451.679673 \tValidation Loss: 1186.725168\n","  Epoch: 100 \tTraining Loss: 4103.896579 \tValidation Loss: 1126.484544\n","  Epoch: 150 \tTraining Loss: 3868.964695 \tValidation Loss: 1037.361191\n","  Epoch: 200 \tTraining Loss: 3891.387563 \tValidation Loss: 995.450502\n","  Epoch: 250 \tTraining Loss: 3849.106482 \tValidation Loss: 1066.984251\n","  Epoch: 300 \tTraining Loss: 3781.240653 \tValidation Loss: 849.984641\n","Best Validation Loss: 653.966450 in epoch 174\n","Best Train Loss: 2958.657603 in epoch 297\n","  Test Loss: 9.055924\n","\n","  Mean Abs Loss: 5.133333\n","\n","  Test Loss: 8.977440\n","\n","  Mean Abs Loss: 5.123810\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3790.386102 \tValidation Loss: 998.584066\n","  Epoch: 100 \tTraining Loss: 3769.232434 \tValidation Loss: 996.154955\n","  Epoch: 150 \tTraining Loss: 3617.940656 \tValidation Loss: 958.370782\n","  Epoch: 200 \tTraining Loss: 3575.432158 \tValidation Loss: 956.958467\n","  Epoch: 250 \tTraining Loss: 3513.289408 \tValidation Loss: 971.410773\n","  Epoch: 300 \tTraining Loss: 3652.561310 \tValidation Loss: 968.599571\n","Best Validation Loss: 524.634556 in epoch 28\n","Best Train Loss: 2717.934303 in epoch 294\n","  Test Loss: 8.846931\n","\n","  Mean Abs Loss: 4.961905\n","\n","  Test Loss: 8.832524\n","\n","  Mean Abs Loss: 4.800000\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4442.374658 \tValidation Loss: 802.257786\n","  Epoch: 100 \tTraining Loss: 4068.483416 \tValidation Loss: 1063.494327\n","  Epoch: 150 \tTraining Loss: 3991.334773 \tValidation Loss: 1061.378342\n","  Epoch: 200 \tTraining Loss: 4002.456939 \tValidation Loss: 1046.252512\n","  Epoch: 250 \tTraining Loss: 3996.245533 \tValidation Loss: 1047.920507\n","  Epoch: 300 \tTraining Loss: 4001.520369 \tValidation Loss: 1053.628644\n","Best Validation Loss: 643.748114 in epoch 122\n","Best Train Loss: 3518.375740 in epoch 121\n","  Test Loss: 10.230189\n","\n","  Mean Abs Loss: 5.990476\n","\n","  Test Loss: 10.139178\n","\n","  Mean Abs Loss: 5.895238\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3774.053074 \tValidation Loss: 1030.002918\n","  Epoch: 100 \tTraining Loss: 3718.443898 \tValidation Loss: 977.348506\n","  Epoch: 150 \tTraining Loss: 3616.920284 \tValidation Loss: 595.629479\n","  Epoch: 200 \tTraining Loss: 3436.468431 \tValidation Loss: 928.238550\n","  Epoch: 250 \tTraining Loss: 3491.603952 \tValidation Loss: 949.017830\n","  Epoch: 300 \tTraining Loss: 2935.601186 \tValidation Loss: 893.445803\n","Best Validation Loss: 561.241307 in epoch 170\n","Best Train Loss: 2755.229322 in epoch 239\n","  Test Loss: 8.591594\n","\n","  Mean Abs Loss: 4.666667\n","\n","  Test Loss: 8.706432\n","\n","  Mean Abs Loss: 4.609524\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4397.752699 \tValidation Loss: 1167.119413\n","  Epoch: 100 \tTraining Loss: 4155.902988 \tValidation Loss: 1177.441500\n","  Epoch: 150 \tTraining Loss: 4064.180205 \tValidation Loss: 1106.526871\n","  Epoch: 200 \tTraining Loss: 3832.348652 \tValidation Loss: 1033.297910\n","  Epoch: 250 \tTraining Loss: 3868.348015 \tValidation Loss: 1053.537301\n","  Epoch: 300 \tTraining Loss: 3816.422711 \tValidation Loss: 699.751504\n","Best Validation Loss: 512.776331 in epoch 153\n","Best Train Loss: 2989.189570 in epoch 294\n","  Test Loss: 9.437088\n","\n","  Mean Abs Loss: 5.352381\n","\n","  Test Loss: 9.697412\n","\n","  Mean Abs Loss: 5.647619\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3813.695031 \tValidation Loss: 1012.744787\n","  Epoch: 100 \tTraining Loss: 3689.008686 \tValidation Loss: 949.710347\n","  Epoch: 150 \tTraining Loss: 3562.902185 \tValidation Loss: 971.042256\n","  Epoch: 200 \tTraining Loss: 3409.584697 \tValidation Loss: 910.322410\n","  Epoch: 250 \tTraining Loss: 3534.977292 \tValidation Loss: 971.786305\n","  Epoch: 300 \tTraining Loss: 3502.298421 \tValidation Loss: 944.584352\n","Best Validation Loss: 516.862400 in epoch 53\n","Best Train Loss: 2681.363192 in epoch 275\n","  Test Loss: 8.714185\n","\n","  Mean Abs Loss: 4.580952\n","\n","  Test Loss: 8.839328\n","\n","  Mean Abs Loss: 4.828571\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4600.183334 \tValidation Loss: 1193.821786\n","  Epoch: 100 \tTraining Loss: 3973.459504 \tValidation Loss: 1106.689996\n","  Epoch: 150 \tTraining Loss: 4047.810209 \tValidation Loss: 1071.010423\n","  Epoch: 200 \tTraining Loss: 3836.043644 \tValidation Loss: 1048.962091\n","  Epoch: 250 \tTraining Loss: 3810.490953 \tValidation Loss: 1055.649055\n","  Epoch: 300 \tTraining Loss: 3792.620674 \tValidation Loss: 638.991033\n","Best Validation Loss: 486.672151 in epoch 143\n","Best Train Loss: 3126.272166 in epoch 136\n","  Test Loss: 8.757631\n","\n","  Mean Abs Loss: 4.876190\n","\n","  Test Loss: 8.736899\n","\n","  Mean Abs Loss: 4.914286\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 4017.721405 \tValidation Loss: 1006.192295\n","  Epoch: 100 \tTraining Loss: 3896.741454 \tValidation Loss: 986.895719\n","  Epoch: 150 \tTraining Loss: 3758.886183 \tValidation Loss: 988.769105\n","  Epoch: 200 \tTraining Loss: 3586.574286 \tValidation Loss: 957.551049\n","  Epoch: 250 \tTraining Loss: 3598.856900 \tValidation Loss: 933.099145\n","  Epoch: 300 \tTraining Loss: 3566.497348 \tValidation Loss: 951.371264\n","Best Validation Loss: 580.551598 in epoch 295\n","Best Train Loss: 2755.216531 in epoch 236\n","  Test Loss: 8.788919\n","\n","  Mean Abs Loss: 4.733333\n","\n","  Test Loss: 8.772558\n","\n","  Mean Abs Loss: 4.742857\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4597.289089 \tValidation Loss: 1176.352936\n","  Epoch: 100 \tTraining Loss: 4259.495268 \tValidation Loss: 1158.072183\n","  Epoch: 150 \tTraining Loss: 4076.065647 \tValidation Loss: 1041.751246\n","  Epoch: 200 \tTraining Loss: 4047.307442 \tValidation Loss: 1128.335392\n","  Epoch: 250 \tTraining Loss: 3986.710190 \tValidation Loss: 1072.771089\n","  Epoch: 300 \tTraining Loss: 3979.747145 \tValidation Loss: 1069.032870\n","Best Validation Loss: 559.583998 in epoch 265\n","Best Train Loss: 3070.831267 in epoch 295\n","  Test Loss: 9.273687\n","\n","  Mean Abs Loss: 5.304762\n","\n","  Test Loss: 9.099595\n","\n","  Mean Abs Loss: 5.009524\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3717.449263 \tValidation Loss: 1031.359536\n","  Epoch: 100 \tTraining Loss: 3752.524119 \tValidation Loss: 1010.637280\n","  Epoch: 150 \tTraining Loss: 3506.444826 \tValidation Loss: 958.006137\n","  Epoch: 200 \tTraining Loss: 3558.985505 \tValidation Loss: 933.639077\n","  Epoch: 250 \tTraining Loss: 3562.965938 \tValidation Loss: 957.081027\n","  Epoch: 300 \tTraining Loss: 3492.844724 \tValidation Loss: 938.092955\n","Best Validation Loss: 424.573558 in epoch 84\n","Best Train Loss: 2891.837910 in epoch 64\n","  Test Loss: 8.936805\n","\n","  Mean Abs Loss: 5.057143\n","\n","  Test Loss: 8.718351\n","\n","  Mean Abs Loss: 4.666667\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4457.446393 \tValidation Loss: 1208.629324\n","  Epoch: 100 \tTraining Loss: 4033.818733 \tValidation Loss: 889.111575\n","  Epoch: 150 \tTraining Loss: 3983.620637 \tValidation Loss: 1054.218618\n","  Epoch: 200 \tTraining Loss: 3848.951236 \tValidation Loss: 1040.062232\n","  Epoch: 250 \tTraining Loss: 3873.892353 \tValidation Loss: 655.964125\n","  Epoch: 300 \tTraining Loss: 3765.136589 \tValidation Loss: 976.953152\n","Best Validation Loss: 550.791474 in epoch 61\n","Best Train Loss: 3525.459167 in epoch 298\n","  Test Loss: 8.882309\n","\n","  Mean Abs Loss: 5.095238\n","\n","  Test Loss: 9.434332\n","\n","  Mean Abs Loss: 5.285714\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3817.170754 \tValidation Loss: 975.203389\n","  Epoch: 100 \tTraining Loss: 3653.223722 \tValidation Loss: 997.416367\n","  Epoch: 150 \tTraining Loss: 3680.697030 \tValidation Loss: 983.608304\n","  Epoch: 200 \tTraining Loss: 3542.718269 \tValidation Loss: 593.531264\n","  Epoch: 250 \tTraining Loss: 3524.463666 \tValidation Loss: 591.506132\n","  Epoch: 300 \tTraining Loss: 3574.628194 \tValidation Loss: 985.778015\n","Best Validation Loss: 580.646445 in epoch 213\n","Best Train Loss: 3268.039975 in epoch 169\n","  Test Loss: 8.554309\n","\n","  Mean Abs Loss: 4.580952\n","\n","  Test Loss: 8.693824\n","\n","  Mean Abs Loss: 4.657143\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4471.872502 \tValidation Loss: 1194.355822\n","  Epoch: 100 \tTraining Loss: 4044.055033 \tValidation Loss: 1092.515517\n","  Epoch: 150 \tTraining Loss: 3877.807788 \tValidation Loss: 1072.449607\n","  Epoch: 200 \tTraining Loss: 3862.487781 \tValidation Loss: 1065.712930\n","  Epoch: 250 \tTraining Loss: 3916.044928 \tValidation Loss: 1021.068283\n","  Epoch: 300 \tTraining Loss: 3812.413557 \tValidation Loss: 1011.695661\n","Best Validation Loss: 572.851601 in epoch 119\n","Best Train Loss: 3366.559380 in epoch 76\n","  Test Loss: 9.589781\n","\n","  Mean Abs Loss: 5.371429\n","\n","  Test Loss: 9.252273\n","\n","  Mean Abs Loss: 5.142857\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3794.869035 \tValidation Loss: 1007.665609\n","  Epoch: 100 \tTraining Loss: 3657.261056 \tValidation Loss: 970.463247\n","  Epoch: 150 \tTraining Loss: 3622.436566 \tValidation Loss: 955.537144\n","  Epoch: 200 \tTraining Loss: 3589.817710 \tValidation Loss: 942.663245\n","  Epoch: 250 \tTraining Loss: 3594.671387 \tValidation Loss: 947.783840\n","  Epoch: 300 \tTraining Loss: 3545.565998 \tValidation Loss: 943.501556\n","Best Validation Loss: 394.866124 in epoch 221\n","Best Train Loss: 2383.098116 in epoch 267\n","  Test Loss: 8.757299\n","\n","  Mean Abs Loss: 4.752381\n","\n","  Test Loss: 8.647114\n","\n","  Mean Abs Loss: 4.657143\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4419.626128 \tValidation Loss: 1250.515078\n","  Epoch: 100 \tTraining Loss: 4280.982314 \tValidation Loss: 1186.154490\n","  Epoch: 150 \tTraining Loss: 3292.843894 \tValidation Loss: 1097.590036\n","  Epoch: 200 \tTraining Loss: 3977.006546 \tValidation Loss: 1066.896864\n","  Epoch: 250 \tTraining Loss: 3939.723900 \tValidation Loss: 874.717908\n","  Epoch: 300 \tTraining Loss: 4033.048624 \tValidation Loss: 1044.458756\n","Best Validation Loss: 661.395611 in epoch 224\n","Best Train Loss: 3229.160280 in epoch 158\n","  Test Loss: 9.627744\n","\n","  Mean Abs Loss: 5.495238\n","\n","  Test Loss: 9.654215\n","\n","  Mean Abs Loss: 5.542857\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3863.189141 \tValidation Loss: 1028.252254\n","  Epoch: 100 \tTraining Loss: 3481.091237 \tValidation Loss: 994.687853\n","  Epoch: 150 \tTraining Loss: 3265.577402 \tValidation Loss: 985.969986\n","  Epoch: 200 \tTraining Loss: 3562.494243 \tValidation Loss: 993.237446\n","  Epoch: 250 \tTraining Loss: 3488.890021 \tValidation Loss: 980.323888\n","  Epoch: 300 \tTraining Loss: 3482.397092 \tValidation Loss: 772.733861\n","Best Validation Loss: 591.133736 in epoch 291\n","Best Train Loss: 2735.404653 in epoch 274\n","  Test Loss: 8.881516\n","\n","  Mean Abs Loss: 4.790476\n","\n","  Test Loss: 8.918398\n","\n","  Mean Abs Loss: 4.876190\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4491.364552 \tValidation Loss: 1209.967323\n","  Epoch: 100 \tTraining Loss: 4141.385836 \tValidation Loss: 1089.482628\n","  Epoch: 150 \tTraining Loss: 3963.717613 \tValidation Loss: 1074.522517\n","  Epoch: 200 \tTraining Loss: 3855.068577 \tValidation Loss: 1065.486352\n","  Epoch: 250 \tTraining Loss: 3953.391622 \tValidation Loss: 1043.165747\n","  Epoch: 300 \tTraining Loss: 3829.233448 \tValidation Loss: 1038.855304\n","Best Validation Loss: 546.573341 in epoch 244\n","Best Train Loss: 3088.509344 in epoch 281\n","  Test Loss: 8.829771\n","\n","  Mean Abs Loss: 5.123810\n","\n","  Test Loss: 8.871880\n","\n","  Mean Abs Loss: 5.085714\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3852.248930 \tValidation Loss: 998.559930\n","  Epoch: 100 \tTraining Loss: 3750.586615 \tValidation Loss: 953.743299\n","  Epoch: 150 \tTraining Loss: 3580.670762 \tValidation Loss: 990.007541\n","  Epoch: 200 \tTraining Loss: 3481.725743 \tValidation Loss: 983.635697\n","  Epoch: 250 \tTraining Loss: 3564.689988 \tValidation Loss: 972.582644\n","  Epoch: 300 \tTraining Loss: 3514.128151 \tValidation Loss: 594.705855\n","Best Validation Loss: 520.371975 in epoch 31\n","Best Train Loss: 2661.296092 in epoch 272\n","  Test Loss: 8.621929\n","\n","  Mean Abs Loss: 4.819048\n","\n","  Test Loss: 8.805658\n","\n","  Mean Abs Loss: 4.742857\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4548.374606 \tValidation Loss: 1233.248974\n","  Epoch: 100 \tTraining Loss: 4137.483601 \tValidation Loss: 1087.253296\n","  Epoch: 150 \tTraining Loss: 3944.301245 \tValidation Loss: 1070.048205\n","  Epoch: 200 \tTraining Loss: 3934.451547 \tValidation Loss: 1058.880481\n","  Epoch: 250 \tTraining Loss: 3639.781782 \tValidation Loss: 1071.828132\n","  Epoch: 300 \tTraining Loss: 3900.233997 \tValidation Loss: 1054.659273\n","Best Validation Loss: 654.111769 in epoch 255\n","Best Train Loss: 3059.438983 in epoch 206\n","  Test Loss: 9.427136\n","\n","  Mean Abs Loss: 5.361905\n","\n","  Test Loss: 9.387769\n","\n","  Mean Abs Loss: 5.342857\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3817.883325 \tValidation Loss: 998.307255\n","  Epoch: 100 \tTraining Loss: 3756.590668 \tValidation Loss: 996.684698\n","  Epoch: 150 \tTraining Loss: 3595.816861 \tValidation Loss: 967.584633\n","  Epoch: 200 \tTraining Loss: 3542.463591 \tValidation Loss: 932.837152\n","  Epoch: 250 \tTraining Loss: 3483.182897 \tValidation Loss: 955.710095\n","  Epoch: 300 \tTraining Loss: 3508.590099 \tValidation Loss: 820.602575\n","Best Validation Loss: 553.695543 in epoch 272\n","Best Train Loss: 2752.726460 in epoch 227\n","  Test Loss: 8.612129\n","\n","  Mean Abs Loss: 4.800000\n","\n","  Test Loss: 8.894507\n","\n","  Mean Abs Loss: 4.933333\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4720.053261 \tValidation Loss: 1283.429457\n","  Epoch: 100 \tTraining Loss: 4153.297225 \tValidation Loss: 1146.466173\n","  Epoch: 150 \tTraining Loss: 3972.345794 \tValidation Loss: 917.852688\n","  Epoch: 200 \tTraining Loss: 4034.259120 \tValidation Loss: 1083.756897\n","  Epoch: 250 \tTraining Loss: 3922.341321 \tValidation Loss: 1067.747221\n","  Epoch: 300 \tTraining Loss: 4034.742578 \tValidation Loss: 681.700050\n","Best Validation Loss: 674.004917 in epoch 275\n","Best Train Loss: 3102.271537 in epoch 198\n","  Test Loss: 9.805527\n","\n","  Mean Abs Loss: 5.676190\n","\n","  Test Loss: 9.299043\n","\n","  Mean Abs Loss: 5.104762\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3875.507137 \tValidation Loss: 1005.163826\n","  Epoch: 100 \tTraining Loss: 3706.058635 \tValidation Loss: 1005.507748\n","  Epoch: 150 \tTraining Loss: 3572.795069 \tValidation Loss: 947.244557\n","  Epoch: 200 \tTraining Loss: 3495.256104 \tValidation Loss: 963.095161\n","  Epoch: 250 \tTraining Loss: 3419.377565 \tValidation Loss: 960.315793\n","  Epoch: 300 \tTraining Loss: 3432.682263 \tValidation Loss: 918.698769\n","Best Validation Loss: 535.829825 in epoch 41\n","Best Train Loss: 2701.992833 in epoch 296\n","  Test Loss: 8.859989\n","\n","  Mean Abs Loss: 4.942857\n","\n","  Test Loss: 8.913404\n","\n","  Mean Abs Loss: 5.104762\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4641.658114 \tValidation Loss: 1234.493749\n","  Epoch: 100 \tTraining Loss: 4191.651275 \tValidation Loss: 1137.753173\n","  Epoch: 150 \tTraining Loss: 4036.040702 \tValidation Loss: 1111.253197\n","  Epoch: 200 \tTraining Loss: 3937.773533 \tValidation Loss: 1038.226950\n","  Epoch: 250 \tTraining Loss: 3794.512805 \tValidation Loss: 1051.657430\n","  Epoch: 300 \tTraining Loss: 3700.752546 \tValidation Loss: 1052.187661\n","Best Validation Loss: 647.500790 in epoch 251\n","Best Train Loss: 3105.792307 in epoch 287\n","  Test Loss: 9.040366\n","\n","  Mean Abs Loss: 5.180952\n","\n","  Test Loss: 9.197915\n","\n","  Mean Abs Loss: 5.295238\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3784.470645 \tValidation Loss: 988.347244\n","  Epoch: 100 \tTraining Loss: 3896.301240 \tValidation Loss: 994.006011\n","  Epoch: 150 \tTraining Loss: 3883.465221 \tValidation Loss: 970.697245\n","  Epoch: 200 \tTraining Loss: 3759.730525 \tValidation Loss: 965.761580\n","  Epoch: 250 \tTraining Loss: 3445.800391 \tValidation Loss: 958.002892\n","  Epoch: 300 \tTraining Loss: 3548.018571 \tValidation Loss: 980.060546\n","Best Validation Loss: 571.728944 in epoch 297\n","Best Train Loss: 2785.094487 in epoch 244\n","  Test Loss: 8.772428\n","\n","  Mean Abs Loss: 4.771429\n","\n","  Test Loss: 8.530128\n","\n","  Mean Abs Loss: 4.704762\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4504.203185 \tValidation Loss: 1217.214270\n","  Epoch: 100 \tTraining Loss: 4363.666314 \tValidation Loss: 1124.322559\n","  Epoch: 150 \tTraining Loss: 4060.390329 \tValidation Loss: 1095.962823\n","  Epoch: 200 \tTraining Loss: 4005.829244 \tValidation Loss: 1060.550214\n","  Epoch: 250 \tTraining Loss: 3797.193448 \tValidation Loss: 1030.508896\n","  Epoch: 300 \tTraining Loss: 3872.601918 \tValidation Loss: 1035.087794\n","Best Validation Loss: 624.699026 in epoch 277\n","Best Train Loss: 3114.703095 in epoch 180\n","  Test Loss: 9.447449\n","\n","  Mean Abs Loss: 5.314286\n","\n","  Test Loss: 8.765584\n","\n","  Mean Abs Loss: 5.000000\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3933.067918 \tValidation Loss: 1008.547521\n","  Epoch: 100 \tTraining Loss: 3737.154057 \tValidation Loss: 972.682074\n","  Epoch: 150 \tTraining Loss: 3384.455140 \tValidation Loss: 860.202009\n","  Epoch: 200 \tTraining Loss: 3620.687858 \tValidation Loss: 907.531192\n","  Epoch: 250 \tTraining Loss: 3575.240086 \tValidation Loss: 916.113996\n","  Epoch: 300 \tTraining Loss: 3521.554658 \tValidation Loss: 964.060509\n","Best Validation Loss: 580.738949 in epoch 152\n","Best Train Loss: 2769.543394 in epoch 155\n","  Test Loss: 8.719590\n","\n","  Mean Abs Loss: 4.695238\n","\n","  Test Loss: 8.595468\n","\n","  Mean Abs Loss: 4.647619\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4778.596496 \tValidation Loss: 1253.944484\n","  Epoch: 100 \tTraining Loss: 4293.084544 \tValidation Loss: 1058.410236\n","  Epoch: 150 \tTraining Loss: 4071.280541 \tValidation Loss: 1057.004250\n","  Epoch: 200 \tTraining Loss: 4005.644043 \tValidation Loss: 1043.295431\n","  Epoch: 250 \tTraining Loss: 3935.857279 \tValidation Loss: 1053.559518\n","  Epoch: 300 \tTraining Loss: 3880.166697 \tValidation Loss: 1056.910440\n","Best Validation Loss: 545.236112 in epoch 130\n","Best Train Loss: 2965.955992 in epoch 162\n","  Test Loss: 8.880547\n","\n","  Mean Abs Loss: 4.971429\n","\n","  Test Loss: 8.888156\n","\n","  Mean Abs Loss: 4.895238\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3947.867300 \tValidation Loss: 1282.415280\n","  Epoch: 100 \tTraining Loss: 3840.355352 \tValidation Loss: 994.467977\n","  Epoch: 150 \tTraining Loss: 3468.300498 \tValidation Loss: 985.807770\n","  Epoch: 200 \tTraining Loss: 3586.241235 \tValidation Loss: 933.960537\n","  Epoch: 250 \tTraining Loss: 3587.108363 \tValidation Loss: 948.618108\n","  Epoch: 300 \tTraining Loss: 3607.587388 \tValidation Loss: 966.968175\n","Best Validation Loss: 580.242512 in epoch 235\n","Best Train Loss: 2755.085265 in epoch 240\n","  Test Loss: 8.547437\n","\n","  Mean Abs Loss: 4.771429\n","\n","  Test Loss: 8.477018\n","\n","  Mean Abs Loss: 4.638095\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4377.549806 \tValidation Loss: 1210.788113\n","  Epoch: 100 \tTraining Loss: 3932.757426 \tValidation Loss: 1090.321221\n","  Epoch: 150 \tTraining Loss: 4011.067447 \tValidation Loss: 1056.309020\n","  Epoch: 200 \tTraining Loss: 3863.998426 \tValidation Loss: 1052.998533\n","  Epoch: 250 \tTraining Loss: 3747.068284 \tValidation Loss: 971.608290\n","  Epoch: 300 \tTraining Loss: 3856.945545 \tValidation Loss: 1014.880081\n","Best Validation Loss: 605.858135 in epoch 270\n","Best Train Loss: 3004.960043 in epoch 243\n","  Test Loss: 8.644660\n","\n","  Mean Abs Loss: 4.933333\n","\n","  Test Loss: 8.664460\n","\n","  Mean Abs Loss: 4.980952\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3705.198224 \tValidation Loss: 1015.038930\n","  Epoch: 100 \tTraining Loss: 3704.283720 \tValidation Loss: 983.534134\n","  Epoch: 150 \tTraining Loss: 3627.662121 \tValidation Loss: 972.517875\n","  Epoch: 200 \tTraining Loss: 3489.771965 \tValidation Loss: 921.492147\n","  Epoch: 250 \tTraining Loss: 3211.603847 \tValidation Loss: 912.300057\n","  Epoch: 300 \tTraining Loss: 3508.260487 \tValidation Loss: 988.629788\n","Best Validation Loss: 555.956587 in epoch 149\n","Best Train Loss: 2677.277831 in epoch 265\n","  Test Loss: 8.843798\n","\n","  Mean Abs Loss: 4.857143\n","\n","  Test Loss: 8.686026\n","\n","  Mean Abs Loss: 4.666667\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4644.699468 \tValidation Loss: 1263.101048\n","  Epoch: 100 \tTraining Loss: 4272.702934 \tValidation Loss: 1160.161346\n","  Epoch: 150 \tTraining Loss: 4016.357976 \tValidation Loss: 1088.574340\n","  Epoch: 200 \tTraining Loss: 4006.018892 \tValidation Loss: 1059.072165\n","  Epoch: 250 \tTraining Loss: 3992.382725 \tValidation Loss: 804.472067\n","  Epoch: 300 \tTraining Loss: 3966.175910 \tValidation Loss: 1025.040766\n","Best Validation Loss: 628.631473 in epoch 213\n","Best Train Loss: 3518.287136 in epoch 219\n","  Test Loss: 9.384944\n","\n","  Mean Abs Loss: 5.304762\n","\n","  Test Loss: 9.394745\n","\n","  Mean Abs Loss: 5.295238\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3836.120850 \tValidation Loss: 1019.617184\n","  Epoch: 100 \tTraining Loss: 3710.868731 \tValidation Loss: 962.636233\n","  Epoch: 150 \tTraining Loss: 3597.832074 \tValidation Loss: 924.993314\n","  Epoch: 200 \tTraining Loss: 3557.402130 \tValidation Loss: 932.565548\n","  Epoch: 250 \tTraining Loss: 3570.361123 \tValidation Loss: 947.904296\n","  Epoch: 300 \tTraining Loss: 3449.947496 \tValidation Loss: 964.862270\n","Best Validation Loss: 406.452485 in epoch 128\n","Best Train Loss: 2847.770860 in epoch 147\n","  Test Loss: 8.620731\n","\n","  Mean Abs Loss: 4.714286\n","\n","  Test Loss: 8.751807\n","\n","  Mean Abs Loss: 4.685714\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4485.475041 \tValidation Loss: 1144.964548\n","  Epoch: 100 \tTraining Loss: 4155.514225 \tValidation Loss: 1082.066182\n","  Epoch: 150 \tTraining Loss: 4042.954967 \tValidation Loss: 1019.300329\n","  Epoch: 200 \tTraining Loss: 3945.967760 \tValidation Loss: 917.801717\n","  Epoch: 250 \tTraining Loss: 4033.458627 \tValidation Loss: 1022.141412\n","  Epoch: 300 \tTraining Loss: 3888.799481 \tValidation Loss: 1009.143766\n","Best Validation Loss: 628.337128 in epoch 274\n","Best Train Loss: 2992.541783 in epoch 290\n","  Test Loss: 8.626191\n","\n","  Mean Abs Loss: 4.704762\n","\n","  Test Loss: 8.674611\n","\n","  Mean Abs Loss: 4.704762\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3983.121268 \tValidation Loss: 970.944490\n","  Epoch: 100 \tTraining Loss: 3812.757314 \tValidation Loss: 966.989468\n","  Epoch: 150 \tTraining Loss: 3557.118523 \tValidation Loss: 593.809745\n","  Epoch: 200 \tTraining Loss: 3545.738544 \tValidation Loss: 932.379288\n","  Epoch: 250 \tTraining Loss: 3550.486540 \tValidation Loss: 973.216412\n","  Epoch: 300 \tTraining Loss: 3439.084799 \tValidation Loss: 943.856395\n","Best Validation Loss: 401.217116 in epoch 211\n","Best Train Loss: 2652.617933 in epoch 271\n","  Test Loss: 8.608468\n","\n","  Mean Abs Loss: 4.704762\n","\n","  Test Loss: 8.582452\n","\n","  Mean Abs Loss: 4.666667\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4627.644956 \tValidation Loss: 1155.164400\n","  Epoch: 100 \tTraining Loss: 4100.740713 \tValidation Loss: 1066.498353\n","  Epoch: 150 \tTraining Loss: 3975.250951 \tValidation Loss: 1072.232410\n","  Epoch: 200 \tTraining Loss: 3889.611608 \tValidation Loss: 1039.221892\n","  Epoch: 250 \tTraining Loss: 3926.591203 \tValidation Loss: 1049.462962\n","  Epoch: 300 \tTraining Loss: 3826.349083 \tValidation Loss: 1037.912714\n","Best Validation Loss: 631.298369 in epoch 235\n","Best Train Loss: 3579.933080 in epoch 223\n","  Test Loss: 8.791305\n","\n","  Mean Abs Loss: 4.695238\n","\n","  Test Loss: 8.754464\n","\n","  Mean Abs Loss: 4.695238\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 4007.910162 \tValidation Loss: 1033.978768\n","  Epoch: 100 \tTraining Loss: 3686.926871 \tValidation Loss: 942.846265\n","  Epoch: 150 \tTraining Loss: 3680.076310 \tValidation Loss: 865.217086\n","  Epoch: 200 \tTraining Loss: 3568.194347 \tValidation Loss: 974.681060\n","  Epoch: 250 \tTraining Loss: 3601.541034 \tValidation Loss: 976.539055\n","  Epoch: 300 \tTraining Loss: 3397.246941 \tValidation Loss: 958.658256\n","Best Validation Loss: 566.315169 in epoch 90\n","Best Train Loss: 3285.745050 in epoch 137\n","  Test Loss: 8.791404\n","\n","  Mean Abs Loss: 4.657143\n","\n","  Test Loss: 8.688544\n","\n","  Mean Abs Loss: 4.628571\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4442.300234 \tValidation Loss: 1211.322331\n","  Epoch: 100 \tTraining Loss: 4246.781072 \tValidation Loss: 1169.157631\n","  Epoch: 150 \tTraining Loss: 4127.924226 \tValidation Loss: 1113.670484\n","  Epoch: 200 \tTraining Loss: 3940.693605 \tValidation Loss: 1063.530121\n","  Epoch: 250 \tTraining Loss: 3859.487752 \tValidation Loss: 1052.033687\n","  Epoch: 300 \tTraining Loss: 3868.602567 \tValidation Loss: 1031.314793\n","Best Validation Loss: 462.494225 in epoch 185\n","Best Train Loss: 3088.622640 in epoch 161\n","  Test Loss: 10.371359\n","\n","  Mean Abs Loss: 6.228571\n","\n","  Test Loss: 10.227276\n","\n","  Mean Abs Loss: 5.857143\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3775.119882 \tValidation Loss: 1018.171467\n","  Epoch: 100 \tTraining Loss: 3640.461436 \tValidation Loss: 955.934502\n","  Epoch: 150 \tTraining Loss: 3765.309606 \tValidation Loss: 968.491042\n","  Epoch: 200 \tTraining Loss: 3660.927855 \tValidation Loss: 947.636323\n","  Epoch: 250 \tTraining Loss: 3661.868604 \tValidation Loss: 931.201127\n","  Epoch: 300 \tTraining Loss: 3570.710036 \tValidation Loss: 954.583237\n","Best Validation Loss: 539.794239 in epoch 101\n","Best Train Loss: 2911.246755 in epoch 164\n","  Test Loss: 8.813084\n","\n","  Mean Abs Loss: 4.704762\n","\n","  Test Loss: 8.862479\n","\n","  Mean Abs Loss: 4.857143\n","\n","\n","Combined Dimension: 40\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4869.484658 \tValidation Loss: 1385.534436\n","  Epoch: 100 \tTraining Loss: 4267.703257 \tValidation Loss: 1141.720479\n","  Epoch: 150 \tTraining Loss: 4063.995632 \tValidation Loss: 664.403534\n","  Epoch: 200 \tTraining Loss: 3978.419288 \tValidation Loss: 1024.860010\n","  Epoch: 250 \tTraining Loss: 3159.434300 \tValidation Loss: 1003.656676\n","  Epoch: 300 \tTraining Loss: 3880.605158 \tValidation Loss: 1014.382068\n","Best Validation Loss: 524.730120 in epoch 266\n","Best Train Loss: 3096.770297 in epoch 228\n","  Test Loss: 9.491632\n","\n","  Mean Abs Loss: 5.333333\n","\n","  Test Loss: 9.543165\n","\n","  Mean Abs Loss: 5.323810\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Liic3BnEcpyt","colab_type":"code","outputId":"7071bf53-f153-4128-f1d1-1190e4d248fc","executionInfo":{"status":"ok","timestamp":1587004697353,"user_tz":300,"elapsed":1873,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["print(overall_test_mean_error)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["4.3619047619047615\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"b0lbRMJT2dxM","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X3b4qDXF2lgK","colab_type":"text"},"source":["Combined Dimension: 30\n","\n","LSTM Output Dimension: 10\n","\n","Categorical Dimension: 10\n","\n","Numerical Dimension: 15\n","\n","LSTM Dimension: 21\n","\n","Learning Rate: 0.001"]},{"cell_type":"code","metadata":{"id":"WCswgkQY2mMU","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}