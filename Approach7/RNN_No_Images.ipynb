{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RNN_No_Images.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMlqqGmz8y/oFDW0uzc/oZT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"FUEm9QBvYyXr","colab_type":"code","colab":{}},"source":["# Epale Mario! mi piernita\n","\n","import argparse\n","import yaml\n","import time\n","import datetime\n","import cv2\n","import numpy as np\n","import pandas as pd\n","import random\n","\n","from skimage import io, transform\n","import matplotlib.pyplot as plt\n","from scipy.ndimage import zoom\n","from scipy import ndimage, misc\n","\n","import torch\n","import torch.utils.data\n","from torch import nn, optim\n","from torch.nn import functional as F\n","from torchvision import datasets, transforms, utils\n","from torchvision.utils import save_image\n","from torch.utils.data import Dataset, DataLoader, TensorDataset\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import torchvision\n","import torch\n","from torch.autograd import Variable\n","import torch.optim as optim\n","\n","from google.colab import drive\n","import os\n","\n","import warnings\n","from collections import defaultdict\n","\n","from scipy import stats\n","from sklearn.metrics import mean_absolute_error"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vkVVfsA_Y9x5","colab_type":"code","outputId":"a378a905-28fb-49b5-ebe0-65f21521822a","executionInfo":{"status":"ok","timestamp":1586981530489,"user_tz":300,"elapsed":121772,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":128}},"source":["drive.mount('/content/drive', force_remount = True)\n","os.chdir('/content/drive/My Drive/Mosquito-Tec/DA-RNN')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mAFzqb7tZAwM","colab_type":"code","colab":{}},"source":["# Open Data\n","\n","train_csv_file = '/content/drive/My Drive/Colab/mosquito/Final_Mosquito_train6_W.csv'\n","train_frame = pd.read_csv(train_csv_file)\n","\n","train_frame[\"TRAPSET\"] = pd.to_datetime(train_frame[\"TRAPSET\"])\n","train_frame[\"TRAPCOLLECT\"] = pd.to_datetime(train_frame[\"TRAPCOLLECT\"])\n","\n","train_frame[\"TRAPDAYS\"] = (train_frame[\"TRAPCOLLECT\"] - train_frame[\"TRAPSET\"]).dt.days\n","\n","train_frame[\"TRAPSET\"] = train_frame[\"TRAPSET\"].dt.week\n","train_frame[\"TRAPCOLLECT\"] = train_frame[\"TRAPCOLLECT\"].dt.week\n","\n","# Test data\n","\n","test_csv_file = '/content/drive/My Drive/Colab/mosquito/Final_Mosquito_test6_W.csv'\n","test_frame = pd.read_csv(test_csv_file)\n","\n","test_frame[\"TRAPSET\"] = pd.to_datetime(test_frame[\"TRAPSET\"])\n","test_frame[\"TRAPCOLLECT\"] = pd.to_datetime(test_frame[\"TRAPCOLLECT\"])\n","\n","test_frame[\"TRAPDAYS\"] = (test_frame[\"TRAPCOLLECT\"] - test_frame[\"TRAPSET\"]).dt.days\n","\n","test_frame[\"TRAPSET\"] = test_frame[\"TRAPSET\"].dt.week\n","test_frame[\"TRAPCOLLECT\"] = test_frame[\"TRAPCOLLECT\"].dt.week"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W8m5PuFCRrfe","colab_type":"code","colab":{}},"source":["non_temporal_columns = [\"OBJECTID\", \"X\", \"Y\", \"TRAPTYPE\", \"ATTRACTANTUSED\",\n","                        \"TRAPID\", \"LATITUDE\", \"LONGITUDE\", \"ADDRESS\", \"TOWN\",\n","                        \"STATE\", \"COUNTY\", \"TRAPSITE\", \"TRAPSET\", \"SETTIMEOFDAY\",\n","                        \"YEAR\", \"TRAPCOLLECT\", \"COLLECTTIMEOFDAY\", \"GENUS\",\n","                        \"SPECIES\", \"LIFESTAGE\", \"EGGSCOLLECTED\", \"LARVAECOLLECTED\",\n","                        \"PUPAECOLLECTED\", \"REPORTDATE\", \"TRAPDAYS\"]\n","\n","non_num_columns = [\"LATITUDE\", \"LONGITUDE\", \"TRAPDAYS\", \"TRAPSET\"]\n","\n","categorical_columnsT = [\"TRAPTYPE\", \"ATTRACTANTSUSED\", \"TRAPID\", \"ADDRESS\", \"TOWN\",\n","                        \"STATE\", \"COUNTY\", \"TRAPSITE\", \"TRAPSET\", \"SETTIMEOFDAY\",\n","                        \"TRAPCOLLECT\", \"COLLECTTIMEOFDAY\", \"GENUS\",\n","                        \"SPECIES\", \"LIFESTAGE\", \"EGGSCOLLECTED\", \"LARVAECOLLECTED\",\n","                        \"PUPAECOLLECTED\", \"REPORTDATE\"]\n","\n","categorical_columns = [\"TRAPTYPE\", \"ATTRACTANTSUSED\"]\n","\n","temporal_columns = [\"sunriseTime\", \"sunsetTime\", \"moonPhase\", \"precipIntensity\",\n","                    \"precipIntensityMax\", \"precipProbability\", \"temperatureHigh\",\n","                    \"temperatureHighTime\", \"temperatureLow\", \"temperatureLowTime\",\n","                    \"apparentTemperatureHigh\", \"apparentTemperatureHighTime\",\n","                    \"apparentTemperatureLow\", \"apparentTemperatureLowTime\",\n","                    \"dewPoint\", \"humidity\", \"pressure\", \"windSpeed\", \"windGust\",\n","                    \"windGustTime\", \"windBearing\", \"cloudCover\", \"uvIndex\", \n","                    \"uvIndexTime\", \"visibility\", \"temperatureMin\", \"temperatureMinTime\",\n","                    \"temperatureMax\", \"temperatureMaxTime\", \"apparentTemperatureMin\",\n","                    \"apparentTemperatureMinTime\", \"apparentTemperatureMax\", \"apparentTemperatureMaxTime\",\n","                    \"icon\", \"time\", \"precipIntensityMaxTime\", \"precipType\", \"summary\"] \n","\n","numerical_columns = [\"temperatureHigh\", \"uvIndex\", \"precipIntensityMaxTime\", \n","                     \"sunriseTime\", \"sunsetTime\", \"temperatureLow\", \"temperatureLowTime\",\n","                     \"temperatureHighTime\", \"humidity\"]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eKTqERJZEAvP","colab_type":"code","colab":{}},"source":["SEQ_LENGTH = 14\n","NUM_ENTRIES = train_frame.shape[0]\n","\n","column_means = {}\n","column_maxs = {}\n","column_mins = {}\n","\n","for col in numerical_columns:\n","    column_means[col] = 0\n","    column_maxs[col] = 0\n","    column_mins[col] = np.inf\n","\n","    for i in range(1, SEQ_LENGTH + 1):\n","        train_frame[col + str(i)].replace(to_replace=r'No*', value=np.nan, regex=True, inplace=True)\n","        test_frame[col + str(i)].replace(to_replace=r'No*', value=np.nan, regex=True, inplace=True)\n","\n","        if(train_frame[col + str(i)].dtype == np.dtype(object)):\n","            train_frame[col + str(i)] = train_frame[col + str(i)].astype(\"float64\", copy=\"False\")\n","\n","        if(test_frame[col + str(i)].dtype == np.dtype(object)):\n","            test_frame[col + str(i)] = test_frame[col + str(i)].astype(\"float64\", copy=\"False\")\n","\n","        cur_col = train_frame[col + str(i)]\n","\n","        col_mean = cur_col.mean()\n","\n","        train_frame[col + str(i)].replace(to_replace=np.nan, value=col_mean, inplace=True)\n","        test_frame[col + str(i)].replace(to_replace=np.nan, value=col_mean, inplace=True)\n","\n","        column_means[col] += col_mean * NUM_ENTRIES\n","        if cur_col.max() > column_maxs[col]:\n","            column_maxs[col] = cur_col.max()\n","\n","        if cur_col.min() < column_mins[col]:\n","            column_mins[col] = cur_col.min()\n","\n","    column_means[col] /= SEQ_LENGTH * NUM_ENTRIES\n","\n","    for i in range(1, SEQ_LENGTH + 1):\n","        test_frame[col + str(i)] = (test_frame[col + str(i)] - column_means[col]) / (column_maxs[col] - column_mins[col])\n","        train_frame[col + str(i)] = (train_frame[col + str(i)] - column_means[col]) / (column_maxs[col] - column_mins[col])\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aoXrda1jLe0z","colab_type":"code","colab":{}},"source":["# Standardize Numerical Columns (not Temporal)\n","\n","num_means = {}\n","num_maxs = {}\n","num_mins = {}\n","\n","for col in non_num_columns:\n","    cur_col = train_frame[col]\n","    num_means[col] = cur_col.mean()\n","    num_maxs[col] = cur_col.max()\n","    num_mins[col] = cur_col.min()\n","\n","    train_frame[col] = (cur_col - num_means[col]) / (num_maxs[col] - num_mins[col])\n","    test_frame[col] = (test_frame[col] - num_means[col]) / (num_maxs[col] - num_mins[col])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fT25ApU8vdWW","colab_type":"code","outputId":"2fcb96db-3da4-4a5b-9865-f975d46f093b","executionInfo":{"status":"ok","timestamp":1586981666299,"user_tz":300,"elapsed":531,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Convert categories\n","ats_train = []\n","ats_test = []\n","\n","for category in categorical_columns:\n","    train_frame[category] = train_frame[category].astype('category')\n","    test_frame[category] = test_frame[category].astype('category')\n","    ats_train.append(train_frame[category].cat.codes.values)\n","    ats_test.append(test_frame[category].cat.codes.values)\n","\n","train_cat = np.stack(ats_train, 1)\n","test_cat = np.stack(ats_test, 1)\n","\n","categorical_column_sizes = [len(train_frame[column].cat.categories) for column in categorical_columns]\n","embedding_sizes = [(col_size, min(50, (col_size + 1) // 2)) for col_size in categorical_column_sizes]\n","\n","print(embedding_sizes)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["[(3, 2), (4, 2)]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KOy39SYAGsgy","colab_type":"code","colab":{}},"source":["# Organize info by batches/sequences\n","# Shape [784, 14, 35] -> Train\n","\n","#train_features = np.zeros(shape=(train_frame.shape[0], SEQ_LENGTH, len(numerical_columns)+len(non_num_columns)))\n","#test_features = np.zeros(shape=(test_frame.shape[0], SEQ_LENGTH, len(numerical_columns)+len(non_num_columns)))\n","\n","train_features = np.zeros(shape=(train_frame.shape[0], SEQ_LENGTH, len(numerical_columns)))\n","test_features = np.zeros(shape=(test_frame.shape[0], SEQ_LENGTH, len(numerical_columns)))\n","\n","for i in range(1, SEQ_LENGTH + 1):\n","    for row, col in enumerate(numerical_columns):\n","        train_features[:, i-1, row] = train_frame[col + str(i)]\n","        test_features[:, i-1, row] = test_frame[col + str(i)]\n","\n","    '''    \n","    for row, col in enumerate(non_num_columns):\n","        train_features[:, i-1, row+len(numerical_columns)] = train_frame[col]\n","        test_features[:, i-1, row+len(numerical_columns)] = test_frame[col]\n","    '''"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J0aIP-SKMQ9R","colab_type":"code","colab":{}},"source":["# Organize info that is not temporal\n","train_y = train_frame[\"TOTAL\"].to_numpy()\n","test_y = test_frame[\"TOTAL\"].to_numpy()\n","\n","train_X = np.zeros(shape=(train_frame.shape[0], len(non_num_columns)))\n","test_X = np.zeros(shape=(test_frame.shape[0], len(non_num_columns)))\n","\n","for row, col in enumerate(non_num_columns):\n","    train_X[:, row] = train_frame[col]\n","    test_X[:, row] = test_frame[col]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4SLM3B2ZtcU4","colab_type":"text"},"source":["### Danger! Cuau Validation: DNR"]},{"cell_type":"code","metadata":{"id":"ICC4CaoOy4eh","colab_type":"code","outputId":"f7c5aa6e-c09f-4167-9f66-97ac4425fbb1","executionInfo":{"status":"ok","timestamp":1586981684274,"user_tz":300,"elapsed":700,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Create DataLoaders\n","# number of subprocesses to use for data loading\n","num_workers = 0\n","# how many samples per batch to load\n","batch_size = 16\n","# percentage of training set to use as validation\n","valid_size = 0.2\n","\n","\n","# obtain training indices that will be used for validation\n","num_train = len(train_X)\n","indices = list(range(num_train))\n","np.random.shuffle(indices)\n","split = int(np.floor(valid_size * num_train))\n","train_idx, valid_idx = indices[split:], indices[:split]\n","\n","print(len(train_idx), len(valid_idx))\n","\n","# define samplers for obtaining training and validation batches\n","train_sampler = SubsetRandomSampler(train_idx)\n","valid_sampler = SubsetRandomSampler(valid_idx)\n","\n","train_data = TensorDataset(torch.from_numpy(train_features), \n","                           torch.from_numpy(train_X), \n","                           torch.from_numpy(train_cat),\n","                           torch.from_numpy(train_y))\n","\n","test_data = TensorDataset(torch.from_numpy(test_features), \n","                          torch.from_numpy(test_X), \n","                          torch.from_numpy(test_cat),\n","                          torch.from_numpy(test_y))\n","\n","\n","# prepare data loaders (combine dataset and sampler)\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size,\n","    sampler = train_sampler, num_workers = num_workers, drop_last=True)\n","valid_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, \n","    sampler = valid_sampler, num_workers = num_workers, drop_last=True)\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size, \n","    num_workers = num_workers, shuffle = False, drop_last=True)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["596 149\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"i1VwYmtPtgGO","colab_type":"code","outputId":"19ab58ae-5bed-448a-867e-253019c42182","executionInfo":{"status":"ok","timestamp":1586981681621,"user_tz":300,"elapsed":596,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["# Create DataLoaders\n","# number of subprocesses to use for data loading\n","num_workers = 0\n","# how many samples per batch to load\n","batch_size = 4\n","# percentage of training set to use as validation\n","valid_size = 0.15\n","\n","\n","# obtain training indices that will be used for validation\n","num_train = len(train_X)\n","indices = list(range(num_train))\n","np.random.shuffle(indices)\n","split = int(np.floor(valid_size * num_train))\n","train_idx, valid_idx = indices[split:], indices[:split]\n","\n","print(len(train_idx), len(valid_idx))\n","\n","# define samplers for obtaining training and validation batches\n","train_sampler = SubsetRandomSampler(train_idx)\n","valid_sampler = SubsetRandomSampler(valid_idx)\n","\n","train_data = TensorDataset(torch.from_numpy(train_features[train_idx]), \n","                           torch.from_numpy(train_X[train_idx]), \n","                           torch.from_numpy(train_cat[train_idx]),\n","                           torch.from_numpy(train_y[train_idx]))\n","\n","valid_data = TensorDataset(torch.from_numpy(train_features[valid_idx]), \n","                           torch.from_numpy(train_X[valid_idx]), \n","                           torch.from_numpy(train_cat[valid_idx]),\n","                           torch.from_numpy(train_y[valid_idx]))\n","\n","test_data = TensorDataset(torch.from_numpy(test_features), \n","                          torch.from_numpy(test_X), \n","                          torch.from_numpy(test_cat),\n","                          torch.from_numpy(test_y))\n","\n","print(len(train_data), len(valid_data))\n","\n","# prepare data loaders (combine dataset and sampler)\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n","    num_workers=num_workers, shuffle=True, drop_last=True)\n","valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, \n","    num_workers=num_workers, shuffle=True, drop_last=True)\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n","    num_workers=num_workers, shuffle=False, drop_last=True)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["634 111\n","634 111\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JBf5h7sS3nVc","colab_type":"code","colab":{}},"source":["dataiter = iter(train_loader)\n","sample_features, sample_X, sample_cat, sample_y = dataiter.next()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M9CE7bQE36vD","colab_type":"code","outputId":"9315b42a-e342-4ef3-a6ca-49bb2556ecb6","executionInfo":{"status":"ok","timestamp":1586904168421,"user_tz":300,"elapsed":381,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":90}},"source":["print(sample_features.shape)\n","print(sample_X.shape)\n","print(sample_cat.shape)\n","print(sample_y.shape)"],"execution_count":86,"outputs":[{"output_type":"stream","text":["torch.Size([16, 14, 5])\n","torch.Size([16, 3])\n","torch.Size([16, 2])\n","torch.Size([16])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uwzut-CseGO3","colab_type":"code","outputId":"97491240-ba85-4256-e0b6-eef6f6f8418f","executionInfo":{"status":"ok","timestamp":1586981688228,"user_tz":300,"elapsed":800,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# First checking if GPU is available\n","train_on_gpu = torch.cuda.is_available()\n","\n","if(train_on_gpu):\n","    print('Training on GPU.')\n","else:\n","    print('No GPU available, training on CPU.')\n","\n","#train_on_gpu = False"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Training on GPU.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fKaB5Hx1377j","colab_type":"code","colab":{}},"source":["class MosquitoLSTM(nn.Module):\n","    def __init__(self, num_numerical_columns, hidden_dim, n_layers):\n","        super(MosquitoLSTM, self).__init__()\n","\n","        self.n_layers = n_layers\n","        self.hidden_dim = hidden_dim\n","\n","        '''\n","        embed_size = 0\n","        for embed in embedding_sizes:\n","            embed_size += embed[1]\n","\n","        self.all_embeddings = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in embedding_sizes])\n","        '''\n","\n","        # For LSTM\n","        self.lstm = nn.LSTM(num_numerical_columns, hidden_dim, n_layers,\n","                            dropout=0.3, batch_first=True)\n","        \n","        # dropout layer\n","        self.dropout = nn.Dropout(0.3)\n","\n","        self.dense = nn.Sequential(\n","            \n","            nn.Linear(hidden_dim, 1)\n","        )\n","        self.ReLU = nn.ReLU()\n","\n","    def forward(self, X_temp, hidden):\n","        lstm_out, hidden = self.lstm(X_temp, hidden)\n","    \n","        # stack up lstm outputs\n","        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n","        print(lstm_out.shape)\n","        # dropout and fully-connected layer\n","        out = self.dense(lstm_out)\n","        # sigmoid function\n","        out = self.ReLU(out)\n","        \n","        print(out.shape)\n","        # reshape to be batch_size first\n","        out = out.view(batch_size, -1)\n","        print(out.shape)\n","        out = out[:, -1]\n","        print(out.shape)\n","\n","\n","\n","        return out, hidden\n","\n","    def init_hidden(self, batch_size):\n","        ''' Initializes hidden state '''\n","        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n","        # initialized to zero, for hidden state and cell state of LSTM\n","        weight = next(self.parameters()).data\n","        \n","        if (train_on_gpu):\n","            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n","                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n","        else:\n","            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n","                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n","        \n","        return hidden\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MO7LlGaj7oIe","colab_type":"code","colab":{}},"source":["class MosquitoRedux(nn.Module):\n","    def __init__(self, \n","                 lstm_columns, lstm_hidden_dim, lstm_n_layers,\n","                 num_columns, num_hidden_dim,\n","                 embedding_sizes, cat_hidden_dim):\n","        super(MosquitoRedux, self).__init__()\n","\n","        self.lstm_n_layers = lstm_n_layers\n","        self.lstm_hidden_dim = lstm_hidden_dim\n","\n","        self.num_hidden_dim = num_hidden_dim\n","        self.cat_hidden_dim = cat_hidden_dim\n","\n","        embed_size = 0\n","        for embed in embedding_sizes:\n","            embed_size += embed[1]\n","\n","        self.all_embeddings = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in embedding_sizes])\n","\n","        # For LSTM\n","        self.lstm = nn.LSTM(lstm_columns, lstm_hidden_dim, lstm_n_layers,\n","                            dropout=0.3, batch_first=True)\n","        \n","        # LSTM dense layer\n","        self.lstm_dense = nn.Sequential(\n","            nn.BatchNorm1d(lstm_hidden_dim),\n","            nn.Dropout(0.3),\n","            nn.Linear(lstm_hidden_dim, 1),\n","            nn.ReLU()\n","        )\n","        \n","        self.num_dense = nn.Sequential(\n","            nn.Dropout(0.3),\n","            nn.Linear(num_columns, num_hidden_dim),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm1d(num_hidden_dim),\n","            nn.Dropout(0.3),\n","            nn.Linear(num_hidden_dim, num_hidden_dim),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm1d(num_hidden_dim),\n","            nn.Dropout(0.3),\n","            nn.Linear(num_hidden_dim, num_hidden_dim),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm1d(num_hidden_dim),\n","            nn.Linear(num_hidden_dim, 1),\n","            nn.ReLU()\n","        )\n","\n","        self.cat_dense = nn.Sequential(\n","            nn.Dropout(0.3),\n","            nn.Linear(embed_size, cat_hidden_dim),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm1d(cat_hidden_dim),\n","            nn.Dropout(0.3),\n","            nn.Linear(cat_hidden_dim, cat_hidden_dim),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm1d(cat_hidden_dim),\n","            nn.Dropout(0.3),\n","            nn.Linear(cat_hidden_dim, cat_hidden_dim),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm1d(cat_hidden_dim),\n","            nn.Linear(cat_hidden_dim, 1),\n","            nn.ReLU()\n","        )\n","\n","    def forward(self, X_temp, hidden, X_num, X_cat):\n","        # Forward lstm\n","        lstm_out, hidden = self.lstm(X_temp, hidden)\n","        # stack up lstm outputs\n","        lstm_out = lstm_out.contiguous().view(-1, self.lstm_hidden_dim)\n","        # dropout and fully-connected layer\n","        lstm_out = self.lstm_dense(lstm_out)\n","      \n","        # reshape to be batch_size first\n","        lstm_out = lstm_out.view(batch_size, -1)\n","        lstm_out = lstm_out[:, -1]\n","        lstm_out = lstm_out.view(batch_size, -1)\n","\n","        # Forward num\n","        num_out = self.num_dense(X_num)\n","\n","        # Forward cat\n","        embeddings = []\n","        for i, e in enumerate(self.all_embeddings):\n","            embed = e(X_cat[:, i])\n","            embeddings.append(embed)\n","\n","        cat_out = torch.cat(embeddings, 1)\n","        cat_out = self.cat_dense(cat_out)\n","\n","        return lstm_out, hidden, num_out, cat_out\n","\n","    def init_hidden(self, batch_size):\n","        ''' Initializes hidden state '''\n","        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n","        # initialized to zero, for hidden state and cell state of LSTM\n","        weight = next(self.parameters()).data\n","        \n","        if (train_on_gpu):\n","            hidden = (weight.new(self.lstm_n_layers, batch_size, self.lstm_hidden_dim).zero_().cuda(),\n","                  weight.new(self.lstm_n_layers, batch_size, self.lstm_hidden_dim).zero_().cuda())\n","        else:\n","            hidden = (weight.new(self.lstm_n_layers, batch_size, self.lstm_hidden_dim).zero_(),\n","                      weight.new(self.lstm_n_layers, batch_size, self.lstm_hidden_dim).zero_())\n","        \n","        return hidden"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MjjvR7ARfbN5","colab_type":"code","outputId":"a5b56b1b-30e6-4da6-c2f7-e72d484b8f19","executionInfo":{"status":"ok","timestamp":1586933793908,"user_tz":300,"elapsed":813,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Instantiate the model w/ hyperparams\n","a_h = 3\n","\n","lstm_hidden_dim = np.int(len(train_idx) / (a_h * len(numerical_columns) + 1))\n","print(lstm_hidden_dim)\n","lstm_n_layers = 2\n","\n","num_hidden_dim = 15\n","cat_hidden_dim = 15\n","\n","#net = MosquitoLSTM(len(numerical_columns), hidden_dim, n_layers).double()\n","net = MosquitoRedux(\n","    len(numerical_columns), lstm_hidden_dim, lstm_n_layers,\n","    len(non_num_columns), num_hidden_dim,\n","    embedding_sizes, cat_hidden_dim\n",").double()\n","\n","weight_lstm = 1.0\n","weight_num = 2.0\n","weight_cat = 1.0"],"execution_count":487,"outputs":[{"output_type":"stream","text":["21\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"caOIYuZ5foOw","colab_type":"code","colab":{}},"source":["lr = 0.001\n","\n","criterion = nn.MSELoss()\n","#optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n","optimizer = torch.optim.RMSprop(net.parameters(), lr=lr)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jzkXPhqufsic","colab_type":"code","outputId":"efaf72ba-3e55-48c5-9b5d-9948f9ed06ef","executionInfo":{"status":"ok","timestamp":1586923700434,"user_tz":300,"elapsed":168759,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# training params\n","\n","epochs = 400 # 3-4 is approx where I noticed the validation loss stop decreasing\n","\n","counter = 0\n","print_every = 10\n","clip = 10 # gradient clipping\n","\n","# move model to GPU, if available\n","if(train_on_gpu):\n","    net.cuda()\n","\n","net.train()\n","# train for some number of epochs\n","valid_loss_min = np.Inf\n","\n","for epoch in range(epochs):\n","    # initialize hidden state\n","    h = net.init_hidden(batch_size)\n","\n","    train_loss = 0.0\n","    valid_loss = 0.0\n","\n","    net.train()\n","\n","    # batch loop\n","    for X_temp, X_num, X_cat, labels in train_loader:\n","        counter += 1\n","\n","        if(train_on_gpu):\n","            X_temp, X_num, X_cat, labels = X_temp.cuda(), X_num.cuda(), X_cat.type(torch.LongTensor).cuda(), labels.cuda()\n","\n","        # Creating new variables for the hidden state, otherwise\n","        # we'd backprop through the entire training history\n","        h = tuple([each.data for each in h])\n","        # zero accumulated gradients\n","        net.zero_grad()\n","\n","        # get the output from the model\n","        lstm_output, h, num_output, cat_output = net(X_temp, h, X_num, X_cat)\n","\n","        output = (weight_lstm*lstm_output + weight_num*num_output + weight_cat*cat_output) \n","\n","        # calculate the loss and perform backprop\n","        loss = criterion(output.squeeze(), labels.double())\n","        loss.backward()\n","        train_loss += loss.item()\n","        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","        nn.utils.clip_grad_norm_(net.parameters(), clip)\n","        optimizer.step()\n","\n","    net.eval()\n","\n","    val_h = net.init_hidden(batch_size)\n","\n","    for X_temp, X_num, X_cat, labels in valid_loader:\n","        \n","        \n","        if(train_on_gpu):\n","            X_temp, X_num, X_cat, labels = X_temp.cuda(), X_num.cuda(), X_cat.type(torch.LongTensor).cuda(), labels.cuda()\n","\n","        # Creating new variables for the hidden state, otherwise\n","        # we'd backprop through the entire training history\n","        val_h = tuple([each.data for each in val_h])\n","\n","        # get the output from the model\n","        lstm_output, h, num_output, cat_output = net(X_temp, h, X_num, X_cat)\n","\n","        # calculate the loss and perform backprop\n","        output = (weight_lstm*lstm_output + weight_num*num_output + weight_cat*cat_output) \n","        loss = criterion(output.squeeze(), labels.double())\n","        valid_loss += loss.item()\n","\n","    # save model if validation loss has decreased\n","    if valid_loss <= valid_loss_min:\n","        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","        valid_loss_min,\n","        valid_loss))\n","        torch.save(net.state_dict(), 'lstm_test.pt')\n","        valid_loss_min = valid_loss\n","        \n","    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n","            epoch, train_loss, valid_loss))"],"execution_count":379,"outputs":[{"output_type":"stream","text":["Validation loss decreased (inf --> 1063.810599).  Saving model ...\n","Epoch: 0 \tTraining Loss: 4902.986458 \tValidation Loss: 1063.810599\n","Validation loss decreased (1063.810599 --> 1010.740524).  Saving model ...\n","Epoch: 1 \tTraining Loss: 4528.126603 \tValidation Loss: 1010.740524\n","Epoch: 2 \tTraining Loss: 4321.250742 \tValidation Loss: 1019.153390\n","Validation loss decreased (1010.740524 --> 790.559787).  Saving model ...\n","Epoch: 3 \tTraining Loss: 4298.738146 \tValidation Loss: 790.559787\n","Validation loss decreased (790.559787 --> 788.113032).  Saving model ...\n","Epoch: 4 \tTraining Loss: 4151.630487 \tValidation Loss: 788.113032\n","Epoch: 5 \tTraining Loss: 4001.181914 \tValidation Loss: 942.267813\n","Epoch: 6 \tTraining Loss: 4183.231023 \tValidation Loss: 953.540055\n","Epoch: 7 \tTraining Loss: 4021.229664 \tValidation Loss: 910.900171\n","Epoch: 8 \tTraining Loss: 4069.782589 \tValidation Loss: 942.354421\n","Epoch: 9 \tTraining Loss: 4039.964342 \tValidation Loss: 968.322827\n","Epoch: 10 \tTraining Loss: 3960.657928 \tValidation Loss: 859.746226\n","Epoch: 11 \tTraining Loss: 3992.804889 \tValidation Loss: 962.385893\n","Epoch: 12 \tTraining Loss: 4073.931558 \tValidation Loss: 916.914474\n","Epoch: 13 \tTraining Loss: 4020.756905 \tValidation Loss: 945.938688\n","Epoch: 14 \tTraining Loss: 4063.930741 \tValidation Loss: 976.674579\n","Epoch: 15 \tTraining Loss: 4040.273404 \tValidation Loss: 892.743500\n","Epoch: 16 \tTraining Loss: 3139.971909 \tValidation Loss: 954.365069\n","Epoch: 17 \tTraining Loss: 4060.167099 \tValidation Loss: 953.029332\n","Epoch: 18 \tTraining Loss: 4059.023678 \tValidation Loss: 915.728428\n","Epoch: 19 \tTraining Loss: 3963.972188 \tValidation Loss: 956.412105\n","Epoch: 20 \tTraining Loss: 3917.386838 \tValidation Loss: 892.234124\n","Epoch: 21 \tTraining Loss: 3974.464383 \tValidation Loss: 939.849364\n","Epoch: 22 \tTraining Loss: 3922.569558 \tValidation Loss: 951.745127\n","Epoch: 23 \tTraining Loss: 3838.124238 \tValidation Loss: 907.878958\n","Epoch: 24 \tTraining Loss: 3973.836096 \tValidation Loss: 928.306695\n","Epoch: 25 \tTraining Loss: 3925.872197 \tValidation Loss: 1010.771514\n","Epoch: 26 \tTraining Loss: 4072.696164 \tValidation Loss: 966.208509\n","Epoch: 27 \tTraining Loss: 3929.462006 \tValidation Loss: 936.945429\n","Epoch: 28 \tTraining Loss: 3881.088143 \tValidation Loss: 935.232045\n","Epoch: 29 \tTraining Loss: 3815.092979 \tValidation Loss: 933.868704\n","Validation loss decreased (788.113032 --> 778.794350).  Saving model ...\n","Epoch: 30 \tTraining Loss: 3926.549424 \tValidation Loss: 778.794350\n","Epoch: 31 \tTraining Loss: 3892.851435 \tValidation Loss: 911.501090\n","Epoch: 32 \tTraining Loss: 3829.624486 \tValidation Loss: 936.745191\n","Epoch: 33 \tTraining Loss: 3779.653069 \tValidation Loss: 931.099982\n","Epoch: 34 \tTraining Loss: 3516.694271 \tValidation Loss: 930.604605\n","Epoch: 35 \tTraining Loss: 3835.122952 \tValidation Loss: 962.641713\n","Epoch: 36 \tTraining Loss: 3825.273889 \tValidation Loss: 915.044680\n","Epoch: 37 \tTraining Loss: 3844.533442 \tValidation Loss: 922.052588\n","Epoch: 38 \tTraining Loss: 3808.159224 \tValidation Loss: 897.883912\n","Epoch: 39 \tTraining Loss: 3793.423553 \tValidation Loss: 924.209021\n","Epoch: 40 \tTraining Loss: 3769.365939 \tValidation Loss: 816.985080\n","Epoch: 41 \tTraining Loss: 3787.416049 \tValidation Loss: 887.653672\n","Epoch: 42 \tTraining Loss: 3850.656385 \tValidation Loss: 920.602154\n","Epoch: 43 \tTraining Loss: 3738.353130 \tValidation Loss: 896.850612\n","Validation loss decreased (778.794350 --> 591.720069).  Saving model ...\n","Epoch: 44 \tTraining Loss: 3823.879824 \tValidation Loss: 591.720069\n","Epoch: 45 \tTraining Loss: 3697.565320 \tValidation Loss: 941.674602\n","Epoch: 46 \tTraining Loss: 3837.695961 \tValidation Loss: 924.278358\n","Epoch: 47 \tTraining Loss: 3778.464097 \tValidation Loss: 896.809567\n","Epoch: 48 \tTraining Loss: 3555.551868 \tValidation Loss: 899.409208\n","Epoch: 49 \tTraining Loss: 3806.429891 \tValidation Loss: 710.097916\n","Epoch: 50 \tTraining Loss: 3845.138778 \tValidation Loss: 886.939085\n","Epoch: 51 \tTraining Loss: 3713.275382 \tValidation Loss: 906.733160\n","Epoch: 52 \tTraining Loss: 3830.486929 \tValidation Loss: 801.166579\n","Epoch: 53 \tTraining Loss: 3789.678678 \tValidation Loss: 910.083177\n","Epoch: 54 \tTraining Loss: 3805.242171 \tValidation Loss: 904.318094\n","Epoch: 55 \tTraining Loss: 3712.499105 \tValidation Loss: 939.002967\n","Epoch: 56 \tTraining Loss: 3706.938634 \tValidation Loss: 822.772953\n","Epoch: 57 \tTraining Loss: 3799.861291 \tValidation Loss: 877.054421\n","Epoch: 58 \tTraining Loss: 3727.523757 \tValidation Loss: 855.292781\n","Epoch: 59 \tTraining Loss: 3637.569505 \tValidation Loss: 739.856807\n","Epoch: 60 \tTraining Loss: 3746.457208 \tValidation Loss: 894.757579\n","Epoch: 61 \tTraining Loss: 3761.585555 \tValidation Loss: 830.044012\n","Epoch: 62 \tTraining Loss: 3706.307010 \tValidation Loss: 907.800647\n","Epoch: 63 \tTraining Loss: 3728.783246 \tValidation Loss: 921.097553\n","Epoch: 64 \tTraining Loss: 3666.874648 \tValidation Loss: 931.827210\n","Epoch: 65 \tTraining Loss: 3773.551674 \tValidation Loss: 909.992542\n","Epoch: 66 \tTraining Loss: 3702.316191 \tValidation Loss: 958.139127\n","Epoch: 67 \tTraining Loss: 3722.387655 \tValidation Loss: 959.816195\n","Epoch: 68 \tTraining Loss: 3672.287460 \tValidation Loss: 923.672115\n","Epoch: 69 \tTraining Loss: 3701.387339 \tValidation Loss: 912.007884\n","Epoch: 70 \tTraining Loss: 3638.101025 \tValidation Loss: 910.381558\n","Epoch: 71 \tTraining Loss: 3709.616767 \tValidation Loss: 919.665182\n","Epoch: 72 \tTraining Loss: 3649.086501 \tValidation Loss: 919.311700\n","Epoch: 73 \tTraining Loss: 3653.564676 \tValidation Loss: 925.249300\n","Epoch: 74 \tTraining Loss: 3677.197416 \tValidation Loss: 804.082561\n","Epoch: 75 \tTraining Loss: 3618.983928 \tValidation Loss: 924.193861\n","Epoch: 76 \tTraining Loss: 3699.573488 \tValidation Loss: 924.935913\n","Epoch: 77 \tTraining Loss: 3642.728597 \tValidation Loss: 921.201771\n","Epoch: 78 \tTraining Loss: 3652.893454 \tValidation Loss: 902.159388\n","Epoch: 79 \tTraining Loss: 3667.545241 \tValidation Loss: 903.247526\n","Epoch: 80 \tTraining Loss: 3691.709887 \tValidation Loss: 921.190952\n","Epoch: 81 \tTraining Loss: 3748.633995 \tValidation Loss: 910.733033\n","Epoch: 82 \tTraining Loss: 3799.552397 \tValidation Loss: 922.346110\n","Epoch: 83 \tTraining Loss: 3670.691268 \tValidation Loss: 911.055476\n","Epoch: 84 \tTraining Loss: 3759.329491 \tValidation Loss: 831.196758\n","Epoch: 85 \tTraining Loss: 3715.411835 \tValidation Loss: 913.852533\n","Epoch: 86 \tTraining Loss: 3282.234081 \tValidation Loss: 927.197403\n","Epoch: 87 \tTraining Loss: 3660.966221 \tValidation Loss: 937.666703\n","Epoch: 88 \tTraining Loss: 3705.784096 \tValidation Loss: 843.550976\n","Epoch: 89 \tTraining Loss: 3625.987921 \tValidation Loss: 948.693986\n","Epoch: 90 \tTraining Loss: 3690.189483 \tValidation Loss: 912.741569\n","Epoch: 91 \tTraining Loss: 3633.442380 \tValidation Loss: 793.465999\n","Epoch: 92 \tTraining Loss: 3630.126814 \tValidation Loss: 929.162555\n","Epoch: 93 \tTraining Loss: 3645.351647 \tValidation Loss: 921.577764\n","Epoch: 94 \tTraining Loss: 3648.943836 \tValidation Loss: 894.533381\n","Epoch: 95 \tTraining Loss: 3693.441114 \tValidation Loss: 921.648583\n","Epoch: 96 \tTraining Loss: 3738.272412 \tValidation Loss: 920.143394\n","Epoch: 97 \tTraining Loss: 3658.535249 \tValidation Loss: 905.608811\n","Epoch: 98 \tTraining Loss: 3666.129216 \tValidation Loss: 905.274010\n","Epoch: 99 \tTraining Loss: 3441.777583 \tValidation Loss: 819.735320\n","Epoch: 100 \tTraining Loss: 3679.638255 \tValidation Loss: 851.920358\n","Epoch: 101 \tTraining Loss: 3705.595721 \tValidation Loss: 926.534809\n","Epoch: 102 \tTraining Loss: 3706.040590 \tValidation Loss: 916.556420\n","Epoch: 103 \tTraining Loss: 3640.544424 \tValidation Loss: 925.670826\n","Epoch: 104 \tTraining Loss: 3694.157592 \tValidation Loss: 902.804939\n","Epoch: 105 \tTraining Loss: 3671.265869 \tValidation Loss: 911.449376\n","Epoch: 106 \tTraining Loss: 3600.695581 \tValidation Loss: 917.226483\n","Epoch: 107 \tTraining Loss: 3597.463141 \tValidation Loss: 777.557816\n","Epoch: 108 \tTraining Loss: 3564.609682 \tValidation Loss: 913.159427\n","Epoch: 109 \tTraining Loss: 3536.765363 \tValidation Loss: 908.219703\n","Epoch: 110 \tTraining Loss: 3574.456269 \tValidation Loss: 927.243309\n","Epoch: 111 \tTraining Loss: 3695.334839 \tValidation Loss: 917.889419\n","Epoch: 112 \tTraining Loss: 3617.250422 \tValidation Loss: 906.111012\n","Epoch: 113 \tTraining Loss: 3602.880812 \tValidation Loss: 915.932495\n","Epoch: 114 \tTraining Loss: 3576.340550 \tValidation Loss: 926.671038\n","Epoch: 115 \tTraining Loss: 3595.682456 \tValidation Loss: 775.426441\n","Epoch: 116 \tTraining Loss: 3640.864624 \tValidation Loss: 909.583169\n","Epoch: 117 \tTraining Loss: 3621.551306 \tValidation Loss: 927.255655\n","Epoch: 118 \tTraining Loss: 3664.787632 \tValidation Loss: 914.830602\n","Epoch: 119 \tTraining Loss: 3637.295170 \tValidation Loss: 798.658374\n","Epoch: 120 \tTraining Loss: 3557.988209 \tValidation Loss: 925.550273\n","Epoch: 121 \tTraining Loss: 3600.511852 \tValidation Loss: 928.277747\n","Epoch: 122 \tTraining Loss: 3625.307896 \tValidation Loss: 929.659994\n","Epoch: 123 \tTraining Loss: 3534.787615 \tValidation Loss: 921.181980\n","Epoch: 124 \tTraining Loss: 3636.903017 \tValidation Loss: 892.886237\n","Epoch: 125 \tTraining Loss: 3649.309664 \tValidation Loss: 930.386726\n","Epoch: 126 \tTraining Loss: 3643.022649 \tValidation Loss: 931.689690\n","Epoch: 127 \tTraining Loss: 3672.332533 \tValidation Loss: 950.917113\n","Epoch: 128 \tTraining Loss: 3610.542003 \tValidation Loss: 915.306162\n","Epoch: 129 \tTraining Loss: 3744.210562 \tValidation Loss: 841.656105\n","Epoch: 130 \tTraining Loss: 3613.791214 \tValidation Loss: 921.087533\n","Epoch: 131 \tTraining Loss: 3545.076699 \tValidation Loss: 925.706388\n","Epoch: 132 \tTraining Loss: 3618.269981 \tValidation Loss: 920.080927\n","Epoch: 133 \tTraining Loss: 3633.871237 \tValidation Loss: 904.006784\n","Epoch: 134 \tTraining Loss: 3601.798511 \tValidation Loss: 922.647791\n","Epoch: 135 \tTraining Loss: 3617.329790 \tValidation Loss: 882.643336\n","Epoch: 136 \tTraining Loss: 3695.112376 \tValidation Loss: 699.886881\n","Epoch: 137 \tTraining Loss: 3656.851439 \tValidation Loss: 920.755421\n","Epoch: 138 \tTraining Loss: 3658.539342 \tValidation Loss: 930.604008\n","Epoch: 139 \tTraining Loss: 3529.643260 \tValidation Loss: 932.745322\n","Epoch: 140 \tTraining Loss: 3642.430436 \tValidation Loss: 866.465129\n","Epoch: 141 \tTraining Loss: 3613.374312 \tValidation Loss: 904.432699\n","Epoch: 142 \tTraining Loss: 3650.061819 \tValidation Loss: 784.380610\n","Epoch: 143 \tTraining Loss: 3633.042436 \tValidation Loss: 927.562125\n","Epoch: 144 \tTraining Loss: 3660.086430 \tValidation Loss: 922.936115\n","Epoch: 145 \tTraining Loss: 3605.468107 \tValidation Loss: 909.079174\n","Epoch: 146 \tTraining Loss: 3617.740311 \tValidation Loss: 891.483170\n","Epoch: 147 \tTraining Loss: 3615.915308 \tValidation Loss: 841.758215\n","Epoch: 148 \tTraining Loss: 3600.912477 \tValidation Loss: 935.860477\n","Epoch: 149 \tTraining Loss: 3599.621786 \tValidation Loss: 933.734369\n","Epoch: 150 \tTraining Loss: 3534.424035 \tValidation Loss: 939.470172\n","Epoch: 151 \tTraining Loss: 3561.862491 \tValidation Loss: 833.217541\n","Epoch: 152 \tTraining Loss: 3658.471355 \tValidation Loss: 849.866639\n","Epoch: 153 \tTraining Loss: 3557.228939 \tValidation Loss: 944.469956\n","Epoch: 154 \tTraining Loss: 3637.361514 \tValidation Loss: 924.508053\n","Epoch: 155 \tTraining Loss: 3653.466488 \tValidation Loss: 923.251413\n","Epoch: 156 \tTraining Loss: 3665.492621 \tValidation Loss: 918.796751\n","Epoch: 157 \tTraining Loss: 3564.617360 \tValidation Loss: 933.976653\n","Epoch: 158 \tTraining Loss: 3625.671366 \tValidation Loss: 903.856580\n","Epoch: 159 \tTraining Loss: 3680.497660 \tValidation Loss: 924.245049\n","Epoch: 160 \tTraining Loss: 3566.035442 \tValidation Loss: 930.347567\n","Epoch: 161 \tTraining Loss: 3559.542651 \tValidation Loss: 925.730617\n","Epoch: 162 \tTraining Loss: 3566.301508 \tValidation Loss: 897.633383\n","Epoch: 163 \tTraining Loss: 3579.783542 \tValidation Loss: 925.028039\n","Epoch: 164 \tTraining Loss: 3580.390539 \tValidation Loss: 890.190170\n","Epoch: 165 \tTraining Loss: 3576.304464 \tValidation Loss: 936.868676\n","Epoch: 166 \tTraining Loss: 3508.348380 \tValidation Loss: 934.628205\n","Epoch: 167 \tTraining Loss: 3597.737559 \tValidation Loss: 923.391940\n","Epoch: 168 \tTraining Loss: 3540.709296 \tValidation Loss: 914.745624\n","Epoch: 169 \tTraining Loss: 3541.578382 \tValidation Loss: 932.293748\n","Epoch: 170 \tTraining Loss: 3465.667943 \tValidation Loss: 928.255773\n","Epoch: 171 \tTraining Loss: 3614.827921 \tValidation Loss: 912.127889\n","Epoch: 172 \tTraining Loss: 3683.318276 \tValidation Loss: 930.414844\n","Epoch: 173 \tTraining Loss: 3572.880304 \tValidation Loss: 939.581809\n","Epoch: 174 \tTraining Loss: 3592.019166 \tValidation Loss: 930.452961\n","Epoch: 175 \tTraining Loss: 3624.637177 \tValidation Loss: 859.312566\n","Epoch: 176 \tTraining Loss: 3642.327468 \tValidation Loss: 929.095772\n","Epoch: 177 \tTraining Loss: 3559.140243 \tValidation Loss: 740.271827\n","Epoch: 178 \tTraining Loss: 3622.336794 \tValidation Loss: 865.574912\n","Epoch: 179 \tTraining Loss: 3561.864278 \tValidation Loss: 921.433308\n","Epoch: 180 \tTraining Loss: 3564.221080 \tValidation Loss: 918.825146\n","Epoch: 181 \tTraining Loss: 3560.351687 \tValidation Loss: 730.900245\n","Epoch: 182 \tTraining Loss: 3593.602856 \tValidation Loss: 933.365504\n","Epoch: 183 \tTraining Loss: 3574.040950 \tValidation Loss: 944.175354\n","Epoch: 184 \tTraining Loss: 3592.038857 \tValidation Loss: 929.715188\n","Epoch: 185 \tTraining Loss: 3627.410544 \tValidation Loss: 940.450092\n","Epoch: 186 \tTraining Loss: 3502.958156 \tValidation Loss: 892.954046\n","Epoch: 187 \tTraining Loss: 3252.482286 \tValidation Loss: 884.178012\n","Epoch: 188 \tTraining Loss: 3556.089174 \tValidation Loss: 887.475454\n","Epoch: 189 \tTraining Loss: 3508.234551 \tValidation Loss: 936.333267\n","Epoch: 190 \tTraining Loss: 3532.562039 \tValidation Loss: 903.107510\n","Epoch: 191 \tTraining Loss: 3550.708963 \tValidation Loss: 902.419963\n","Epoch: 192 \tTraining Loss: 3584.889843 \tValidation Loss: 821.747359\n","Epoch: 193 \tTraining Loss: 3498.720190 \tValidation Loss: 930.884865\n","Epoch: 194 \tTraining Loss: 3649.535693 \tValidation Loss: 931.419733\n","Epoch: 195 \tTraining Loss: 3527.073101 \tValidation Loss: 885.525025\n","Epoch: 196 \tTraining Loss: 3572.973989 \tValidation Loss: 939.524022\n","Epoch: 197 \tTraining Loss: 3594.204518 \tValidation Loss: 906.663603\n","Epoch: 198 \tTraining Loss: 3506.296814 \tValidation Loss: 920.428877\n","Epoch: 199 \tTraining Loss: 3496.952554 \tValidation Loss: 930.290084\n","Epoch: 200 \tTraining Loss: 3609.267553 \tValidation Loss: 926.738022\n","Epoch: 201 \tTraining Loss: 3482.888938 \tValidation Loss: 940.611845\n","Epoch: 202 \tTraining Loss: 3562.759937 \tValidation Loss: 933.040490\n","Epoch: 203 \tTraining Loss: 3547.635124 \tValidation Loss: 855.807829\n","Epoch: 204 \tTraining Loss: 3517.822646 \tValidation Loss: 911.917468\n","Epoch: 205 \tTraining Loss: 3605.878745 \tValidation Loss: 909.614995\n","Epoch: 206 \tTraining Loss: 3546.537768 \tValidation Loss: 929.701839\n","Epoch: 207 \tTraining Loss: 3610.378766 \tValidation Loss: 791.962379\n","Epoch: 208 \tTraining Loss: 3622.022953 \tValidation Loss: 898.405739\n","Epoch: 209 \tTraining Loss: 3586.801438 \tValidation Loss: 918.236922\n","Epoch: 210 \tTraining Loss: 3588.997553 \tValidation Loss: 928.797434\n","Epoch: 211 \tTraining Loss: 3627.758803 \tValidation Loss: 920.746462\n","Epoch: 212 \tTraining Loss: 3522.937199 \tValidation Loss: 933.412466\n","Epoch: 213 \tTraining Loss: 3636.579459 \tValidation Loss: 911.787753\n","Epoch: 214 \tTraining Loss: 3515.274791 \tValidation Loss: 908.500341\n","Epoch: 215 \tTraining Loss: 3594.251850 \tValidation Loss: 916.395140\n","Epoch: 216 \tTraining Loss: 3576.062302 \tValidation Loss: 823.676336\n","Epoch: 217 \tTraining Loss: 3377.704178 \tValidation Loss: 902.183786\n","Epoch: 218 \tTraining Loss: 3522.446070 \tValidation Loss: 915.248374\n","Epoch: 219 \tTraining Loss: 2834.752525 \tValidation Loss: 933.683235\n","Epoch: 220 \tTraining Loss: 3530.744246 \tValidation Loss: 909.363780\n","Epoch: 221 \tTraining Loss: 3536.541333 \tValidation Loss: 863.491244\n","Epoch: 222 \tTraining Loss: 3587.639797 \tValidation Loss: 924.324057\n","Epoch: 223 \tTraining Loss: 3518.447180 \tValidation Loss: 830.877162\n","Epoch: 224 \tTraining Loss: 3529.526092 \tValidation Loss: 937.569886\n","Epoch: 225 \tTraining Loss: 3514.612096 \tValidation Loss: 922.162652\n","Epoch: 226 \tTraining Loss: 3534.272257 \tValidation Loss: 926.776176\n","Epoch: 227 \tTraining Loss: 3565.739612 \tValidation Loss: 917.345682\n","Epoch: 228 \tTraining Loss: 3558.906870 \tValidation Loss: 921.325753\n","Epoch: 229 \tTraining Loss: 3551.964952 \tValidation Loss: 926.156432\n","Epoch: 230 \tTraining Loss: 3562.573392 \tValidation Loss: 938.158328\n","Epoch: 231 \tTraining Loss: 3553.140888 \tValidation Loss: 934.311676\n","Epoch: 232 \tTraining Loss: 3477.513671 \tValidation Loss: 927.772301\n","Epoch: 233 \tTraining Loss: 3542.012683 \tValidation Loss: 907.787771\n","Epoch: 234 \tTraining Loss: 3531.008364 \tValidation Loss: 924.130086\n","Epoch: 235 \tTraining Loss: 3678.930397 \tValidation Loss: 920.835540\n","Epoch: 236 \tTraining Loss: 3286.235081 \tValidation Loss: 928.862446\n","Epoch: 237 \tTraining Loss: 3501.167850 \tValidation Loss: 841.492812\n","Epoch: 238 \tTraining Loss: 3545.530477 \tValidation Loss: 904.198208\n","Epoch: 239 \tTraining Loss: 3542.518191 \tValidation Loss: 938.501626\n","Epoch: 240 \tTraining Loss: 3556.692622 \tValidation Loss: 927.031418\n","Epoch: 241 \tTraining Loss: 2740.846567 \tValidation Loss: 829.684371\n","Epoch: 242 \tTraining Loss: 3553.666767 \tValidation Loss: 939.783813\n","Epoch: 243 \tTraining Loss: 3476.817890 \tValidation Loss: 932.856721\n","Epoch: 244 \tTraining Loss: 3518.572078 \tValidation Loss: 921.263461\n","Epoch: 245 \tTraining Loss: 3515.197388 \tValidation Loss: 750.961822\n","Epoch: 246 \tTraining Loss: 3570.095749 \tValidation Loss: 925.542868\n","Epoch: 247 \tTraining Loss: 3514.677663 \tValidation Loss: 870.150265\n","Epoch: 248 \tTraining Loss: 3619.256531 \tValidation Loss: 939.872545\n","Epoch: 249 \tTraining Loss: 3578.544484 \tValidation Loss: 934.387380\n","Epoch: 250 \tTraining Loss: 3515.034420 \tValidation Loss: 929.320966\n","Epoch: 251 \tTraining Loss: 3433.462959 \tValidation Loss: 740.507774\n","Epoch: 252 \tTraining Loss: 2747.110172 \tValidation Loss: 926.653432\n","Epoch: 253 \tTraining Loss: 3538.146007 \tValidation Loss: 918.906383\n","Epoch: 254 \tTraining Loss: 3555.055538 \tValidation Loss: 888.230886\n","Epoch: 255 \tTraining Loss: 3668.477126 \tValidation Loss: 923.010669\n","Epoch: 256 \tTraining Loss: 3541.869734 \tValidation Loss: 945.703774\n","Epoch: 257 \tTraining Loss: 3532.875641 \tValidation Loss: 939.784248\n","Epoch: 258 \tTraining Loss: 3472.040553 \tValidation Loss: 747.790288\n","Epoch: 259 \tTraining Loss: 3554.999422 \tValidation Loss: 930.836487\n","Epoch: 260 \tTraining Loss: 3613.652263 \tValidation Loss: 932.918034\n","Epoch: 261 \tTraining Loss: 3584.654187 \tValidation Loss: 922.068555\n","Epoch: 262 \tTraining Loss: 3575.788616 \tValidation Loss: 918.153573\n","Epoch: 263 \tTraining Loss: 3533.476848 \tValidation Loss: 914.379650\n","Epoch: 264 \tTraining Loss: 3544.640355 \tValidation Loss: 909.547566\n","Epoch: 265 \tTraining Loss: 3350.425888 \tValidation Loss: 866.630635\n","Epoch: 266 \tTraining Loss: 3588.750375 \tValidation Loss: 933.638776\n","Epoch: 267 \tTraining Loss: 3201.513763 \tValidation Loss: 897.476934\n","Epoch: 268 \tTraining Loss: 3551.588865 \tValidation Loss: 928.833422\n","Epoch: 269 \tTraining Loss: 3486.201001 \tValidation Loss: 741.001543\n","Epoch: 270 \tTraining Loss: 3590.045775 \tValidation Loss: 723.780760\n","Epoch: 271 \tTraining Loss: 3502.415014 \tValidation Loss: 905.486182\n","Epoch: 272 \tTraining Loss: 3430.421707 \tValidation Loss: 934.792705\n","Epoch: 273 \tTraining Loss: 3537.707358 \tValidation Loss: 913.049911\n","Epoch: 274 \tTraining Loss: 3537.698226 \tValidation Loss: 942.249296\n","Epoch: 275 \tTraining Loss: 3503.560276 \tValidation Loss: 942.318570\n","Epoch: 276 \tTraining Loss: 3523.651075 \tValidation Loss: 908.575048\n","Epoch: 277 \tTraining Loss: 3529.852860 \tValidation Loss: 829.860839\n","Epoch: 278 \tTraining Loss: 3490.733716 \tValidation Loss: 900.897260\n","Epoch: 279 \tTraining Loss: 3588.211141 \tValidation Loss: 914.338661\n","Epoch: 280 \tTraining Loss: 3538.920874 \tValidation Loss: 910.549541\n","Epoch: 281 \tTraining Loss: 3504.293069 \tValidation Loss: 777.971103\n","Epoch: 282 \tTraining Loss: 3523.391863 \tValidation Loss: 905.322845\n","Epoch: 283 \tTraining Loss: 3471.704860 \tValidation Loss: 917.096929\n","Epoch: 284 \tTraining Loss: 3595.753612 \tValidation Loss: 876.856372\n","Epoch: 285 \tTraining Loss: 3522.998770 \tValidation Loss: 817.716830\n","Epoch: 286 \tTraining Loss: 3490.091545 \tValidation Loss: 924.462879\n","Epoch: 287 \tTraining Loss: 3554.481975 \tValidation Loss: 919.131029\n","Epoch: 288 \tTraining Loss: 3493.958041 \tValidation Loss: 890.117692\n","Epoch: 289 \tTraining Loss: 3495.433080 \tValidation Loss: 910.050931\n","Epoch: 290 \tTraining Loss: 3544.212359 \tValidation Loss: 919.063558\n","Epoch: 291 \tTraining Loss: 3486.703439 \tValidation Loss: 809.806686\n","Epoch: 292 \tTraining Loss: 3520.248590 \tValidation Loss: 905.598703\n","Epoch: 293 \tTraining Loss: 3346.872143 \tValidation Loss: 906.315243\n","Epoch: 294 \tTraining Loss: 3447.168624 \tValidation Loss: 912.813842\n","Epoch: 295 \tTraining Loss: 3569.588119 \tValidation Loss: 816.960743\n","Epoch: 296 \tTraining Loss: 3459.574438 \tValidation Loss: 913.128430\n","Epoch: 297 \tTraining Loss: 3518.641665 \tValidation Loss: 918.355897\n","Epoch: 298 \tTraining Loss: 3537.335137 \tValidation Loss: 825.234842\n","Epoch: 299 \tTraining Loss: 3530.889179 \tValidation Loss: 924.485149\n","Epoch: 300 \tTraining Loss: 3564.688755 \tValidation Loss: 898.419040\n","Epoch: 301 \tTraining Loss: 3535.211194 \tValidation Loss: 923.120488\n","Epoch: 302 \tTraining Loss: 3504.374916 \tValidation Loss: 928.047094\n","Epoch: 303 \tTraining Loss: 3569.550665 \tValidation Loss: 819.967022\n","Epoch: 304 \tTraining Loss: 3545.622057 \tValidation Loss: 918.052864\n","Epoch: 305 \tTraining Loss: 3533.018181 \tValidation Loss: 833.914965\n","Epoch: 306 \tTraining Loss: 3558.082347 \tValidation Loss: 926.355819\n","Epoch: 307 \tTraining Loss: 3334.821604 \tValidation Loss: 923.851172\n","Epoch: 308 \tTraining Loss: 3498.325347 \tValidation Loss: 915.341330\n","Epoch: 309 \tTraining Loss: 3463.961515 \tValidation Loss: 921.301985\n","Epoch: 310 \tTraining Loss: 3498.165682 \tValidation Loss: 771.130929\n","Epoch: 311 \tTraining Loss: 3617.923402 \tValidation Loss: 955.411444\n","Epoch: 312 \tTraining Loss: 3496.519169 \tValidation Loss: 848.998602\n","Epoch: 313 \tTraining Loss: 3519.588391 \tValidation Loss: 858.932856\n","Epoch: 314 \tTraining Loss: 3501.547808 \tValidation Loss: 934.711627\n","Epoch: 315 \tTraining Loss: 3445.668074 \tValidation Loss: 934.173167\n","Epoch: 316 \tTraining Loss: 3486.893839 \tValidation Loss: 920.493497\n","Epoch: 317 \tTraining Loss: 3418.969370 \tValidation Loss: 903.518924\n","Epoch: 318 \tTraining Loss: 3523.646911 \tValidation Loss: 920.367432\n","Epoch: 319 \tTraining Loss: 3538.592512 \tValidation Loss: 934.101044\n","Epoch: 320 \tTraining Loss: 3448.212166 \tValidation Loss: 938.258820\n","Epoch: 321 \tTraining Loss: 3492.122049 \tValidation Loss: 906.767192\n","Epoch: 322 \tTraining Loss: 3557.818696 \tValidation Loss: 814.374997\n","Epoch: 323 \tTraining Loss: 3584.544465 \tValidation Loss: 919.780658\n","Epoch: 324 \tTraining Loss: 3512.137365 \tValidation Loss: 844.148900\n","Epoch: 325 \tTraining Loss: 3495.675602 \tValidation Loss: 841.232426\n","Epoch: 326 \tTraining Loss: 3577.145614 \tValidation Loss: 917.169824\n","Epoch: 327 \tTraining Loss: 3538.884040 \tValidation Loss: 918.113174\n","Epoch: 328 \tTraining Loss: 3483.751905 \tValidation Loss: 910.373114\n","Epoch: 329 \tTraining Loss: 3478.914015 \tValidation Loss: 806.017214\n","Epoch: 330 \tTraining Loss: 3422.263017 \tValidation Loss: 888.532506\n","Epoch: 331 \tTraining Loss: 3220.005841 \tValidation Loss: 929.866173\n","Epoch: 332 \tTraining Loss: 3494.197872 \tValidation Loss: 938.046882\n","Epoch: 333 \tTraining Loss: 3438.821524 \tValidation Loss: 929.140040\n","Epoch: 334 \tTraining Loss: 3574.169910 \tValidation Loss: 928.314885\n","Epoch: 335 \tTraining Loss: 3488.687022 \tValidation Loss: 876.733389\n","Epoch: 336 \tTraining Loss: 3509.607013 \tValidation Loss: 928.233190\n","Epoch: 337 \tTraining Loss: 3460.922671 \tValidation Loss: 921.168419\n","Epoch: 338 \tTraining Loss: 3559.480618 \tValidation Loss: 940.733136\n","Epoch: 339 \tTraining Loss: 3593.068153 \tValidation Loss: 939.881101\n","Epoch: 340 \tTraining Loss: 3459.179738 \tValidation Loss: 930.674709\n","Epoch: 341 \tTraining Loss: 3361.750096 \tValidation Loss: 923.521361\n","Epoch: 342 \tTraining Loss: 3483.828239 \tValidation Loss: 919.942749\n","Epoch: 343 \tTraining Loss: 3549.401489 \tValidation Loss: 718.280966\n","Epoch: 344 \tTraining Loss: 3500.541889 \tValidation Loss: 938.551370\n","Epoch: 345 \tTraining Loss: 3532.771786 \tValidation Loss: 941.187888\n","Epoch: 346 \tTraining Loss: 3325.356657 \tValidation Loss: 770.190774\n","Epoch: 347 \tTraining Loss: 3550.352535 \tValidation Loss: 894.227854\n","Epoch: 348 \tTraining Loss: 3553.895247 \tValidation Loss: 906.681354\n","Epoch: 349 \tTraining Loss: 3589.353726 \tValidation Loss: 911.820081\n","Epoch: 350 \tTraining Loss: 3542.324086 \tValidation Loss: 901.751745\n","Epoch: 351 \tTraining Loss: 3358.239558 \tValidation Loss: 910.155222\n","Epoch: 352 \tTraining Loss: 3509.928012 \tValidation Loss: 917.133789\n","Epoch: 353 \tTraining Loss: 3568.927917 \tValidation Loss: 907.523000\n","Epoch: 354 \tTraining Loss: 3501.294112 \tValidation Loss: 880.015038\n","Epoch: 355 \tTraining Loss: 3458.937976 \tValidation Loss: 920.821282\n","Epoch: 356 \tTraining Loss: 3553.756964 \tValidation Loss: 899.216732\n","Epoch: 357 \tTraining Loss: 3410.294760 \tValidation Loss: 914.248715\n","Epoch: 358 \tTraining Loss: 3431.141519 \tValidation Loss: 919.492042\n","Epoch: 359 \tTraining Loss: 3584.627949 \tValidation Loss: 941.714132\n","Epoch: 360 \tTraining Loss: 3231.339148 \tValidation Loss: 811.287557\n","Epoch: 361 \tTraining Loss: 3559.523538 \tValidation Loss: 920.717265\n","Epoch: 362 \tTraining Loss: 3473.241519 \tValidation Loss: 928.438809\n","Epoch: 363 \tTraining Loss: 3563.504891 \tValidation Loss: 919.146927\n","Epoch: 364 \tTraining Loss: 3533.274045 \tValidation Loss: 917.761629\n","Epoch: 365 \tTraining Loss: 3497.469975 \tValidation Loss: 773.244192\n","Epoch: 366 \tTraining Loss: 3561.932573 \tValidation Loss: 926.564127\n","Epoch: 367 \tTraining Loss: 3513.555005 \tValidation Loss: 917.128508\n","Epoch: 368 \tTraining Loss: 3515.954560 \tValidation Loss: 934.565924\n","Epoch: 369 \tTraining Loss: 3515.256230 \tValidation Loss: 907.526021\n","Epoch: 370 \tTraining Loss: 3478.500462 \tValidation Loss: 899.024908\n","Epoch: 371 \tTraining Loss: 3481.789714 \tValidation Loss: 739.918334\n","Epoch: 372 \tTraining Loss: 3451.255990 \tValidation Loss: 928.195087\n","Epoch: 373 \tTraining Loss: 3441.543987 \tValidation Loss: 930.693787\n","Epoch: 374 \tTraining Loss: 3421.679936 \tValidation Loss: 914.088260\n","Epoch: 375 \tTraining Loss: 3417.513519 \tValidation Loss: 935.774387\n","Epoch: 376 \tTraining Loss: 3570.929799 \tValidation Loss: 892.317568\n","Epoch: 377 \tTraining Loss: 3565.762984 \tValidation Loss: 940.310238\n","Epoch: 378 \tTraining Loss: 3547.892386 \tValidation Loss: 939.861884\n","Epoch: 379 \tTraining Loss: 3487.577420 \tValidation Loss: 927.196007\n","Epoch: 380 \tTraining Loss: 3589.891445 \tValidation Loss: 907.528062\n","Epoch: 381 \tTraining Loss: 3435.696842 \tValidation Loss: 822.765490\n","Epoch: 382 \tTraining Loss: 3537.284392 \tValidation Loss: 946.509277\n","Epoch: 383 \tTraining Loss: 3571.371333 \tValidation Loss: 921.371467\n","Epoch: 384 \tTraining Loss: 3568.422438 \tValidation Loss: 885.416856\n","Epoch: 385 \tTraining Loss: 3531.806174 \tValidation Loss: 910.453249\n","Epoch: 386 \tTraining Loss: 3558.343727 \tValidation Loss: 921.700055\n","Epoch: 387 \tTraining Loss: 3440.433331 \tValidation Loss: 882.471776\n","Epoch: 388 \tTraining Loss: 3497.936126 \tValidation Loss: 919.756853\n","Epoch: 389 \tTraining Loss: 3565.650736 \tValidation Loss: 920.192272\n","Epoch: 390 \tTraining Loss: 3472.358243 \tValidation Loss: 930.493050\n","Epoch: 391 \tTraining Loss: 3510.644250 \tValidation Loss: 908.892687\n","Epoch: 392 \tTraining Loss: 3535.662079 \tValidation Loss: 891.944155\n","Epoch: 393 \tTraining Loss: 3544.798003 \tValidation Loss: 924.106674\n","Epoch: 394 \tTraining Loss: 3607.565091 \tValidation Loss: 919.231493\n","Epoch: 395 \tTraining Loss: 3499.620817 \tValidation Loss: 910.985127\n","Epoch: 396 \tTraining Loss: 3518.209177 \tValidation Loss: 893.146823\n","Epoch: 397 \tTraining Loss: 3442.841378 \tValidation Loss: 912.532562\n","Epoch: 398 \tTraining Loss: 3451.407067 \tValidation Loss: 917.585555\n","Epoch: 399 \tTraining Loss: 3526.182022 \tValidation Loss: 915.413056\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MPiYWv9UoFP_","colab_type":"code","outputId":"78b8212d-d385-4687-cd0e-34973134cfd3","executionInfo":{"status":"ok","timestamp":1586923766564,"user_tz":300,"elapsed":622,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["net.load_state_dict(torch.load('lstm_test.pt'))"],"execution_count":381,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":381}]},{"cell_type":"code","metadata":{"id":"LOYJ_iJ0gCED","colab_type":"code","outputId":"74a43145-6f7b-4ab0-b710-9eb8993ed03e","executionInfo":{"status":"ok","timestamp":1586923766881,"user_tz":300,"elapsed":592,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":199}},"source":["\n","\n","# track test loss\n","test_loss = 0.0\n","mean_abs = 0.0\n","\n","net.eval()\n","# iterate over test data\n","#bin_op.binarization()\n","test_h = net.init_hidden(batch_size)\n","\n","for batch_idx, (X_temp, X_num, X_cat, labels) in enumerate(test_loader):\n","    if(train_on_gpu):\n","        X_temp, X_num, X_cat, labels = X_temp.cuda(), X_num.cuda(), X_cat.type(torch.LongTensor).cuda(), labels.cuda()\n","    \n","    test_h = tuple([each.data for each in test_h])\n","\n","    # get the output from the model\n","    lstm_output, h, num_output, cat_output = net(X_temp, h, X_num, X_cat)\n","\n","    output = (weight_lstm*lstm_output + weight_num*num_output + weight_cat*cat_output) \n","    # / (weight_lstm + weight_num + weight_cat)\n","\n","    # calculate the loss and perform backprop\n","    loss = criterion(output.squeeze(), labels.double())\n","\n","    out_np = output.detach().squeeze().cpu().numpy().astype(int).T\n","    target_np = labels.detach().cpu().numpy().astype(int).T\n","\n","    print([(out, tar) for out, tar in zip(out_np, target_np)])\n","\n","    mean_abs += mean_absolute_error(out_np, target_np) * batch_size\n","        \n","    test_loss += loss.item() * batch_size\n","\n","# calculate average losses\n","test_loss = np.sqrt(test_loss / len(test_loader.dataset))\n","mean_abs /= len(test_loader.dataset)\n","print('Test Loss: {:.6f}\\n'.format(test_loss))\n","print('Mean Abs Loss: {:.6f}\\n'.format(mean_abs))"],"execution_count":382,"outputs":[{"output_type":"stream","text":["[(4, 3), (4, 3), (3, 2), (6, 2), (5, 2), (5, 3), (6, 3), (2, 3), (2, 2), (2, 6), (2, 4), (2, 4), (2, 5), (2, 3), (2, 2), (5, 16)]\n","[(4, 4), (2, 10), (2, 5), (2, 3), (2, 8), (9, 2), (6, 9), (2, 66), (7, 3), (5, 9), (5, 11), (5, 8), (4, 25), (4, 4), (8, 3), (7, 2)]\n","[(7, 3), (6, 3), (7, 6), (8, 3), (8, 8), (8, 8), (7, 6), (7, 9), (8, 12), (8, 18), (7, 24), (7, 19), (7, 3), (7, 15), (8, 12), (8, 12)]\n","[(7, 17), (7, 9), (7, 3), (7, 12), (7, 33), (7, 11), (7, 6), (7, 2), (7, 7), (7, 5), (7, 2), (8, 11), (8, 23), (8, 4), (7, 7), (7, 5)]\n","[(6, 6), (7, 10), (7, 12), (6, 5), (7, 10), (5, 2), (7, 3), (7, 32), (7, 13), (5, 27), (8, 9), (8, 12), (5, 16), (8, 6), (5, 3), (7, 7)]\n","[(5, 2), (5, 6), (7, 10), (7, 6), (7, 2), (4, 3), (5, 5), (7, 2), (7, 5), (4, 17), (7, 18), (7, 3), (4, 8), (7, 2), (7, 15), (4, 5)]\n","Test Loss: 8.987268\n","\n","Mean Abs Loss: 4.809524\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fdmJvO0ZNXDu","colab_type":"text"},"source":["## Mixed Model"]},{"cell_type":"code","metadata":{"id":"iALvtBlwNZc0","colab_type":"code","colab":{}},"source":["class MosquitoMixed(nn.Module):\n","    def __init__(self, \n","                 lstm_columns, lstm_hidden_dim, lstm_n_layers, lstm_output,\n","                 num_columns, num_hidden_dim,\n","                 embedding_sizes, cat_hidden_dim,\n","                 combined_dim):\n","        super(MosquitoMixed, self).__init__()\n","\n","        self.lstm_n_layers = lstm_n_layers\n","        self.lstm_hidden_dim = lstm_hidden_dim\n","\n","        self.num_hidden_dim = num_hidden_dim\n","        self.cat_hidden_dim = cat_hidden_dim\n","\n","        embed_size = 0\n","        for embed in embedding_sizes:\n","            embed_size += embed[1]\n","\n","        self.all_embeddings = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in embedding_sizes])\n","\n","        # For LSTM\n","        self.lstm = nn.LSTM(lstm_columns, lstm_hidden_dim, lstm_n_layers,\n","                            dropout=0.3, batch_first=True)\n","        \n","        # LSTM dense layer\n","        self.lstm_dense = nn.Sequential(\n","            nn.BatchNorm1d(lstm_hidden_dim),\n","            nn.Dropout(0.3),\n","            nn.Linear(lstm_hidden_dim, lstm_output),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm1d(lstm_output)\n","        )\n","        \n","        self.num_dense = nn.Sequential(\n","            nn.Dropout(0.3),\n","            nn.Linear(num_columns, num_hidden_dim),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm1d(num_hidden_dim),\n","            nn.Dropout(0.3),\n","            nn.Linear(num_hidden_dim, num_hidden_dim),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm1d(num_hidden_dim)\n","        )\n","\n","        self.cat_dense = nn.Sequential(\n","            nn.Dropout(0.3),\n","            nn.Linear(embed_size, cat_hidden_dim),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm1d(cat_hidden_dim),\n","            nn.Dropout(0.3),\n","            nn.Linear(cat_hidden_dim, cat_hidden_dim),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm1d(cat_hidden_dim)\n","        )\n","\n","        combined_output = lstm_output + num_hidden_dim + cat_hidden_dim\n","\n","        self.combined_dense = nn.Sequential(\n","            nn.Dropout(0.3),\n","            nn.Linear(combined_output, combined_dim),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm1d(combined_dim),\n","            nn.Dropout(0.3),\n","            nn.Linear(combined_dim, combined_dim),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm1d(combined_dim),\n","            nn.Linear(combined_dim, 1),\n","            nn.ReLU()\n","        )\n","\n","    def forward(self, X_temp, hidden, X_num, X_cat):\n","        # Forward lstm\n","        lstm_out, hidden = self.lstm(X_temp, hidden)\n","        # stack up lstm outputs\n","        lstm_out = lstm_out.contiguous().view(-1, self.lstm_hidden_dim)\n","        # dropout and fully-connected layer\n","        lstm_out = self.lstm_dense(lstm_out)\n","      \n","        # reshape to be batch_size first\n","        lstm_out = lstm_out.view(batch_size, SEQ_LENGTH, -1)\n","        lstm_out = lstm_out[:, -1, :]\n","\n","        # Forward num\n","        num_out = self.num_dense(X_num)\n","\n","        # Forward cat\n","        embeddings = []\n","        for i, e in enumerate(self.all_embeddings):\n","            embed = e(X_cat[:, i])\n","            embeddings.append(embed)\n","\n","        cat_out = torch.cat(embeddings, 1)\n","        cat_out = self.cat_dense(cat_out)\n","\n","        combined_out = torch.cat((lstm_out, num_out, cat_out), dim=1)\n","        combined_out = self.combined_dense(combined_out)\n","\n","        return combined_out, hidden\n","\n","    def init_hidden(self, batch_size):\n","        ''' Initializes hidden state '''\n","        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n","        # initialized to zero, for hidden state and cell state of LSTM\n","        weight = next(self.parameters()).data\n","        \n","        if (train_on_gpu):\n","            hidden = (weight.new(self.lstm_n_layers, batch_size, self.lstm_hidden_dim).zero_().cuda(),\n","                  weight.new(self.lstm_n_layers, batch_size, self.lstm_hidden_dim).zero_().cuda())\n","        else:\n","            hidden = (weight.new(self.lstm_n_layers, batch_size, self.lstm_hidden_dim).zero_(),\n","                      weight.new(self.lstm_n_layers, batch_size, self.lstm_hidden_dim).zero_())\n","        \n","        return hidden"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nG0TTenANiCE","colab_type":"code","outputId":"7256b60c-7ff9-40c5-c3d4-a28f79c6604a","executionInfo":{"status":"ok","timestamp":1586927566063,"user_tz":300,"elapsed":623,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Instantiate the model w/ hyperparams\n","a_h = 3\n","\n","lstm_hidden_dim = np.int(len(train_idx) / (a_h * len(numerical_columns) + 1))\n","print(lstm_hidden_dim)\n","lstm_n_layers = 2\n","\n","num_hidden_dim = 15\n","cat_hidden_dim = 15\n","lstm_output = 15\n","combined_dim = 30\n","\n","#net = MosquitoLSTM(len(numerical_columns), hidden_dim, n_layers).double()\n","net = MosquitoMixed(\n","    len(numerical_columns), lstm_hidden_dim, lstm_n_layers, lstm_output,\n","    len(non_num_columns), num_hidden_dim,\n","    embedding_sizes, cat_hidden_dim,\n","    combined_dim\n",").double()"],"execution_count":435,"outputs":[{"output_type":"stream","text":["23\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ke9eeDONNieR","colab_type":"code","colab":{}},"source":["lr = 0.0005\n","\n","criterion = nn.MSELoss()\n","#optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n","optimizer = torch.optim.RMSprop(net.parameters(), lr=lr)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QmqNAusjNlqH","colab_type":"code","outputId":"bd3410bb-5f90-4d72-8c08-7d39445e09cd","executionInfo":{"status":"ok","timestamp":1586927743453,"user_tz":300,"elapsed":171250,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# training params\n","\n","epochs = 400 # 3-4 is approx where I noticed the validation loss stop decreasing\n","\n","counter = 0\n","print_every = 10\n","clip = 8 # gradient clipping\n","\n","# move model to GPU, if available\n","if(train_on_gpu):\n","    net.cuda()\n","\n","net.train()\n","# train for some number of epochs\n","valid_loss_min = np.Inf\n","\n","for epoch in range(epochs):\n","    # initialize hidden state\n","    h = net.init_hidden(batch_size)\n","\n","    train_loss = 0.0\n","    valid_loss = 0.0\n","\n","    net.train()\n","\n","    # batch loop\n","    for X_temp, X_num, X_cat, labels in train_loader:\n","        counter += 1\n","\n","        if(train_on_gpu):\n","            X_temp, X_num, X_cat, labels = X_temp.cuda(), X_num.cuda(), X_cat.type(torch.LongTensor).cuda(), labels.cuda()\n","\n","        # Creating new variables for the hidden state, otherwise\n","        # we'd backprop through the entire training history\n","        h = tuple([each.data for each in h])\n","        # zero accumulated gradients\n","        net.zero_grad()\n","\n","        # get the output from the model\n","        output, h = net(X_temp, h, X_num, X_cat)\n","\n","        # calculate the loss and perform backprop\n","        loss = criterion(output.squeeze(), labels.double())\n","        loss.backward()\n","        train_loss += loss.item()\n","        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","        nn.utils.clip_grad_norm_(net.parameters(), clip)\n","        optimizer.step()\n","\n","    net.eval()\n","\n","    val_h = net.init_hidden(batch_size)\n","\n","    for X_temp, X_num, X_cat, labels in valid_loader:\n","        \n","        \n","        if(train_on_gpu):\n","            X_temp, X_num, X_cat, labels = X_temp.cuda(), X_num.cuda(), X_cat.type(torch.LongTensor).cuda(), labels.cuda()\n","\n","        # Creating new variables for the hidden state, otherwise\n","        # we'd backprop through the entire training history\n","        val_h = tuple([each.data for each in val_h])\n","\n","        # get the output from the model\n","        output, h = net(X_temp, h, X_num, X_cat)\n","\n","        # calculate the loss and perform backprop\n","        loss = criterion(output.squeeze(), labels.double())\n","        valid_loss += loss.item()\n","\n","    # save model if validation loss has decreased\n","    if valid_loss <= valid_loss_min:\n","        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","        valid_loss_min,\n","        valid_loss))\n","        torch.save(net.state_dict(), 'mixed_model.pt')\n","        valid_loss_min = valid_loss\n","        \n","    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n","            epoch, train_loss, valid_loss))"],"execution_count":438,"outputs":[{"output_type":"stream","text":["Validation loss decreased (inf --> 1408.388703).  Saving model ...\n","Epoch: 0 \tTraining Loss: 5613.927077 \tValidation Loss: 1408.388703\n","Epoch: 1 \tTraining Loss: 5464.741120 \tValidation Loss: 1425.810636\n","Validation loss decreased (1408.388703 --> 1398.965393).  Saving model ...\n","Epoch: 2 \tTraining Loss: 5332.130202 \tValidation Loss: 1398.965393\n","Validation loss decreased (1398.965393 --> 1121.663717).  Saving model ...\n","Epoch: 3 \tTraining Loss: 5184.529398 \tValidation Loss: 1121.663717\n","Epoch: 4 \tTraining Loss: 5103.049155 \tValidation Loss: 1327.744298\n","Epoch: 5 \tTraining Loss: 4991.016889 \tValidation Loss: 1251.445485\n","Epoch: 6 \tTraining Loss: 4956.225093 \tValidation Loss: 1255.050812\n","Epoch: 7 \tTraining Loss: 3920.977041 \tValidation Loss: 1225.111348\n","Epoch: 8 \tTraining Loss: 4758.061543 \tValidation Loss: 1242.986504\n","Epoch: 9 \tTraining Loss: 4698.551310 \tValidation Loss: 1223.246880\n","Epoch: 10 \tTraining Loss: 4575.154572 \tValidation Loss: 1181.207521\n","Epoch: 11 \tTraining Loss: 4562.272252 \tValidation Loss: 1187.592221\n","Validation loss decreased (1121.663717 --> 1003.328698).  Saving model ...\n","Epoch: 12 \tTraining Loss: 4601.256597 \tValidation Loss: 1003.328698\n","Validation loss decreased (1003.328698 --> 943.293971).  Saving model ...\n","Epoch: 13 \tTraining Loss: 4549.384667 \tValidation Loss: 943.293971\n","Epoch: 14 \tTraining Loss: 4475.240220 \tValidation Loss: 1087.989385\n","Epoch: 15 \tTraining Loss: 4314.748794 \tValidation Loss: 1172.764157\n","Epoch: 16 \tTraining Loss: 3619.717063 \tValidation Loss: 1137.183387\n","Epoch: 17 \tTraining Loss: 4464.598506 \tValidation Loss: 1044.737271\n","Epoch: 18 \tTraining Loss: 4416.828679 \tValidation Loss: 1106.009552\n","Epoch: 19 \tTraining Loss: 4378.109719 \tValidation Loss: 1099.915399\n","Epoch: 20 \tTraining Loss: 4353.555397 \tValidation Loss: 1113.140434\n","Epoch: 21 \tTraining Loss: 4338.986533 \tValidation Loss: 1082.377342\n","Epoch: 22 \tTraining Loss: 4439.005950 \tValidation Loss: 1143.776270\n","Epoch: 23 \tTraining Loss: 4360.307087 \tValidation Loss: 1100.741529\n","Epoch: 24 \tTraining Loss: 4274.916120 \tValidation Loss: 1080.669816\n","Epoch: 25 \tTraining Loss: 4344.885421 \tValidation Loss: 1060.975055\n","Epoch: 26 \tTraining Loss: 4303.224271 \tValidation Loss: 1110.640613\n","Epoch: 27 \tTraining Loss: 4202.681283 \tValidation Loss: 1082.347487\n","Epoch: 28 \tTraining Loss: 4199.208450 \tValidation Loss: 1077.824932\n","Epoch: 29 \tTraining Loss: 4234.628685 \tValidation Loss: 1056.267335\n","Epoch: 30 \tTraining Loss: 4155.500540 \tValidation Loss: 1012.655533\n","Epoch: 31 \tTraining Loss: 4194.901412 \tValidation Loss: 1044.444875\n","Epoch: 32 \tTraining Loss: 4219.328675 \tValidation Loss: 1063.745791\n","Epoch: 33 \tTraining Loss: 4105.243827 \tValidation Loss: 1042.981109\n","Epoch: 34 \tTraining Loss: 4181.943315 \tValidation Loss: 999.559516\n","Epoch: 35 \tTraining Loss: 4118.503295 \tValidation Loss: 976.768237\n","Validation loss decreased (943.293971 --> 826.242358).  Saving model ...\n","Epoch: 36 \tTraining Loss: 4201.622486 \tValidation Loss: 826.242358\n","Epoch: 37 \tTraining Loss: 4181.090819 \tValidation Loss: 897.210557\n","Epoch: 38 \tTraining Loss: 4130.551755 \tValidation Loss: 992.202108\n","Epoch: 39 \tTraining Loss: 3807.274130 \tValidation Loss: 1011.601095\n","Epoch: 40 \tTraining Loss: 3250.264448 \tValidation Loss: 970.678512\n","Epoch: 41 \tTraining Loss: 4046.993914 \tValidation Loss: 938.389730\n","Epoch: 42 \tTraining Loss: 4244.871480 \tValidation Loss: 963.981151\n","Epoch: 43 \tTraining Loss: 4076.617916 \tValidation Loss: 1004.278110\n","Epoch: 44 \tTraining Loss: 4047.879851 \tValidation Loss: 989.865758\n","Epoch: 45 \tTraining Loss: 4091.001101 \tValidation Loss: 880.768318\n","Epoch: 46 \tTraining Loss: 4060.057595 \tValidation Loss: 902.455675\n","Epoch: 47 \tTraining Loss: 4072.207376 \tValidation Loss: 969.150708\n","Epoch: 48 \tTraining Loss: 4073.112569 \tValidation Loss: 987.304027\n","Epoch: 49 \tTraining Loss: 4047.838639 \tValidation Loss: 861.442664\n","Epoch: 50 \tTraining Loss: 4177.974648 \tValidation Loss: 863.570199\n","Epoch: 51 \tTraining Loss: 3970.239132 \tValidation Loss: 988.074335\n","Epoch: 52 \tTraining Loss: 4123.502215 \tValidation Loss: 985.662650\n","Epoch: 53 \tTraining Loss: 4092.470368 \tValidation Loss: 890.169742\n","Epoch: 54 \tTraining Loss: 4069.390346 \tValidation Loss: 987.543667\n","Epoch: 55 \tTraining Loss: 3962.132708 \tValidation Loss: 944.412945\n","Epoch: 56 \tTraining Loss: 4077.997454 \tValidation Loss: 1052.463173\n","Epoch: 57 \tTraining Loss: 4078.555309 \tValidation Loss: 1112.451932\n","Epoch: 58 \tTraining Loss: 4064.598502 \tValidation Loss: 854.911056\n","Epoch: 59 \tTraining Loss: 4056.738984 \tValidation Loss: 957.020979\n","Epoch: 60 \tTraining Loss: 4116.089262 \tValidation Loss: 1017.513853\n","Epoch: 61 \tTraining Loss: 3963.247367 \tValidation Loss: 1073.132700\n","Epoch: 62 \tTraining Loss: 4138.824250 \tValidation Loss: 1014.207854\n","Epoch: 63 \tTraining Loss: 4150.220491 \tValidation Loss: 906.632745\n","Epoch: 64 \tTraining Loss: 4048.727386 \tValidation Loss: 961.837957\n","Epoch: 65 \tTraining Loss: 4102.823576 \tValidation Loss: 962.172904\n","Epoch: 66 \tTraining Loss: 4090.797924 \tValidation Loss: 926.069977\n","Epoch: 67 \tTraining Loss: 3795.896220 \tValidation Loss: 977.148407\n","Epoch: 68 \tTraining Loss: 4147.295290 \tValidation Loss: 967.199216\n","Epoch: 69 \tTraining Loss: 4034.613794 \tValidation Loss: 962.481669\n","Epoch: 70 \tTraining Loss: 4035.007639 \tValidation Loss: 939.667193\n","Epoch: 71 \tTraining Loss: 4125.533224 \tValidation Loss: 933.221110\n","Epoch: 72 \tTraining Loss: 4069.902220 \tValidation Loss: 942.480131\n","Epoch: 73 \tTraining Loss: 4088.686642 \tValidation Loss: 942.801775\n","Epoch: 74 \tTraining Loss: 4055.526095 \tValidation Loss: 840.282099\n","Epoch: 75 \tTraining Loss: 3991.432550 \tValidation Loss: 1008.420676\n","Epoch: 76 \tTraining Loss: 4152.975843 \tValidation Loss: 938.096740\n","Epoch: 77 \tTraining Loss: 4020.895529 \tValidation Loss: 965.944157\n","Epoch: 78 \tTraining Loss: 3110.366464 \tValidation Loss: 952.431310\n","Epoch: 79 \tTraining Loss: 3930.716965 \tValidation Loss: 942.969531\n","Epoch: 80 \tTraining Loss: 3866.670220 \tValidation Loss: 959.970517\n","Epoch: 81 \tTraining Loss: 4018.208919 \tValidation Loss: 949.652657\n","Epoch: 82 \tTraining Loss: 3921.949018 \tValidation Loss: 944.501743\n","Epoch: 83 \tTraining Loss: 3961.154764 \tValidation Loss: 958.912900\n","Epoch: 84 \tTraining Loss: 3983.718756 \tValidation Loss: 933.232008\n","Epoch: 85 \tTraining Loss: 4027.256970 \tValidation Loss: 948.677360\n","Epoch: 86 \tTraining Loss: 3874.818439 \tValidation Loss: 956.284596\n","Validation loss decreased (826.242358 --> 821.236630).  Saving model ...\n","Epoch: 87 \tTraining Loss: 3884.709025 \tValidation Loss: 821.236630\n","Epoch: 88 \tTraining Loss: 4031.747770 \tValidation Loss: 842.030395\n","Epoch: 89 \tTraining Loss: 3881.810312 \tValidation Loss: 958.215557\n","Epoch: 90 \tTraining Loss: 3968.084079 \tValidation Loss: 905.333580\n","Epoch: 91 \tTraining Loss: 3956.481463 \tValidation Loss: 954.600212\n","Epoch: 92 \tTraining Loss: 3788.388302 \tValidation Loss: 960.617429\n","Epoch: 93 \tTraining Loss: 3970.818295 \tValidation Loss: 933.132775\n","Epoch: 94 \tTraining Loss: 3976.993080 \tValidation Loss: 909.227254\n","Epoch: 95 \tTraining Loss: 3985.898502 \tValidation Loss: 918.199320\n","Epoch: 96 \tTraining Loss: 3948.008072 \tValidation Loss: 839.805573\n","Epoch: 97 \tTraining Loss: 3938.150116 \tValidation Loss: 960.602816\n","Epoch: 98 \tTraining Loss: 3959.165266 \tValidation Loss: 950.772389\n","Epoch: 99 \tTraining Loss: 3986.261632 \tValidation Loss: 925.595149\n","Epoch: 100 \tTraining Loss: 3696.849486 \tValidation Loss: 938.048900\n","Epoch: 101 \tTraining Loss: 3831.556138 \tValidation Loss: 955.273490\n","Epoch: 102 \tTraining Loss: 3899.697022 \tValidation Loss: 970.195425\n","Epoch: 103 \tTraining Loss: 3584.741736 \tValidation Loss: 960.626984\n","Epoch: 104 \tTraining Loss: 3852.817641 \tValidation Loss: 954.021551\n","Validation loss decreased (821.236630 --> 686.134851).  Saving model ...\n","Epoch: 105 \tTraining Loss: 3898.904234 \tValidation Loss: 686.134851\n","Epoch: 106 \tTraining Loss: 3927.646464 \tValidation Loss: 817.860457\n","Epoch: 107 \tTraining Loss: 3883.992979 \tValidation Loss: 866.125686\n","Epoch: 108 \tTraining Loss: 3982.654794 \tValidation Loss: 959.192565\n","Epoch: 109 \tTraining Loss: 3933.059689 \tValidation Loss: 960.835640\n","Epoch: 110 \tTraining Loss: 3839.016282 \tValidation Loss: 951.876734\n","Epoch: 111 \tTraining Loss: 3927.581441 \tValidation Loss: 951.600701\n","Epoch: 112 \tTraining Loss: 3953.218749 \tValidation Loss: 951.169358\n","Epoch: 113 \tTraining Loss: 3875.593121 \tValidation Loss: 946.823861\n","Epoch: 114 \tTraining Loss: 3909.860712 \tValidation Loss: 960.533286\n","Epoch: 115 \tTraining Loss: 3935.103749 \tValidation Loss: 973.840727\n","Epoch: 116 \tTraining Loss: 3824.950579 \tValidation Loss: 935.004408\n","Epoch: 117 \tTraining Loss: 4003.172553 \tValidation Loss: 903.461645\n","Epoch: 118 \tTraining Loss: 3943.298866 \tValidation Loss: 951.794443\n","Epoch: 119 \tTraining Loss: 3947.015909 \tValidation Loss: 947.644554\n","Epoch: 120 \tTraining Loss: 3900.040645 \tValidation Loss: 900.445399\n","Epoch: 121 \tTraining Loss: 3919.212613 \tValidation Loss: 916.726483\n","Epoch: 122 \tTraining Loss: 3890.693921 \tValidation Loss: 922.839198\n","Epoch: 123 \tTraining Loss: 3903.416673 \tValidation Loss: 922.226310\n","Epoch: 124 \tTraining Loss: 3575.283223 \tValidation Loss: 941.171741\n","Epoch: 125 \tTraining Loss: 3436.880896 \tValidation Loss: 937.806189\n","Epoch: 126 \tTraining Loss: 3895.681430 \tValidation Loss: 956.774494\n","Epoch: 127 \tTraining Loss: 3888.372303 \tValidation Loss: 750.035700\n","Epoch: 128 \tTraining Loss: 3919.007015 \tValidation Loss: 951.480005\n","Epoch: 129 \tTraining Loss: 3949.808791 \tValidation Loss: 952.989084\n","Epoch: 130 \tTraining Loss: 3888.520019 \tValidation Loss: 944.213088\n","Epoch: 131 \tTraining Loss: 3965.543775 \tValidation Loss: 949.972197\n","Epoch: 132 \tTraining Loss: 3905.122032 \tValidation Loss: 933.998634\n","Epoch: 133 \tTraining Loss: 3935.252929 \tValidation Loss: 934.713846\n","Epoch: 134 \tTraining Loss: 3512.434405 \tValidation Loss: 953.519575\n","Epoch: 135 \tTraining Loss: 3955.216780 \tValidation Loss: 950.607637\n","Epoch: 136 \tTraining Loss: 3970.025690 \tValidation Loss: 957.236112\n","Epoch: 137 \tTraining Loss: 3928.170874 \tValidation Loss: 831.009051\n","Epoch: 138 \tTraining Loss: 3900.873248 \tValidation Loss: 954.246059\n","Epoch: 139 \tTraining Loss: 3874.931531 \tValidation Loss: 950.273338\n","Epoch: 140 \tTraining Loss: 3797.565418 \tValidation Loss: 896.843971\n","Epoch: 141 \tTraining Loss: 3923.733034 \tValidation Loss: 736.944530\n","Epoch: 142 \tTraining Loss: 3879.206722 \tValidation Loss: 912.266101\n","Epoch: 143 \tTraining Loss: 3833.029541 \tValidation Loss: 932.609358\n","Epoch: 144 \tTraining Loss: 3703.453633 \tValidation Loss: 805.974954\n","Epoch: 145 \tTraining Loss: 3902.962707 \tValidation Loss: 897.659189\n","Epoch: 146 \tTraining Loss: 3827.417131 \tValidation Loss: 927.317976\n","Epoch: 147 \tTraining Loss: 3942.422723 \tValidation Loss: 956.230874\n","Epoch: 148 \tTraining Loss: 3872.581927 \tValidation Loss: 944.296371\n","Epoch: 149 \tTraining Loss: 3853.202416 \tValidation Loss: 867.030378\n","Epoch: 150 \tTraining Loss: 3852.535661 \tValidation Loss: 922.425064\n","Epoch: 151 \tTraining Loss: 3859.447919 \tValidation Loss: 943.816486\n","Epoch: 152 \tTraining Loss: 3852.204243 \tValidation Loss: 938.464315\n","Epoch: 153 \tTraining Loss: 3916.001362 \tValidation Loss: 790.163682\n","Epoch: 154 \tTraining Loss: 3861.550000 \tValidation Loss: 950.334446\n","Epoch: 155 \tTraining Loss: 3895.096539 \tValidation Loss: 919.476355\n","Epoch: 156 \tTraining Loss: 3853.199730 \tValidation Loss: 954.340687\n","Epoch: 157 \tTraining Loss: 3825.232060 \tValidation Loss: 917.653944\n","Epoch: 158 \tTraining Loss: 3877.212665 \tValidation Loss: 901.807944\n","Epoch: 159 \tTraining Loss: 3868.545157 \tValidation Loss: 917.903498\n","Epoch: 160 \tTraining Loss: 3957.919905 \tValidation Loss: 930.019592\n","Epoch: 161 \tTraining Loss: 3881.980869 \tValidation Loss: 898.005400\n","Epoch: 162 \tTraining Loss: 3768.268926 \tValidation Loss: 949.861536\n","Epoch: 163 \tTraining Loss: 3833.436163 \tValidation Loss: 928.268745\n","Epoch: 164 \tTraining Loss: 3830.257455 \tValidation Loss: 915.828100\n","Epoch: 165 \tTraining Loss: 3780.652626 \tValidation Loss: 919.847624\n","Epoch: 166 \tTraining Loss: 3924.305331 \tValidation Loss: 924.877051\n","Epoch: 167 \tTraining Loss: 3931.622856 \tValidation Loss: 929.500151\n","Epoch: 168 \tTraining Loss: 3875.610258 \tValidation Loss: 873.928296\n","Epoch: 169 \tTraining Loss: 3684.575731 \tValidation Loss: 906.166183\n","Epoch: 170 \tTraining Loss: 3800.564803 \tValidation Loss: 930.821030\n","Epoch: 171 \tTraining Loss: 3915.773582 \tValidation Loss: 912.480801\n","Epoch: 172 \tTraining Loss: 3823.694937 \tValidation Loss: 937.771662\n","Epoch: 173 \tTraining Loss: 3772.325674 \tValidation Loss: 920.159569\n","Epoch: 174 \tTraining Loss: 3845.612172 \tValidation Loss: 913.369449\n","Epoch: 175 \tTraining Loss: 3849.356631 \tValidation Loss: 947.002919\n","Epoch: 176 \tTraining Loss: 3819.344641 \tValidation Loss: 782.258159\n","Epoch: 177 \tTraining Loss: 3808.115069 \tValidation Loss: 922.582336\n","Epoch: 178 \tTraining Loss: 3819.560194 \tValidation Loss: 965.064566\n","Epoch: 179 \tTraining Loss: 3837.922943 \tValidation Loss: 920.221827\n","Epoch: 180 \tTraining Loss: 3362.176390 \tValidation Loss: 912.777815\n","Epoch: 181 \tTraining Loss: 3766.848251 \tValidation Loss: 924.658643\n","Epoch: 182 \tTraining Loss: 3967.251417 \tValidation Loss: 915.362785\n","Epoch: 183 \tTraining Loss: 3880.084442 \tValidation Loss: 940.009726\n","Epoch: 184 \tTraining Loss: 3847.809949 \tValidation Loss: 927.084925\n","Epoch: 185 \tTraining Loss: 3906.048935 \tValidation Loss: 915.692598\n","Epoch: 186 \tTraining Loss: 3843.228275 \tValidation Loss: 928.114793\n","Epoch: 187 \tTraining Loss: 3830.575471 \tValidation Loss: 922.377654\n","Epoch: 188 \tTraining Loss: 3735.196999 \tValidation Loss: 912.658345\n","Epoch: 189 \tTraining Loss: 3831.968903 \tValidation Loss: 924.206181\n","Epoch: 190 \tTraining Loss: 3835.144896 \tValidation Loss: 902.491770\n","Epoch: 191 \tTraining Loss: 3762.894109 \tValidation Loss: 943.978877\n","Epoch: 192 \tTraining Loss: 3832.422348 \tValidation Loss: 911.461522\n","Epoch: 193 \tTraining Loss: 3747.773779 \tValidation Loss: 911.405008\n","Epoch: 194 \tTraining Loss: 3738.890977 \tValidation Loss: 902.295315\n","Epoch: 195 \tTraining Loss: 3844.341465 \tValidation Loss: 946.767855\n","Epoch: 196 \tTraining Loss: 3798.921984 \tValidation Loss: 920.480511\n","Epoch: 197 \tTraining Loss: 3849.063571 \tValidation Loss: 897.145026\n","Epoch: 198 \tTraining Loss: 3800.395512 \tValidation Loss: 912.792409\n","Epoch: 199 \tTraining Loss: 3753.565685 \tValidation Loss: 901.423561\n","Epoch: 200 \tTraining Loss: 3748.681098 \tValidation Loss: 914.606780\n","Epoch: 201 \tTraining Loss: 3844.419911 \tValidation Loss: 817.747479\n","Epoch: 202 \tTraining Loss: 3723.378840 \tValidation Loss: 944.524930\n","Epoch: 203 \tTraining Loss: 3826.671275 \tValidation Loss: 945.589676\n","Epoch: 204 \tTraining Loss: 3803.181716 \tValidation Loss: 967.205486\n","Epoch: 205 \tTraining Loss: 3769.301497 \tValidation Loss: 928.394589\n","Epoch: 206 \tTraining Loss: 3743.715853 \tValidation Loss: 952.945636\n","Epoch: 207 \tTraining Loss: 3797.273146 \tValidation Loss: 909.741581\n","Epoch: 208 \tTraining Loss: 3836.973163 \tValidation Loss: 950.227410\n","Epoch: 209 \tTraining Loss: 3868.776414 \tValidation Loss: 923.699465\n","Epoch: 210 \tTraining Loss: 3824.230854 \tValidation Loss: 924.784710\n","Epoch: 211 \tTraining Loss: 3886.651748 \tValidation Loss: 801.195239\n","Epoch: 212 \tTraining Loss: 3799.177593 \tValidation Loss: 946.494225\n","Epoch: 213 \tTraining Loss: 3755.015146 \tValidation Loss: 919.091041\n","Epoch: 214 \tTraining Loss: 3661.819589 \tValidation Loss: 900.549385\n","Epoch: 215 \tTraining Loss: 3730.251688 \tValidation Loss: 924.899511\n","Epoch: 216 \tTraining Loss: 3704.551580 \tValidation Loss: 924.785896\n","Epoch: 217 \tTraining Loss: 3872.717923 \tValidation Loss: 899.341553\n","Epoch: 218 \tTraining Loss: 3788.624575 \tValidation Loss: 919.218371\n","Epoch: 219 \tTraining Loss: 3653.707480 \tValidation Loss: 936.479872\n","Epoch: 220 \tTraining Loss: 3758.059357 \tValidation Loss: 935.698545\n","Epoch: 221 \tTraining Loss: 3696.128934 \tValidation Loss: 872.213008\n","Epoch: 222 \tTraining Loss: 3762.379577 \tValidation Loss: 910.536020\n","Epoch: 223 \tTraining Loss: 3750.986974 \tValidation Loss: 946.954234\n","Epoch: 224 \tTraining Loss: 3876.110835 \tValidation Loss: 905.507466\n","Epoch: 225 \tTraining Loss: 3781.948660 \tValidation Loss: 909.104217\n","Epoch: 226 \tTraining Loss: 3764.025657 \tValidation Loss: 933.698592\n","Epoch: 227 \tTraining Loss: 3770.007466 \tValidation Loss: 923.582065\n","Epoch: 228 \tTraining Loss: 3778.922956 \tValidation Loss: 930.563756\n","Epoch: 229 \tTraining Loss: 3879.790756 \tValidation Loss: 925.103676\n","Epoch: 230 \tTraining Loss: 3797.248303 \tValidation Loss: 915.051266\n","Epoch: 231 \tTraining Loss: 3815.678995 \tValidation Loss: 955.173301\n","Epoch: 232 \tTraining Loss: 3750.440319 \tValidation Loss: 920.617474\n","Epoch: 233 \tTraining Loss: 3684.074432 \tValidation Loss: 921.784806\n","Epoch: 234 \tTraining Loss: 3788.805537 \tValidation Loss: 902.408601\n","Epoch: 235 \tTraining Loss: 3735.200918 \tValidation Loss: 918.567759\n","Epoch: 236 \tTraining Loss: 3773.427219 \tValidation Loss: 923.013997\n","Epoch: 237 \tTraining Loss: 3889.533553 \tValidation Loss: 948.607115\n","Epoch: 238 \tTraining Loss: 3757.647529 \tValidation Loss: 913.951501\n","Epoch: 239 \tTraining Loss: 3789.239673 \tValidation Loss: 934.209030\n","Epoch: 240 \tTraining Loss: 3706.989941 \tValidation Loss: 931.148379\n","Epoch: 241 \tTraining Loss: 3815.842160 \tValidation Loss: 928.580767\n","Epoch: 242 \tTraining Loss: 3816.048969 \tValidation Loss: 915.863639\n","Epoch: 243 \tTraining Loss: 3820.138243 \tValidation Loss: 905.954802\n","Epoch: 244 \tTraining Loss: 3734.986200 \tValidation Loss: 908.491021\n","Epoch: 245 \tTraining Loss: 3818.606003 \tValidation Loss: 924.686195\n","Epoch: 246 \tTraining Loss: 3680.113107 \tValidation Loss: 922.002814\n","Epoch: 247 \tTraining Loss: 3462.258657 \tValidation Loss: 918.306382\n","Epoch: 248 \tTraining Loss: 3728.460006 \tValidation Loss: 916.841065\n","Epoch: 249 \tTraining Loss: 3679.036001 \tValidation Loss: 892.376744\n","Epoch: 250 \tTraining Loss: 3762.547817 \tValidation Loss: 918.589541\n","Epoch: 251 \tTraining Loss: 3637.022793 \tValidation Loss: 902.661486\n","Epoch: 252 \tTraining Loss: 3861.122695 \tValidation Loss: 729.228480\n","Epoch: 253 \tTraining Loss: 3699.554810 \tValidation Loss: 942.083565\n","Epoch: 254 \tTraining Loss: 3695.628547 \tValidation Loss: 937.522185\n","Epoch: 255 \tTraining Loss: 3790.194696 \tValidation Loss: 904.472672\n","Epoch: 256 \tTraining Loss: 3732.437416 \tValidation Loss: 922.775010\n","Epoch: 257 \tTraining Loss: 3695.565250 \tValidation Loss: 812.663682\n","Epoch: 258 \tTraining Loss: 3761.283512 \tValidation Loss: 921.812306\n","Epoch: 259 \tTraining Loss: 3699.254148 \tValidation Loss: 933.780725\n","Epoch: 260 \tTraining Loss: 3700.274415 \tValidation Loss: 914.012227\n","Epoch: 261 \tTraining Loss: 3680.133822 \tValidation Loss: 925.591527\n","Epoch: 262 \tTraining Loss: 3731.064180 \tValidation Loss: 907.980712\n","Epoch: 263 \tTraining Loss: 3692.006306 \tValidation Loss: 909.872060\n","Epoch: 264 \tTraining Loss: 3639.899494 \tValidation Loss: 940.339260\n","Epoch: 265 \tTraining Loss: 3705.156692 \tValidation Loss: 926.021073\n","Epoch: 266 \tTraining Loss: 3743.393503 \tValidation Loss: 919.245300\n","Epoch: 267 \tTraining Loss: 3625.899208 \tValidation Loss: 922.753665\n","Epoch: 268 \tTraining Loss: 3803.634394 \tValidation Loss: 939.460525\n","Epoch: 269 \tTraining Loss: 3823.560459 \tValidation Loss: 944.228897\n","Epoch: 270 \tTraining Loss: 3830.936848 \tValidation Loss: 827.993083\n","Epoch: 271 \tTraining Loss: 3661.014280 \tValidation Loss: 923.334974\n","Epoch: 272 \tTraining Loss: 3709.638537 \tValidation Loss: 928.008484\n","Epoch: 273 \tTraining Loss: 3696.330117 \tValidation Loss: 915.795919\n","Epoch: 274 \tTraining Loss: 3651.518793 \tValidation Loss: 936.751548\n","Epoch: 275 \tTraining Loss: 3665.296900 \tValidation Loss: 919.178017\n","Epoch: 276 \tTraining Loss: 3721.737938 \tValidation Loss: 918.969718\n","Epoch: 277 \tTraining Loss: 3763.114536 \tValidation Loss: 936.609512\n","Epoch: 278 \tTraining Loss: 3749.182575 \tValidation Loss: 929.988302\n","Epoch: 279 \tTraining Loss: 3705.182624 \tValidation Loss: 882.299340\n","Epoch: 280 \tTraining Loss: 3700.958997 \tValidation Loss: 815.379020\n","Epoch: 281 \tTraining Loss: 3686.878452 \tValidation Loss: 922.983289\n","Epoch: 282 \tTraining Loss: 3676.419016 \tValidation Loss: 909.661958\n","Epoch: 283 \tTraining Loss: 3717.519057 \tValidation Loss: 900.386841\n","Epoch: 284 \tTraining Loss: 3743.263624 \tValidation Loss: 896.064700\n","Epoch: 285 \tTraining Loss: 3724.817435 \tValidation Loss: 896.412044\n","Epoch: 286 \tTraining Loss: 3742.891617 \tValidation Loss: 878.915560\n","Epoch: 287 \tTraining Loss: 3690.676383 \tValidation Loss: 918.817867\n","Epoch: 288 \tTraining Loss: 3726.984467 \tValidation Loss: 904.573050\n","Epoch: 289 \tTraining Loss: 3764.097214 \tValidation Loss: 917.216106\n","Epoch: 290 \tTraining Loss: 3620.335370 \tValidation Loss: 885.505562\n","Epoch: 291 \tTraining Loss: 3319.061270 \tValidation Loss: 924.236584\n","Epoch: 292 \tTraining Loss: 3704.670572 \tValidation Loss: 852.623878\n","Epoch: 293 \tTraining Loss: 3629.589723 \tValidation Loss: 917.505723\n","Epoch: 294 \tTraining Loss: 3683.929185 \tValidation Loss: 914.921870\n","Epoch: 295 \tTraining Loss: 3681.928711 \tValidation Loss: 911.757383\n","Epoch: 296 \tTraining Loss: 3711.304542 \tValidation Loss: 845.687477\n","Epoch: 297 \tTraining Loss: 3715.595707 \tValidation Loss: 720.178986\n","Epoch: 298 \tTraining Loss: 3696.431609 \tValidation Loss: 913.321040\n","Epoch: 299 \tTraining Loss: 3719.108494 \tValidation Loss: 911.335388\n","Epoch: 300 \tTraining Loss: 3722.749558 \tValidation Loss: 920.178476\n","Epoch: 301 \tTraining Loss: 3700.509603 \tValidation Loss: 907.460433\n","Epoch: 302 \tTraining Loss: 3635.004743 \tValidation Loss: 914.119037\n","Epoch: 303 \tTraining Loss: 3781.916682 \tValidation Loss: 887.992085\n","Epoch: 304 \tTraining Loss: 3723.388119 \tValidation Loss: 902.640530\n","Epoch: 305 \tTraining Loss: 3571.313864 \tValidation Loss: 904.305974\n","Epoch: 306 \tTraining Loss: 3720.946837 \tValidation Loss: 897.840889\n","Epoch: 307 \tTraining Loss: 3620.203721 \tValidation Loss: 909.825163\n","Epoch: 308 \tTraining Loss: 3695.958697 \tValidation Loss: 889.926603\n","Epoch: 309 \tTraining Loss: 3643.468317 \tValidation Loss: 923.606199\n","Epoch: 310 \tTraining Loss: 3624.363649 \tValidation Loss: 834.705686\n","Epoch: 311 \tTraining Loss: 3644.678652 \tValidation Loss: 908.341729\n","Epoch: 312 \tTraining Loss: 3695.859920 \tValidation Loss: 910.413485\n","Epoch: 313 \tTraining Loss: 3686.892223 \tValidation Loss: 908.533307\n","Epoch: 314 \tTraining Loss: 3738.948034 \tValidation Loss: 914.821337\n","Epoch: 315 \tTraining Loss: 3689.882222 \tValidation Loss: 826.285792\n","Epoch: 316 \tTraining Loss: 3603.098903 \tValidation Loss: 865.772555\n","Epoch: 317 \tTraining Loss: 3598.400232 \tValidation Loss: 875.939756\n","Epoch: 318 \tTraining Loss: 3645.571340 \tValidation Loss: 895.992620\n","Epoch: 319 \tTraining Loss: 3538.561468 \tValidation Loss: 878.592392\n","Epoch: 320 \tTraining Loss: 3653.619101 \tValidation Loss: 815.363574\n","Epoch: 321 \tTraining Loss: 3715.472194 \tValidation Loss: 904.582350\n","Epoch: 322 \tTraining Loss: 3662.085245 \tValidation Loss: 907.526916\n","Epoch: 323 \tTraining Loss: 3603.759044 \tValidation Loss: 896.470000\n","Epoch: 324 \tTraining Loss: 3634.384235 \tValidation Loss: 918.491019\n","Epoch: 325 \tTraining Loss: 3623.975691 \tValidation Loss: 912.525388\n","Epoch: 326 \tTraining Loss: 3638.333999 \tValidation Loss: 887.723099\n","Epoch: 327 \tTraining Loss: 3655.164717 \tValidation Loss: 899.188293\n","Epoch: 328 \tTraining Loss: 3633.419444 \tValidation Loss: 907.135501\n","Epoch: 329 \tTraining Loss: 3613.469670 \tValidation Loss: 914.032909\n","Epoch: 330 \tTraining Loss: 3671.764994 \tValidation Loss: 886.130451\n","Epoch: 331 \tTraining Loss: 3596.235153 \tValidation Loss: 924.061351\n","Epoch: 332 \tTraining Loss: 3231.448865 \tValidation Loss: 907.778500\n","Epoch: 333 \tTraining Loss: 3618.138761 \tValidation Loss: 906.401760\n","Epoch: 334 \tTraining Loss: 3637.511331 \tValidation Loss: 920.759065\n","Epoch: 335 \tTraining Loss: 3708.677004 \tValidation Loss: 897.351336\n","Epoch: 336 \tTraining Loss: 3526.368447 \tValidation Loss: 888.777114\n","Epoch: 337 \tTraining Loss: 3695.375858 \tValidation Loss: 913.669659\n","Epoch: 338 \tTraining Loss: 3619.215191 \tValidation Loss: 808.767154\n","Epoch: 339 \tTraining Loss: 3620.029985 \tValidation Loss: 893.013295\n","Epoch: 340 \tTraining Loss: 3658.735051 \tValidation Loss: 807.569277\n","Epoch: 341 \tTraining Loss: 3617.560179 \tValidation Loss: 877.467420\n","Epoch: 342 \tTraining Loss: 3665.527699 \tValidation Loss: 870.433130\n","Epoch: 343 \tTraining Loss: 3715.464195 \tValidation Loss: 809.103321\n","Epoch: 344 \tTraining Loss: 3602.539033 \tValidation Loss: 768.286710\n","Epoch: 345 \tTraining Loss: 3603.557882 \tValidation Loss: 914.527720\n","Epoch: 346 \tTraining Loss: 3697.382652 \tValidation Loss: 888.470800\n","Epoch: 347 \tTraining Loss: 3499.850502 \tValidation Loss: 851.982441\n","Epoch: 348 \tTraining Loss: 3415.281069 \tValidation Loss: 878.602393\n","Epoch: 349 \tTraining Loss: 3705.555717 \tValidation Loss: 831.990592\n","Epoch: 350 \tTraining Loss: 3739.105594 \tValidation Loss: 895.131010\n","Epoch: 351 \tTraining Loss: 3743.740381 \tValidation Loss: 902.849679\n","Epoch: 352 \tTraining Loss: 3667.187548 \tValidation Loss: 906.121432\n","Epoch: 353 \tTraining Loss: 3664.419269 \tValidation Loss: 869.856925\n","Epoch: 354 \tTraining Loss: 3649.063638 \tValidation Loss: 893.097467\n","Epoch: 355 \tTraining Loss: 3676.212546 \tValidation Loss: 902.600960\n","Epoch: 356 \tTraining Loss: 3564.971145 \tValidation Loss: 908.073553\n","Epoch: 357 \tTraining Loss: 3663.679838 \tValidation Loss: 894.257983\n","Epoch: 358 \tTraining Loss: 3597.727959 \tValidation Loss: 909.350221\n","Epoch: 359 \tTraining Loss: 3572.416625 \tValidation Loss: 725.021012\n","Epoch: 360 \tTraining Loss: 3637.247662 \tValidation Loss: 898.810764\n","Epoch: 361 \tTraining Loss: 3638.638407 \tValidation Loss: 908.563472\n","Epoch: 362 \tTraining Loss: 3727.776871 \tValidation Loss: 915.457281\n","Epoch: 363 \tTraining Loss: 3630.815946 \tValidation Loss: 913.432116\n","Epoch: 364 \tTraining Loss: 3701.397388 \tValidation Loss: 853.950177\n","Epoch: 365 \tTraining Loss: 3637.707670 \tValidation Loss: 873.731833\n","Epoch: 366 \tTraining Loss: 3685.653671 \tValidation Loss: 919.199127\n","Epoch: 367 \tTraining Loss: 3618.260614 \tValidation Loss: 916.626279\n","Epoch: 368 \tTraining Loss: 3614.663822 \tValidation Loss: 919.913056\n","Epoch: 369 \tTraining Loss: 3593.233540 \tValidation Loss: 781.144918\n","Epoch: 370 \tTraining Loss: 3559.338765 \tValidation Loss: 917.752149\n","Epoch: 371 \tTraining Loss: 3587.870673 \tValidation Loss: 892.425512\n","Epoch: 372 \tTraining Loss: 3570.636379 \tValidation Loss: 897.147509\n","Epoch: 373 \tTraining Loss: 3632.019557 \tValidation Loss: 912.851209\n","Epoch: 374 \tTraining Loss: 3617.449277 \tValidation Loss: 907.092779\n","Epoch: 375 \tTraining Loss: 3635.448285 \tValidation Loss: 910.513249\n","Epoch: 376 \tTraining Loss: 3615.824289 \tValidation Loss: 905.232423\n","Epoch: 377 \tTraining Loss: 3663.072034 \tValidation Loss: 911.484791\n","Epoch: 378 \tTraining Loss: 3473.488199 \tValidation Loss: 907.817486\n","Epoch: 379 \tTraining Loss: 3553.029615 \tValidation Loss: 912.962773\n","Epoch: 380 \tTraining Loss: 3508.819844 \tValidation Loss: 703.749281\n","Epoch: 381 \tTraining Loss: 3533.888509 \tValidation Loss: 924.317793\n","Epoch: 382 \tTraining Loss: 3629.560503 \tValidation Loss: 924.718387\n","Epoch: 383 \tTraining Loss: 3695.077498 \tValidation Loss: 925.001631\n","Epoch: 384 \tTraining Loss: 3575.649250 \tValidation Loss: 714.510530\n","Epoch: 385 \tTraining Loss: 3566.378588 \tValidation Loss: 916.527340\n","Epoch: 386 \tTraining Loss: 3533.722657 \tValidation Loss: 933.183346\n","Validation loss decreased (686.134851 --> 642.829318).  Saving model ...\n","Epoch: 387 \tTraining Loss: 3573.038820 \tValidation Loss: 642.829318\n","Epoch: 388 \tTraining Loss: 3692.292549 \tValidation Loss: 933.492989\n","Epoch: 389 \tTraining Loss: 3697.063977 \tValidation Loss: 923.881933\n","Epoch: 390 \tTraining Loss: 3640.119638 \tValidation Loss: 922.359502\n","Epoch: 391 \tTraining Loss: 3634.812868 \tValidation Loss: 854.638008\n","Epoch: 392 \tTraining Loss: 3721.460966 \tValidation Loss: 927.238286\n","Epoch: 393 \tTraining Loss: 3588.703511 \tValidation Loss: 928.550975\n","Epoch: 394 \tTraining Loss: 3633.251668 \tValidation Loss: 923.112673\n","Epoch: 395 \tTraining Loss: 3578.653287 \tValidation Loss: 892.294006\n","Epoch: 396 \tTraining Loss: 3476.178192 \tValidation Loss: 907.180144\n","Epoch: 397 \tTraining Loss: 3612.604861 \tValidation Loss: 907.409999\n","Epoch: 398 \tTraining Loss: 3567.475261 \tValidation Loss: 906.384060\n","Epoch: 399 \tTraining Loss: 3607.108436 \tValidation Loss: 894.684867\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OqtVdqkZP21K","colab_type":"code","outputId":"f79edf13-0848-4e19-9654-6b472476c90e","executionInfo":{"status":"ok","timestamp":1586929265664,"user_tz":300,"elapsed":809,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["net.load_state_dict(torch.load('mixed_model.pt'))"],"execution_count":440,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":440}]},{"cell_type":"code","metadata":{"id":"iRCN4REPP93Y","colab_type":"code","outputId":"f5aa18ba-328b-40d6-d252-4df6ee482872","executionInfo":{"status":"ok","timestamp":1586929260594,"user_tz":300,"elapsed":1369,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":199}},"source":["from sklearn.metrics import mean_absolute_error\n","\n","# track test loss\n","test_loss = 0.0\n","mean_abs = 0.0\n","\n","net.eval()\n","# iterate over test data\n","#bin_op.binarization()\n","test_h = net.init_hidden(batch_size)\n","\n","for batch_idx, (X_temp, X_num, X_cat, labels) in enumerate(test_loader):\n","    if(train_on_gpu):\n","        X_temp, X_num, X_cat, labels = X_temp.cuda(), X_num.cuda(), X_cat.type(torch.LongTensor).cuda(), labels.cuda()\n","    \n","    test_h = tuple([each.data for each in test_h])\n","\n","    # get the output from the model\n","    output, h = net(X_temp, h, X_num, X_cat)\n","    # / (weight_lstm + weight_num + weight_cat)\n","\n","    # calculate the loss and perform backprop\n","    loss = criterion(output.squeeze(), labels.double())\n","\n","    out_np = output.detach().squeeze().cpu().numpy().astype(int).T\n","    target_np = labels.detach().cpu().numpy().astype(int).T\n","\n","    print([(out, tar) for out, tar in zip(out_np, target_np)])\n","\n","    mean_abs += mean_absolute_error(out_np, target_np) * batch_size\n","        \n","    test_loss += loss.item() * batch_size\n","\n","# calculate average losses\n","test_loss = np.sqrt(test_loss / len(test_loader.dataset))\n","mean_abs /= len(test_loader.dataset)\n","print('Test Loss: {:.6f}\\n'.format(test_loss))\n","print('Mean Abs Loss: {:.6f}\\n'.format(mean_abs))"],"execution_count":439,"outputs":[{"output_type":"stream","text":["[(4, 3), (4, 3), (4, 2), (7, 2), (4, 2), (4, 3), (8, 3), (3, 3), (3, 2), (3, 6), (3, 4), (3, 4), (3, 5), (3, 3), (3, 2), (3, 16)]\n","[(3, 4), (3, 10), (3, 5), (4, 3), (4, 8), (8, 2), (6, 9), (4, 66), (6, 3), (7, 9), (7, 11), (7, 8), (5, 25), (5, 4), (6, 3), (5, 2)]\n","[(10, 3), (5, 3), (6, 6), (5, 3), (6, 8), (6, 8), (5, 6), (5, 9), (6, 12), (6, 18), (5, 24), (5, 19), (8, 3), (5, 15), (7, 12), (7, 12)]\n","[(5, 17), (5, 9), (7, 3), (5, 12), (6, 33), (6, 11), (5, 6), (6, 2), (5, 7), (5, 5), (5, 2), (8, 11), (6, 23), (6, 4), (5, 7), (7, 5)]\n","[(5, 6), (7, 10), (7, 12), (6, 5), (7, 10), (4, 2), (8, 3), (7, 32), (7, 13), (4, 27), (8, 9), (8, 12), (5, 16), (8, 6), (4, 3), (7, 7)]\n","[(4, 2), (4, 6), (8, 10), (7, 6), (7, 2), (4, 3), (5, 5), (8, 2), (8, 5), (5, 17), (7, 18), (6, 3), (5, 8), (7, 2), (6, 15), (5, 5)]\n","Test Loss: 8.977377\n","\n","Mean Abs Loss: 4.866667\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5GSpw9ExoD6B","colab_type":"text"},"source":["## Deluxe Mixed Model"]},{"cell_type":"code","metadata":{"id":"98-Fe0zNWv-v","colab_type":"code","colab":{}},"source":["def train(model, criterion, optimizer, epochs):\n","    # training params\n","\n","    #epochs = 00 # 3-4 is approx where I noticed the validation loss stop decreasing\n","\n","    counter = 0\n","    print_every = 10\n","    clip = 8 # gradient clipping\n","\n","    # move model to GPU, if available\n","    if(train_on_gpu):\n","        model.cuda()\n","\n","    model.train()\n","    # train for some number of epochs\n","    valid_loss_min = np.Inf\n","    train_loss_min = np.Inf\n","\n","    best_train_epoch = 0\n","    best_valid_epoch = 0\n","\n","    for epoch in range(1, epochs + 1):\n","        # initialize hidden state\n","        h = model.init_hidden(batch_size)\n","\n","        train_loss = 0.0\n","        valid_loss = 0.0\n","\n","        model.train()\n","\n","        # batch loop\n","        for X_temp, X_num, X_cat, labels in train_loader:\n","            counter += 1\n","\n","            if(train_on_gpu):\n","                X_temp, X_num, X_cat, labels = X_temp.cuda(), X_num.cuda(), X_cat.type(torch.LongTensor).cuda(), labels.cuda()\n","\n","            # Creating new variables for the hidden state, otherwise\n","            # we'd backprop through the entire training history\n","            h = tuple([each.data for each in h])\n","            # zero accumulated gradients\n","            model.zero_grad()\n","\n","            # get the output from the model\n","            output, h = model(X_temp, h, X_num, X_cat)\n","\n","            # calculate the loss and perform backprop\n","            loss = criterion(output.squeeze(), labels.double())\n","            loss.backward()\n","            train_loss += loss.item()\n","            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","            nn.utils.clip_grad_norm_(model.parameters(), clip)\n","            optimizer.step()\n","\n","        if train_loss <= train_loss_min:\n","            torch.save(model.state_dict(), 'train_mixed_model1.pt')\n","            train_loss_min = train_loss\n","            best_train_epoch = epoch\n","\n","        model.eval()\n","\n","        val_h = model.init_hidden(batch_size)\n","\n","        for X_temp, X_num, X_cat, labels in valid_loader:\n","            \n","            \n","            if(train_on_gpu):\n","                X_temp, X_num, X_cat, labels = X_temp.cuda(), X_num.cuda(), X_cat.type(torch.LongTensor).cuda(), labels.cuda()\n","\n","            # Creating new variables for the hidden state, otherwise\n","            # we'd backprop through the entire training history\n","            val_h = tuple([each.data for each in val_h])\n","\n","            # get the output from the model\n","            output, h = model(X_temp, h, X_num, X_cat)\n","\n","            # calculate the loss and perform backprop\n","            loss = criterion(output.squeeze(), labels.double())\n","            valid_loss += loss.item()\n","\n","        # save model if validation loss has decreased\n","        if valid_loss <= valid_loss_min:\n","            torch.save(model.state_dict(), 'valid_mixed_model1.pt')\n","            valid_loss_min = valid_loss\n","            best_valid_epoch = epoch\n","            \n","        if epoch % 50 == 0:\n","            print('  Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n","                epoch, train_loss, valid_loss))\n","            \n","    print(\"Best Validation Loss: {:.6f} in epoch {}\".format(\n","        valid_loss_min, str(best_valid_epoch)\n","    ))\n","\n","    print(\"Best Train Loss: {:.6f} in epoch {}\".format(\n","        train_loss_min, str(best_train_epoch)\n","    ))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CNlwIrKlXixG","colab_type":"code","colab":{}},"source":["def test(model, criterion):\n","    # track test loss\n","    test_loss = 0.0\n","    mean_abs = 0.0\n","\n","    model.eval()\n","    # iterate over test data\n","    #bin_op.binarization()\n","    test_h = model.init_hidden(batch_size)\n","\n","    for batch_idx, (X_temp, X_num, X_cat, labels) in enumerate(test_loader):\n","        if(train_on_gpu):\n","            X_temp, X_num, X_cat, labels = X_temp.cuda(), X_num.cuda(), X_cat.type(torch.LongTensor).cuda(), labels.cuda()\n","        \n","        test_h = tuple([each.data for each in test_h])\n","\n","        # get the output from the model\n","        output, test_h = model(X_temp, test_h, X_num, X_cat)\n","        # / (weight_lstm + weight_num + weight_cat)\n","\n","        # calculate the loss and perform backprop\n","        loss = criterion(output.squeeze(), labels.double())\n","\n","        out_np = output.detach().squeeze().cpu().numpy().astype(int).T\n","        target_np = labels.detach().cpu().numpy().astype(int).T\n","\n","        #print([(out, tar) for out, tar in zip(out_np, target_np)])\n","\n","        mean_abs += mean_absolute_error(out_np, target_np) * batch_size\n","            \n","        test_loss += loss.item() * batch_size\n","\n","    # calculate average losses\n","    test_loss = np.sqrt(test_loss / len(test_loader.dataset))\n","    mean_abs /= len(test_loader.dataset)\n","    print('  Test Loss: {:.6f}\\n'.format(test_loss))\n","    print('  Mean Abs Loss: {:.6f}\\n'.format(mean_abs))\n","\n","    return mean_abs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"48KzkMyMXrt7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"1fc14ece-63eb-4679-e59c-986e8b267915","executionInfo":{"status":"ok","timestamp":1587024491905,"user_tz":300,"elapsed":13003630,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}}},"source":["learning_rates = [0.001, 0.0001]\n","a_hs = [2, 3, 4]\n","num_hidden_dims = [10, 15, 20]\n","cat_hidden_dims = [10, 15, 20]\n","lstm_outputs = [5, 10, 15]\n","combined_dims = [20]\n","\n","criterion = nn.MSELoss()\n","lstm_n_layers = 2\n","\n","#overall_test_square_error = np.Inf\n","overall_test_mean_error = np.Inf\n","\n","\n","\n","for combined_dim in combined_dims:\n","    for lstm_output in lstm_outputs:\n","        for cat_hidden_dim in cat_hidden_dims:\n","            for num_hidden_dim in num_hidden_dims:\n","                for a_h in a_hs:\n","                    for lr in learning_rates:   \n","                        lstm_hidden_dim = np.int(len(train_idx) / (a_h * len(numerical_columns) + 1))\n","\n","                        net = MosquitoMixed(\n","                            len(numerical_columns), lstm_hidden_dim, lstm_n_layers, lstm_output,\n","                            len(non_num_columns), num_hidden_dim,\n","                            embedding_sizes, cat_hidden_dim,\n","                            combined_dim\n","                        ).double()\n","\n","                        \n","                        epochs = 300\n","\n","                        optimizer = torch.optim.RMSprop(net.parameters(), lr=lr)\n","\n","                        print('\\nCombined Dimension: ' + str(combined_dim))\n","                        print('LSTM Output Dimension: ' + str(lstm_output))\n","                        print('Categorical Dimension: ' + str(cat_hidden_dim))\n","                        print('Numerical Dimension: ' + str(num_hidden_dim))\n","                        print('LSTM Dimension: ' + str(lstm_hidden_dim))\n","                        print('Learning Rate: ' + str(lr))\n","                        \n","                        train(net, criterion, optimizer, epochs)\n","\n","                        net.load_state_dict(torch.load('train_mixed_model1.pt'))\n","\n","                        otme = test(net, criterion)\n","                        \n","\n","                        if otme <= overall_test_mean_error:\n","                            print(' Test loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","                            overall_test_mean_error,\n","                            otme))\n","                            torch.save(net.state_dict(), 'best_mixed_model1.pt')\n","                            overall_test_mean_error = otme\n","\n","                        net.load_state_dict(torch.load('valid_mixed_model1.pt'))\n","\n","                        otme = test(net, criterion)\n","\n","                        if otme <= overall_test_mean_error:\n","                            print(' Test loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","                            overall_test_mean_error,\n","                            otme))\n","                            torch.save(net.state_dict(), 'best_mixed_model1.pt')\n","                            overall_test_mean_error = otme\n","\n","                        \n","\n","\n"],"execution_count":30,"outputs":[{"output_type":"stream","text":["\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3874.219294 \tValidation Loss: 1023.653081\n","  Epoch: 100 \tTraining Loss: 3813.060314 \tValidation Loss: 989.372472\n","  Epoch: 150 \tTraining Loss: 3597.248018 \tValidation Loss: 997.176782\n","  Epoch: 200 \tTraining Loss: 3522.815472 \tValidation Loss: 1000.179383\n","  Epoch: 250 \tTraining Loss: 3189.837519 \tValidation Loss: 984.375305\n","  Epoch: 300 \tTraining Loss: 3533.913345 \tValidation Loss: 974.986786\n","Best Validation Loss: 679.550250 in epoch 298\n","Best Train Loss: 2740.237026 in epoch 246\n","  Test Loss: 8.821063\n","\n","  Mean Abs Loss: 4.933333\n","\n"," Test loss decreased (inf --> 4.933333).  Saving model ...\n","  Test Loss: 8.749639\n","\n","  Mean Abs Loss: 4.714286\n","\n"," Test loss decreased (4.933333 --> 4.714286).  Saving model ...\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4874.424973 \tValidation Loss: 1191.909614\n","  Epoch: 100 \tTraining Loss: 4452.905586 \tValidation Loss: 1136.567791\n","  Epoch: 150 \tTraining Loss: 4242.435877 \tValidation Loss: 1073.927763\n","  Epoch: 200 \tTraining Loss: 4038.821335 \tValidation Loss: 1048.049867\n","  Epoch: 250 \tTraining Loss: 3963.688715 \tValidation Loss: 1023.307062\n","  Epoch: 300 \tTraining Loss: 3909.835566 \tValidation Loss: 1073.261132\n","Best Validation Loss: 710.754217 in epoch 247\n","Best Train Loss: 3182.712586 in epoch 192\n","  Test Loss: 9.551339\n","\n","  Mean Abs Loss: 5.361905\n","\n","  Test Loss: 9.007087\n","\n","  Mean Abs Loss: 5.085714\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3805.248414 \tValidation Loss: 999.845542\n","  Epoch: 100 \tTraining Loss: 3786.074018 \tValidation Loss: 995.905199\n","  Epoch: 150 \tTraining Loss: 3664.381289 \tValidation Loss: 1010.085702\n","  Epoch: 200 \tTraining Loss: 3236.870589 \tValidation Loss: 996.073980\n","  Epoch: 250 \tTraining Loss: 3556.032323 \tValidation Loss: 1026.873519\n","  Epoch: 300 \tTraining Loss: 3577.084225 \tValidation Loss: 903.097842\n","Best Validation Loss: 624.316012 in epoch 130\n","Best Train Loss: 3229.055159 in epoch 203\n","  Test Loss: 8.825146\n","\n","  Mean Abs Loss: 4.752381\n","\n","  Test Loss: 8.889882\n","\n","  Mean Abs Loss: 4.885714\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4807.226530 \tValidation Loss: 1307.102180\n","  Epoch: 100 \tTraining Loss: 4400.290619 \tValidation Loss: 1184.137775\n","  Epoch: 150 \tTraining Loss: 4154.817337 \tValidation Loss: 1081.398331\n","  Epoch: 200 \tTraining Loss: 4001.404847 \tValidation Loss: 978.533957\n","  Epoch: 250 \tTraining Loss: 3887.418484 \tValidation Loss: 1025.810914\n","  Epoch: 300 \tTraining Loss: 3889.082446 \tValidation Loss: 1022.000305\n","Best Validation Loss: 607.304106 in epoch 295\n","Best Train Loss: 3012.116238 in epoch 285\n","  Test Loss: 9.006655\n","\n","  Mean Abs Loss: 5.009524\n","\n","  Test Loss: 8.900740\n","\n","  Mean Abs Loss: 4.904762\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3888.273847 \tValidation Loss: 1033.608680\n","  Epoch: 100 \tTraining Loss: 3693.459090 \tValidation Loss: 1038.420478\n","  Epoch: 150 \tTraining Loss: 3650.451012 \tValidation Loss: 951.763392\n","  Epoch: 200 \tTraining Loss: 3561.953540 \tValidation Loss: 995.459574\n","  Epoch: 250 \tTraining Loss: 3560.953164 \tValidation Loss: 994.131656\n","  Epoch: 300 \tTraining Loss: 3602.044117 \tValidation Loss: 1003.918786\n","Best Validation Loss: 641.902400 in epoch 181\n","Best Train Loss: 2802.830387 in epoch 246\n","  Test Loss: 8.827622\n","\n","  Mean Abs Loss: 4.704762\n","\n"," Test loss decreased (4.714286 --> 4.704762).  Saving model ...\n","  Test Loss: 8.744062\n","\n","  Mean Abs Loss: 4.761905\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4941.830489 \tValidation Loss: 1285.439460\n","  Epoch: 100 \tTraining Loss: 4268.587390 \tValidation Loss: 1095.322065\n","  Epoch: 150 \tTraining Loss: 4114.694599 \tValidation Loss: 869.351097\n","  Epoch: 200 \tTraining Loss: 4099.741965 \tValidation Loss: 1023.849561\n","  Epoch: 250 \tTraining Loss: 3892.386115 \tValidation Loss: 984.968344\n","  Epoch: 300 \tTraining Loss: 3963.976144 \tValidation Loss: 1015.693051\n","Best Validation Loss: 741.576478 in epoch 280\n","Best Train Loss: 3111.400452 in epoch 228\n","  Test Loss: 9.265054\n","\n","  Mean Abs Loss: 5.180952\n","\n","  Test Loss: 8.953127\n","\n","  Mean Abs Loss: 4.847619\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3886.913432 \tValidation Loss: 1010.686048\n","  Epoch: 100 \tTraining Loss: 3609.411919 \tValidation Loss: 1014.858069\n","  Epoch: 150 \tTraining Loss: 3633.895760 \tValidation Loss: 1034.386932\n","  Epoch: 200 \tTraining Loss: 3580.319526 \tValidation Loss: 964.349789\n","  Epoch: 250 \tTraining Loss: 3572.376932 \tValidation Loss: 968.392700\n","  Epoch: 300 \tTraining Loss: 3518.315013 \tValidation Loss: 951.826351\n","Best Validation Loss: 769.060519 in epoch 160\n","Best Train Loss: 2716.942757 in epoch 226\n","  Test Loss: 8.700427\n","\n","  Mean Abs Loss: 4.971429\n","\n","  Test Loss: 8.823856\n","\n","  Mean Abs Loss: 4.819048\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4857.805637 \tValidation Loss: 1335.240462\n","  Epoch: 100 \tTraining Loss: 4369.313681 \tValidation Loss: 1178.700942\n","  Epoch: 150 \tTraining Loss: 4256.499715 \tValidation Loss: 1158.729663\n","  Epoch: 200 \tTraining Loss: 4102.225611 \tValidation Loss: 1043.414800\n","  Epoch: 250 \tTraining Loss: 4065.243741 \tValidation Loss: 1040.846641\n","  Epoch: 300 \tTraining Loss: 3916.371791 \tValidation Loss: 1056.656731\n","Best Validation Loss: 744.903518 in epoch 299\n","Best Train Loss: 3070.527276 in epoch 287\n","  Test Loss: 9.268378\n","\n","  Mean Abs Loss: 5.161905\n","\n","  Test Loss: 9.124766\n","\n","  Mean Abs Loss: 5.038095\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3893.420649 \tValidation Loss: 1025.589008\n","  Epoch: 100 \tTraining Loss: 3384.598555 \tValidation Loss: 902.457022\n","  Epoch: 150 \tTraining Loss: 3559.360002 \tValidation Loss: 972.915066\n","  Epoch: 200 \tTraining Loss: 3636.636048 \tValidation Loss: 821.176426\n","  Epoch: 250 \tTraining Loss: 3554.059800 \tValidation Loss: 993.944622\n","  Epoch: 300 \tTraining Loss: 3422.766641 \tValidation Loss: 974.785827\n","Best Validation Loss: 603.925433 in epoch 146\n","Best Train Loss: 2929.595407 in epoch 101\n","  Test Loss: 8.836193\n","\n","  Mean Abs Loss: 4.704762\n","\n"," Test loss decreased (4.704762 --> 4.704762).  Saving model ...\n","  Test Loss: 8.776646\n","\n","  Mean Abs Loss: 4.685714\n","\n"," Test loss decreased (4.704762 --> 4.685714).  Saving model ...\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4877.638501 \tValidation Loss: 1170.860855\n","  Epoch: 100 \tTraining Loss: 4312.186057 \tValidation Loss: 1177.997565\n","  Epoch: 150 \tTraining Loss: 4083.563776 \tValidation Loss: 1106.550822\n","  Epoch: 200 \tTraining Loss: 3963.105022 \tValidation Loss: 1087.602081\n","  Epoch: 250 \tTraining Loss: 3933.463158 \tValidation Loss: 1087.515174\n","  Epoch: 300 \tTraining Loss: 3962.294661 \tValidation Loss: 1084.142884\n","Best Validation Loss: 724.847325 in epoch 87\n","Best Train Loss: 3138.734655 in epoch 190\n","  Test Loss: 8.941864\n","\n","  Mean Abs Loss: 5.076190\n","\n","  Test Loss: 9.991625\n","\n","  Mean Abs Loss: 6.076190\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3964.209431 \tValidation Loss: 1028.839706\n","  Epoch: 100 \tTraining Loss: 3778.349791 \tValidation Loss: 1006.077110\n","  Epoch: 150 \tTraining Loss: 3753.087317 \tValidation Loss: 998.135874\n","  Epoch: 200 \tTraining Loss: 3660.827676 \tValidation Loss: 987.054607\n","  Epoch: 250 \tTraining Loss: 3607.379065 \tValidation Loss: 986.727586\n","  Epoch: 300 \tTraining Loss: 3551.142872 \tValidation Loss: 963.488873\n","Best Validation Loss: 627.421529 in epoch 229\n","Best Train Loss: 2811.018606 in epoch 181\n","  Test Loss: 8.603766\n","\n","  Mean Abs Loss: 4.657143\n","\n"," Test loss decreased (4.685714 --> 4.657143).  Saving model ...\n","  Test Loss: 8.475176\n","\n","  Mean Abs Loss: 4.580952\n","\n"," Test loss decreased (4.657143 --> 4.580952).  Saving model ...\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 5241.924847 \tValidation Loss: 1547.935620\n","  Epoch: 100 \tTraining Loss: 4430.343268 \tValidation Loss: 1356.587871\n","  Epoch: 150 \tTraining Loss: 4176.840951 \tValidation Loss: 1142.327434\n","  Epoch: 200 \tTraining Loss: 4046.233618 \tValidation Loss: 1118.063044\n","  Epoch: 250 \tTraining Loss: 3844.030604 \tValidation Loss: 910.457713\n","  Epoch: 300 \tTraining Loss: 3821.345759 \tValidation Loss: 1029.970335\n","Best Validation Loss: 808.255178 in epoch 290\n","Best Train Loss: 2980.808161 in epoch 296\n","  Test Loss: 9.129251\n","\n","  Mean Abs Loss: 5.085714\n","\n","  Test Loss: 9.042592\n","\n","  Mean Abs Loss: 5.019048\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3025.910394 \tValidation Loss: 985.198300\n","  Epoch: 100 \tTraining Loss: 3815.106451 \tValidation Loss: 992.874988\n","  Epoch: 150 \tTraining Loss: 3638.812409 \tValidation Loss: 1031.845804\n","  Epoch: 200 \tTraining Loss: 3565.803411 \tValidation Loss: 1003.606025\n","  Epoch: 250 \tTraining Loss: 3490.607777 \tValidation Loss: 1023.397728\n","  Epoch: 300 \tTraining Loss: 3321.666807 \tValidation Loss: 1023.786063\n","Best Validation Loss: 610.892862 in epoch 107\n","Best Train Loss: 3025.910394 in epoch 50\n","  Test Loss: 8.788188\n","\n","  Mean Abs Loss: 4.733333\n","\n","  Test Loss: 8.833757\n","\n","  Mean Abs Loss: 4.895238\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4822.947070 \tValidation Loss: 1333.875197\n","  Epoch: 100 \tTraining Loss: 4372.594962 \tValidation Loss: 1140.586897\n","  Epoch: 150 \tTraining Loss: 4129.139657 \tValidation Loss: 889.832771\n","  Epoch: 200 \tTraining Loss: 3989.617394 \tValidation Loss: 1064.809784\n","  Epoch: 250 \tTraining Loss: 3915.923310 \tValidation Loss: 1065.263706\n","  Epoch: 300 \tTraining Loss: 3817.612105 \tValidation Loss: 990.108597\n","Best Validation Loss: 675.005780 in epoch 217\n","Best Train Loss: 3329.092408 in epoch 135\n","  Test Loss: 8.815831\n","\n","  Mean Abs Loss: 4.828571\n","\n","  Test Loss: 8.727457\n","\n","  Mean Abs Loss: 4.809524\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3798.595830 \tValidation Loss: 967.754530\n","  Epoch: 100 \tTraining Loss: 3822.834323 \tValidation Loss: 991.011592\n","  Epoch: 150 \tTraining Loss: 3652.905670 \tValidation Loss: 975.387304\n","  Epoch: 200 \tTraining Loss: 3693.921508 \tValidation Loss: 983.150123\n","  Epoch: 250 \tTraining Loss: 3566.972208 \tValidation Loss: 975.585340\n","  Epoch: 300 \tTraining Loss: 3530.691872 \tValidation Loss: 910.050005\n","Best Validation Loss: 549.256689 in epoch 245\n","Best Train Loss: 2686.999918 in epoch 258\n","  Test Loss: 8.911495\n","\n","  Mean Abs Loss: 4.809524\n","\n","  Test Loss: 8.812118\n","\n","  Mean Abs Loss: 4.771429\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 5046.339481 \tValidation Loss: 1370.786027\n","  Epoch: 100 \tTraining Loss: 4471.460559 \tValidation Loss: 1229.768779\n","  Epoch: 150 \tTraining Loss: 4074.716798 \tValidation Loss: 1103.505867\n","  Epoch: 200 \tTraining Loss: 3952.004267 \tValidation Loss: 888.581598\n","  Epoch: 250 \tTraining Loss: 3927.000407 \tValidation Loss: 1075.391125\n","  Epoch: 300 \tTraining Loss: 3880.274668 \tValidation Loss: 1061.821312\n","Best Validation Loss: 656.776347 in epoch 133\n","Best Train Loss: 3018.669387 in epoch 289\n","  Test Loss: 8.795896\n","\n","  Mean Abs Loss: 4.838095\n","\n","  Test Loss: 9.108946\n","\n","  Mean Abs Loss: 4.914286\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3803.883505 \tValidation Loss: 1014.878972\n","  Epoch: 100 \tTraining Loss: 3763.557801 \tValidation Loss: 1014.976163\n","  Epoch: 150 \tTraining Loss: 3606.504531 \tValidation Loss: 903.456019\n","  Epoch: 200 \tTraining Loss: 3540.393515 \tValidation Loss: 868.780498\n","  Epoch: 250 \tTraining Loss: 3567.176650 \tValidation Loss: 914.976462\n","  Epoch: 300 \tTraining Loss: 3490.820895 \tValidation Loss: 967.648911\n","Best Validation Loss: 708.940692 in epoch 176\n","Best Train Loss: 3000.963933 in epoch 64\n","  Test Loss: 8.876163\n","\n","  Mean Abs Loss: 4.828571\n","\n","  Test Loss: 8.877886\n","\n","  Mean Abs Loss: 4.809524\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4880.154575 \tValidation Loss: 1274.939541\n","  Epoch: 100 \tTraining Loss: 4377.959038 \tValidation Loss: 1159.124712\n","  Epoch: 150 \tTraining Loss: 4103.613522 \tValidation Loss: 1098.024230\n","  Epoch: 200 \tTraining Loss: 3976.811998 \tValidation Loss: 1046.875553\n","  Epoch: 250 \tTraining Loss: 4006.734960 \tValidation Loss: 1045.044507\n","  Epoch: 300 \tTraining Loss: 4007.888559 \tValidation Loss: 1015.522414\n","Best Validation Loss: 727.731849 in epoch 132\n","Best Train Loss: 3730.586440 in epoch 262\n","  Test Loss: 8.862807\n","\n","  Mean Abs Loss: 4.761905\n","\n","  Test Loss: 8.879803\n","\n","  Mean Abs Loss: 4.819048\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3781.501737 \tValidation Loss: 1014.337225\n","  Epoch: 100 \tTraining Loss: 3773.840908 \tValidation Loss: 966.391433\n","  Epoch: 150 \tTraining Loss: 3654.903239 \tValidation Loss: 871.076526\n","  Epoch: 200 \tTraining Loss: 3616.160037 \tValidation Loss: 977.523530\n","  Epoch: 250 \tTraining Loss: 3658.345252 \tValidation Loss: 957.610390\n","  Epoch: 300 \tTraining Loss: 3586.560049 \tValidation Loss: 962.708037\n","Best Validation Loss: 712.079804 in epoch 138\n","Best Train Loss: 2979.194563 in epoch 30\n","  Test Loss: 8.801771\n","\n","  Mean Abs Loss: 4.990476\n","\n","  Test Loss: 8.555243\n","\n","  Mean Abs Loss: 4.676190\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4963.810538 \tValidation Loss: 1393.623386\n","  Epoch: 100 \tTraining Loss: 4355.110266 \tValidation Loss: 1184.132427\n","  Epoch: 150 \tTraining Loss: 3964.702604 \tValidation Loss: 1089.799840\n","  Epoch: 200 \tTraining Loss: 3959.977479 \tValidation Loss: 981.896432\n","  Epoch: 250 \tTraining Loss: 3808.267981 \tValidation Loss: 1047.347310\n","  Epoch: 300 \tTraining Loss: 3767.129947 \tValidation Loss: 1036.036479\n","Best Validation Loss: 779.026652 in epoch 87\n","Best Train Loss: 2961.740348 in epoch 296\n","  Test Loss: 9.025287\n","\n","  Mean Abs Loss: 5.152381\n","\n","  Test Loss: 10.090207\n","\n","  Mean Abs Loss: 5.923810\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3882.897549 \tValidation Loss: 1009.321043\n","  Epoch: 100 \tTraining Loss: 3789.194231 \tValidation Loss: 1006.602447\n","  Epoch: 150 \tTraining Loss: 3659.029720 \tValidation Loss: 969.412379\n","  Epoch: 200 \tTraining Loss: 3586.855358 \tValidation Loss: 989.437771\n","  Epoch: 250 \tTraining Loss: 3561.935443 \tValidation Loss: 977.456000\n","  Epoch: 300 \tTraining Loss: 3526.733437 \tValidation Loss: 992.978544\n","Best Validation Loss: 742.913459 in epoch 110\n","Best Train Loss: 2863.509372 in epoch 102\n","  Test Loss: 8.728579\n","\n","  Mean Abs Loss: 4.723810\n","\n","  Test Loss: 8.715773\n","\n","  Mean Abs Loss: 4.685714\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 5000.319705 \tValidation Loss: 1422.282925\n","  Epoch: 100 \tTraining Loss: 4430.279584 \tValidation Loss: 1227.954547\n","  Epoch: 150 \tTraining Loss: 4186.974535 \tValidation Loss: 1060.362908\n","  Epoch: 200 \tTraining Loss: 4180.177589 \tValidation Loss: 991.761917\n","  Epoch: 250 \tTraining Loss: 4005.828248 \tValidation Loss: 1023.981118\n","  Epoch: 300 \tTraining Loss: 3991.126536 \tValidation Loss: 1034.139383\n","Best Validation Loss: 625.220630 in epoch 240\n","Best Train Loss: 3180.464774 in epoch 266\n","  Test Loss: 9.300711\n","\n","  Mean Abs Loss: 5.304762\n","\n","  Test Loss: 9.335830\n","\n","  Mean Abs Loss: 5.342857\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3769.945238 \tValidation Loss: 1001.151426\n","  Epoch: 100 \tTraining Loss: 3689.071710 \tValidation Loss: 1008.450434\n","  Epoch: 150 \tTraining Loss: 3685.489773 \tValidation Loss: 950.094866\n","  Epoch: 200 \tTraining Loss: 3586.001115 \tValidation Loss: 953.412667\n","  Epoch: 250 \tTraining Loss: 3658.363104 \tValidation Loss: 956.151317\n","  Epoch: 300 \tTraining Loss: 3463.445256 \tValidation Loss: 975.064209\n","Best Validation Loss: 730.876816 in epoch 72\n","Best Train Loss: 2864.735528 in epoch 157\n","  Test Loss: 8.916029\n","\n","  Mean Abs Loss: 4.876190\n","\n","  Test Loss: 8.749662\n","\n","  Mean Abs Loss: 4.819048\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4905.801475 \tValidation Loss: 1296.254982\n","  Epoch: 100 \tTraining Loss: 4376.805911 \tValidation Loss: 1193.305839\n","  Epoch: 150 \tTraining Loss: 4122.308520 \tValidation Loss: 1129.807477\n","  Epoch: 200 \tTraining Loss: 4127.095040 \tValidation Loss: 1132.358611\n","  Epoch: 250 \tTraining Loss: 3883.439061 \tValidation Loss: 1016.768737\n","  Epoch: 300 \tTraining Loss: 3912.019468 \tValidation Loss: 1039.901229\n","Best Validation Loss: 789.855479 in epoch 298\n","Best Train Loss: 3271.282992 in epoch 159\n","  Test Loss: 9.917149\n","\n","  Mean Abs Loss: 5.771429\n","\n","  Test Loss: 8.856648\n","\n","  Mean Abs Loss: 4.685714\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3824.237056 \tValidation Loss: 1046.840922\n","  Epoch: 100 \tTraining Loss: 3686.238457 \tValidation Loss: 1077.200323\n","  Epoch: 150 \tTraining Loss: 3671.078030 \tValidation Loss: 993.201057\n","  Epoch: 200 \tTraining Loss: 3571.883073 \tValidation Loss: 985.892085\n","  Epoch: 250 \tTraining Loss: 3553.792992 \tValidation Loss: 985.776769\n","  Epoch: 300 \tTraining Loss: 3489.815743 \tValidation Loss: 970.043600\n","Best Validation Loss: 485.610248 in epoch 156\n","Best Train Loss: 2804.043832 in epoch 143\n","  Test Loss: 8.920240\n","\n","  Mean Abs Loss: 4.942857\n","\n","  Test Loss: 8.834004\n","\n","  Mean Abs Loss: 4.819048\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4737.217580 \tValidation Loss: 1251.648524\n","  Epoch: 100 \tTraining Loss: 4118.042393 \tValidation Loss: 1082.919466\n","  Epoch: 150 \tTraining Loss: 3650.272328 \tValidation Loss: 1077.126628\n","  Epoch: 200 \tTraining Loss: 4086.668522 \tValidation Loss: 1050.648833\n","  Epoch: 250 \tTraining Loss: 3884.043070 \tValidation Loss: 1050.214501\n","  Epoch: 300 \tTraining Loss: 3903.812224 \tValidation Loss: 1016.402812\n","Best Validation Loss: 744.798038 in epoch 253\n","Best Train Loss: 2987.376273 in epoch 267\n","  Test Loss: 9.263428\n","\n","  Mean Abs Loss: 5.152381\n","\n","  Test Loss: 9.410296\n","\n","  Mean Abs Loss: 5.276190\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3800.570824 \tValidation Loss: 978.032224\n","  Epoch: 100 \tTraining Loss: 3732.020052 \tValidation Loss: 1004.560742\n","  Epoch: 150 \tTraining Loss: 3654.703769 \tValidation Loss: 1013.981724\n","  Epoch: 200 \tTraining Loss: 3555.319691 \tValidation Loss: 991.228036\n","  Epoch: 250 \tTraining Loss: 3515.948936 \tValidation Loss: 976.735356\n","  Epoch: 300 \tTraining Loss: 3547.869640 \tValidation Loss: 976.072214\n","Best Validation Loss: 689.222318 in epoch 271\n","Best Train Loss: 2840.081490 in epoch 134\n","  Test Loss: 8.709242\n","\n","  Mean Abs Loss: 4.895238\n","\n","  Test Loss: 8.696480\n","\n","  Mean Abs Loss: 4.733333\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4702.273468 \tValidation Loss: 1253.673232\n","  Epoch: 100 \tTraining Loss: 4212.257521 \tValidation Loss: 849.756058\n","  Epoch: 150 \tTraining Loss: 4171.032775 \tValidation Loss: 1041.935399\n","  Epoch: 200 \tTraining Loss: 3829.278829 \tValidation Loss: 1013.918565\n","  Epoch: 250 \tTraining Loss: 4083.070822 \tValidation Loss: 993.520109\n","  Epoch: 300 \tTraining Loss: 4056.639188 \tValidation Loss: 957.956330\n","Best Validation Loss: 687.004088 in epoch 103\n","Best Train Loss: 2861.613800 in epoch 127\n","  Test Loss: 8.761537\n","\n","  Mean Abs Loss: 4.761905\n","\n","  Test Loss: 9.036091\n","\n","  Mean Abs Loss: 4.800000\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3858.296619 \tValidation Loss: 1036.182834\n","  Epoch: 100 \tTraining Loss: 3761.652064 \tValidation Loss: 990.406393\n","  Epoch: 150 \tTraining Loss: 3622.829152 \tValidation Loss: 988.301948\n","  Epoch: 200 \tTraining Loss: 3590.778761 \tValidation Loss: 1002.953894\n","  Epoch: 250 \tTraining Loss: 3542.281358 \tValidation Loss: 998.699617\n","  Epoch: 300 \tTraining Loss: 3526.149054 \tValidation Loss: 977.050754\n","Best Validation Loss: 749.530538 in epoch 251\n","Best Train Loss: 2769.383257 in epoch 296\n","  Test Loss: 8.433963\n","\n","  Mean Abs Loss: 4.580952\n","\n"," Test loss decreased (4.580952 --> 4.580952).  Saving model ...\n","  Test Loss: 8.573140\n","\n","  Mean Abs Loss: 4.580952\n","\n"," Test loss decreased (4.580952 --> 4.580952).  Saving model ...\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 5119.465680 \tValidation Loss: 1329.073504\n","  Epoch: 100 \tTraining Loss: 4524.098932 \tValidation Loss: 1155.842775\n","  Epoch: 150 \tTraining Loss: 4204.848947 \tValidation Loss: 871.215484\n","  Epoch: 200 \tTraining Loss: 4120.012161 \tValidation Loss: 1030.053239\n","  Epoch: 250 \tTraining Loss: 3940.873329 \tValidation Loss: 1027.205750\n","  Epoch: 300 \tTraining Loss: 4021.019869 \tValidation Loss: 1010.228064\n","Best Validation Loss: 669.430501 in epoch 229\n","Best Train Loss: 2953.615924 in epoch 149\n","  Test Loss: 9.088616\n","\n","  Mean Abs Loss: 4.980952\n","\n","  Test Loss: 8.903878\n","\n","  Mean Abs Loss: 4.819048\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3772.674770 \tValidation Loss: 997.697342\n","  Epoch: 100 \tTraining Loss: 3654.534937 \tValidation Loss: 817.047374\n","  Epoch: 150 \tTraining Loss: 3600.346766 \tValidation Loss: 839.204192\n","  Epoch: 200 \tTraining Loss: 3576.695473 \tValidation Loss: 613.745723\n","  Epoch: 250 \tTraining Loss: 3527.412383 \tValidation Loss: 946.010872\n","  Epoch: 300 \tTraining Loss: 3554.337336 \tValidation Loss: 979.929604\n","Best Validation Loss: 543.840263 in epoch 93\n","Best Train Loss: 2709.223715 in epoch 151\n","  Test Loss: 8.808969\n","\n","  Mean Abs Loss: 4.800000\n","\n","  Test Loss: 8.764019\n","\n","  Mean Abs Loss: 4.809524\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4808.396524 \tValidation Loss: 1232.316898\n","  Epoch: 100 \tTraining Loss: 4341.454095 \tValidation Loss: 1048.967744\n","  Epoch: 150 \tTraining Loss: 3630.033603 \tValidation Loss: 1046.686476\n","  Epoch: 200 \tTraining Loss: 4073.720663 \tValidation Loss: 1017.658341\n","  Epoch: 250 \tTraining Loss: 3933.436805 \tValidation Loss: 988.791895\n","  Epoch: 300 \tTraining Loss: 3883.865951 \tValidation Loss: 1014.469545\n","Best Validation Loss: 616.359990 in epoch 253\n","Best Train Loss: 2995.710494 in epoch 224\n","  Test Loss: 8.771725\n","\n","  Mean Abs Loss: 4.685714\n","\n","  Test Loss: 8.735631\n","\n","  Mean Abs Loss: 4.752381\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3805.871497 \tValidation Loss: 1016.070594\n","  Epoch: 100 \tTraining Loss: 3677.376364 \tValidation Loss: 982.679758\n","  Epoch: 150 \tTraining Loss: 3667.993786 \tValidation Loss: 924.482758\n","  Epoch: 200 \tTraining Loss: 3615.984661 \tValidation Loss: 933.104200\n","  Epoch: 250 \tTraining Loss: 3548.514377 \tValidation Loss: 985.728159\n","  Epoch: 300 \tTraining Loss: 3540.564546 \tValidation Loss: 971.244098\n","Best Validation Loss: 663.336382 in epoch 271\n","Best Train Loss: 2768.277845 in epoch 93\n","  Test Loss: 8.810443\n","\n","  Mean Abs Loss: 4.742857\n","\n","  Test Loss: 8.940667\n","\n","  Mean Abs Loss: 4.847619\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 5054.613275 \tValidation Loss: 1396.430456\n","  Epoch: 100 \tTraining Loss: 4485.965616 \tValidation Loss: 1137.550941\n","  Epoch: 150 \tTraining Loss: 4159.825012 \tValidation Loss: 1103.403731\n","  Epoch: 200 \tTraining Loss: 4131.394955 \tValidation Loss: 1099.511636\n","  Epoch: 250 \tTraining Loss: 3878.204731 \tValidation Loss: 977.169803\n","  Epoch: 300 \tTraining Loss: 3938.219518 \tValidation Loss: 881.151586\n","Best Validation Loss: 817.179340 in epoch 193\n","Best Train Loss: 3627.837435 in epoch 211\n","  Test Loss: 9.657209\n","\n","  Mean Abs Loss: 5.333333\n","\n","  Test Loss: 9.623764\n","\n","  Mean Abs Loss: 5.295238\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3887.962933 \tValidation Loss: 1021.464240\n","  Epoch: 100 \tTraining Loss: 3829.144397 \tValidation Loss: 962.982287\n","  Epoch: 150 \tTraining Loss: 3624.096418 \tValidation Loss: 989.362838\n","  Epoch: 200 \tTraining Loss: 3710.727200 \tValidation Loss: 994.328143\n","  Epoch: 250 \tTraining Loss: 3587.336815 \tValidation Loss: 999.576100\n","  Epoch: 300 \tTraining Loss: 3478.205572 \tValidation Loss: 946.503458\n","Best Validation Loss: 562.899321 in epoch 159\n","Best Train Loss: 2867.433806 in epoch 203\n","  Test Loss: 8.699668\n","\n","  Mean Abs Loss: 4.695238\n","\n","  Test Loss: 8.574751\n","\n","  Mean Abs Loss: 4.695238\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4919.462384 \tValidation Loss: 1270.928728\n","  Epoch: 100 \tTraining Loss: 4241.211087 \tValidation Loss: 864.599937\n","  Epoch: 150 \tTraining Loss: 4015.529700 \tValidation Loss: 1087.744724\n","  Epoch: 200 \tTraining Loss: 3953.617076 \tValidation Loss: 1056.204538\n","  Epoch: 250 \tTraining Loss: 3904.811987 \tValidation Loss: 856.023291\n","  Epoch: 300 \tTraining Loss: 3967.960023 \tValidation Loss: 1053.692671\n","Best Validation Loss: 805.433056 in epoch 90\n","Best Train Loss: 3045.261628 in epoch 263\n","  Test Loss: 8.715881\n","\n","  Mean Abs Loss: 4.771429\n","\n","  Test Loss: 8.945845\n","\n","  Mean Abs Loss: 4.828571\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3749.820229 \tValidation Loss: 997.157730\n","  Epoch: 100 \tTraining Loss: 3675.806457 \tValidation Loss: 1002.832845\n","  Epoch: 150 \tTraining Loss: 3660.631386 \tValidation Loss: 994.663536\n","  Epoch: 200 \tTraining Loss: 3605.902230 \tValidation Loss: 976.158898\n","  Epoch: 250 \tTraining Loss: 3536.178904 \tValidation Loss: 973.470416\n","  Epoch: 300 \tTraining Loss: 3342.473179 \tValidation Loss: 971.963641\n","Best Validation Loss: 615.189493 in epoch 173\n","Best Train Loss: 2666.921435 in epoch 278\n","  Test Loss: 8.952307\n","\n","  Mean Abs Loss: 4.857143\n","\n","  Test Loss: 8.898362\n","\n","  Mean Abs Loss: 4.761905\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4859.983271 \tValidation Loss: 1334.966084\n","  Epoch: 100 \tTraining Loss: 4285.478609 \tValidation Loss: 1187.970913\n","  Epoch: 150 \tTraining Loss: 4243.650040 \tValidation Loss: 1050.625151\n","  Epoch: 200 \tTraining Loss: 4076.028796 \tValidation Loss: 1039.986548\n","  Epoch: 250 \tTraining Loss: 3908.057570 \tValidation Loss: 1014.099997\n","  Epoch: 300 \tTraining Loss: 3844.718824 \tValidation Loss: 954.667313\n","Best Validation Loss: 789.018092 in epoch 255\n","Best Train Loss: 3614.404444 in epoch 228\n","  Test Loss: 9.698886\n","\n","  Mean Abs Loss: 5.504762\n","\n","  Test Loss: 9.596082\n","\n","  Mean Abs Loss: 5.323810\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3911.480000 \tValidation Loss: 1036.113342\n","  Epoch: 100 \tTraining Loss: 3727.731910 \tValidation Loss: 985.776395\n","  Epoch: 150 \tTraining Loss: 3796.430743 \tValidation Loss: 998.470470\n","  Epoch: 200 \tTraining Loss: 3612.652097 \tValidation Loss: 961.602533\n","  Epoch: 250 \tTraining Loss: 3567.876188 \tValidation Loss: 955.869979\n","  Epoch: 300 \tTraining Loss: 3593.064526 \tValidation Loss: 964.763740\n","Best Validation Loss: 719.810232 in epoch 161\n","Best Train Loss: 2806.695257 in epoch 232\n","  Test Loss: 8.874478\n","\n","  Mean Abs Loss: 4.923810\n","\n","  Test Loss: 8.783242\n","\n","  Mean Abs Loss: 4.790476\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4796.422154 \tValidation Loss: 1284.779858\n","  Epoch: 100 \tTraining Loss: 4218.057622 \tValidation Loss: 1184.860637\n","  Epoch: 150 \tTraining Loss: 4082.758771 \tValidation Loss: 1101.329828\n","  Epoch: 200 \tTraining Loss: 3918.702938 \tValidation Loss: 950.358117\n","  Epoch: 250 \tTraining Loss: 3939.390616 \tValidation Loss: 1030.660305\n","  Epoch: 300 \tTraining Loss: 3900.339445 \tValidation Loss: 1057.752461\n","Best Validation Loss: 741.937431 in epoch 216\n","Best Train Loss: 3111.847778 in epoch 234\n","  Test Loss: 9.028964\n","\n","  Mean Abs Loss: 5.133333\n","\n","  Test Loss: 8.894124\n","\n","  Mean Abs Loss: 5.009524\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3841.899017 \tValidation Loss: 637.245953\n","  Epoch: 100 \tTraining Loss: 3714.463874 \tValidation Loss: 747.261703\n","  Epoch: 150 \tTraining Loss: 3694.895480 \tValidation Loss: 801.437890\n","  Epoch: 200 \tTraining Loss: 3584.611343 \tValidation Loss: 980.090843\n","  Epoch: 250 \tTraining Loss: 3584.224697 \tValidation Loss: 970.026319\n","  Epoch: 300 \tTraining Loss: 3567.992765 \tValidation Loss: 970.603783\n","Best Validation Loss: 635.673879 in epoch 101\n","Best Train Loss: 2704.976983 in epoch 264\n","  Test Loss: 8.592328\n","\n","  Mean Abs Loss: 4.638095\n","\n","  Test Loss: 8.818612\n","\n","  Mean Abs Loss: 4.752381\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4530.481083 \tValidation Loss: 1251.284545\n","  Epoch: 100 \tTraining Loss: 4164.699757 \tValidation Loss: 930.789321\n","  Epoch: 150 \tTraining Loss: 3992.132261 \tValidation Loss: 1054.466949\n","  Epoch: 200 \tTraining Loss: 3963.906651 \tValidation Loss: 967.502088\n","  Epoch: 250 \tTraining Loss: 3915.582341 \tValidation Loss: 1033.780209\n","  Epoch: 300 \tTraining Loss: 3835.421392 \tValidation Loss: 1028.145326\n","Best Validation Loss: 700.549066 in epoch 189\n","Best Train Loss: 2837.264202 in epoch 247\n","  Test Loss: 8.701978\n","\n","  Mean Abs Loss: 4.914286\n","\n","  Test Loss: 8.878691\n","\n","  Mean Abs Loss: 4.952381\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3910.847600 \tValidation Loss: 992.017216\n","  Epoch: 100 \tTraining Loss: 3786.867947 \tValidation Loss: 999.841470\n","  Epoch: 150 \tTraining Loss: 3675.611897 \tValidation Loss: 978.023913\n","  Epoch: 200 \tTraining Loss: 3454.333778 \tValidation Loss: 966.956913\n","  Epoch: 250 \tTraining Loss: 3488.239975 \tValidation Loss: 935.438576\n","  Epoch: 300 \tTraining Loss: 3506.648942 \tValidation Loss: 940.399134\n","Best Validation Loss: 612.285135 in epoch 232\n","Best Train Loss: 2722.831058 in epoch 262\n","  Test Loss: 8.745338\n","\n","  Mean Abs Loss: 4.695238\n","\n","  Test Loss: 8.706241\n","\n","  Mean Abs Loss: 4.676190\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4735.860517 \tValidation Loss: 1340.014149\n","  Epoch: 100 \tTraining Loss: 4135.577896 \tValidation Loss: 1110.510615\n","  Epoch: 150 \tTraining Loss: 3738.237736 \tValidation Loss: 1086.486611\n","  Epoch: 200 \tTraining Loss: 3918.596610 \tValidation Loss: 1078.674904\n","  Epoch: 250 \tTraining Loss: 3985.198561 \tValidation Loss: 1034.653078\n","  Epoch: 300 \tTraining Loss: 3516.006372 \tValidation Loss: 1032.998943\n","Best Validation Loss: 816.504706 in epoch 277\n","Best Train Loss: 3088.001955 in epoch 197\n","  Test Loss: 8.818139\n","\n","  Mean Abs Loss: 4.704762\n","\n","  Test Loss: 8.729651\n","\n","  Mean Abs Loss: 4.723810\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3920.303306 \tValidation Loss: 1047.066427\n","  Epoch: 100 \tTraining Loss: 3748.250394 \tValidation Loss: 1002.900595\n","  Epoch: 150 \tTraining Loss: 3673.574230 \tValidation Loss: 1012.022743\n","  Epoch: 200 \tTraining Loss: 3616.291023 \tValidation Loss: 994.287480\n","  Epoch: 250 \tTraining Loss: 3491.857465 \tValidation Loss: 1000.944052\n","  Epoch: 300 \tTraining Loss: 3503.565488 \tValidation Loss: 980.258018\n","Best Validation Loss: 638.446792 in epoch 137\n","Best Train Loss: 2952.303866 in epoch 91\n","  Test Loss: 8.974256\n","\n","  Mean Abs Loss: 4.904762\n","\n","  Test Loss: 8.804969\n","\n","  Mean Abs Loss: 4.780952\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4824.055621 \tValidation Loss: 1346.668668\n","  Epoch: 100 \tTraining Loss: 4196.917355 \tValidation Loss: 1117.683728\n","  Epoch: 150 \tTraining Loss: 4137.471072 \tValidation Loss: 1054.138315\n","  Epoch: 200 \tTraining Loss: 4022.543323 \tValidation Loss: 1077.729167\n","  Epoch: 250 \tTraining Loss: 3794.038514 \tValidation Loss: 1033.188943\n","  Epoch: 300 \tTraining Loss: 3759.508229 \tValidation Loss: 1021.795051\n","Best Validation Loss: 808.410627 in epoch 214\n","Best Train Loss: 3505.431908 in epoch 222\n","  Test Loss: 9.713146\n","\n","  Mean Abs Loss: 5.419048\n","\n","  Test Loss: 9.730928\n","\n","  Mean Abs Loss: 5.514286\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3778.825929 \tValidation Loss: 940.519576\n","  Epoch: 100 \tTraining Loss: 3685.511474 \tValidation Loss: 974.001424\n","  Epoch: 150 \tTraining Loss: 3753.969915 \tValidation Loss: 803.872863\n","  Epoch: 200 \tTraining Loss: 3692.648816 \tValidation Loss: 967.000762\n","  Epoch: 250 \tTraining Loss: 3747.625910 \tValidation Loss: 911.951322\n","  Epoch: 300 \tTraining Loss: 3710.413240 \tValidation Loss: 909.539669\n","Best Validation Loss: 600.827223 in epoch 18\n","Best Train Loss: 2936.465852 in epoch 123\n","  Test Loss: 9.717917\n","\n","  Mean Abs Loss: 5.438095\n","\n","  Test Loss: 9.672840\n","\n","  Mean Abs Loss: 5.457143\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 5036.383096 \tValidation Loss: 1234.333425\n","  Epoch: 100 \tTraining Loss: 4518.526834 \tValidation Loss: 999.248634\n","  Epoch: 150 \tTraining Loss: 4315.834073 \tValidation Loss: 994.597557\n","  Epoch: 200 \tTraining Loss: 4176.232921 \tValidation Loss: 990.122686\n","  Epoch: 250 \tTraining Loss: 4012.472582 \tValidation Loss: 989.869284\n","  Epoch: 300 \tTraining Loss: 3983.928363 \tValidation Loss: 801.035447\n","Best Validation Loss: 626.180021 in epoch 132\n","Best Train Loss: 3061.650579 in epoch 286\n","  Test Loss: 8.813315\n","\n","  Mean Abs Loss: 4.961905\n","\n","  Test Loss: 9.020564\n","\n","  Mean Abs Loss: 4.952381\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3794.488605 \tValidation Loss: 989.962909\n","  Epoch: 100 \tTraining Loss: 3753.716589 \tValidation Loss: 1008.036010\n","  Epoch: 150 \tTraining Loss: 3636.575114 \tValidation Loss: 1019.533666\n","  Epoch: 200 \tTraining Loss: 3553.058435 \tValidation Loss: 981.389831\n","  Epoch: 250 \tTraining Loss: 3537.954221 \tValidation Loss: 764.185448\n","  Epoch: 300 \tTraining Loss: 3455.473539 \tValidation Loss: 986.526939\n","Best Validation Loss: 646.427030 in epoch 159\n","Best Train Loss: 2810.747025 in epoch 141\n","  Test Loss: 8.986783\n","\n","  Mean Abs Loss: 4.895238\n","\n","  Test Loss: 8.959561\n","\n","  Mean Abs Loss: 4.838095\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4831.571347 \tValidation Loss: 1181.162719\n","  Epoch: 100 \tTraining Loss: 4388.689057 \tValidation Loss: 1178.533550\n","  Epoch: 150 \tTraining Loss: 4180.768226 \tValidation Loss: 1114.898288\n","  Epoch: 200 \tTraining Loss: 3952.400467 \tValidation Loss: 1055.346913\n","  Epoch: 250 \tTraining Loss: 3932.095762 \tValidation Loss: 1059.876361\n","  Epoch: 300 \tTraining Loss: 3910.794775 \tValidation Loss: 1062.344579\n","Best Validation Loss: 713.836959 in epoch 152\n","Best Train Loss: 3273.981499 in epoch 122\n","  Test Loss: 9.819404\n","\n","  Mean Abs Loss: 5.666667\n","\n","  Test Loss: 9.781355\n","\n","  Mean Abs Loss: 5.619048\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3806.668926 \tValidation Loss: 1012.439452\n","  Epoch: 100 \tTraining Loss: 3732.371781 \tValidation Loss: 974.309544\n","  Epoch: 150 \tTraining Loss: 3836.166347 \tValidation Loss: 982.939104\n","  Epoch: 200 \tTraining Loss: 3638.153547 \tValidation Loss: 999.913412\n","  Epoch: 250 \tTraining Loss: 3519.031675 \tValidation Loss: 979.749425\n","  Epoch: 300 \tTraining Loss: 3569.778759 \tValidation Loss: 1021.810695\n","Best Validation Loss: 672.619733 in epoch 168\n","Best Train Loss: 2838.957693 in epoch 232\n","  Test Loss: 9.405394\n","\n","  Mean Abs Loss: 5.152381\n","\n","  Test Loss: 8.741150\n","\n","  Mean Abs Loss: 4.771429\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4802.235651 \tValidation Loss: 1272.491267\n","  Epoch: 100 \tTraining Loss: 4249.607992 \tValidation Loss: 1121.847797\n","  Epoch: 150 \tTraining Loss: 3983.747010 \tValidation Loss: 996.775297\n","  Epoch: 200 \tTraining Loss: 3894.282599 \tValidation Loss: 816.742200\n","  Epoch: 250 \tTraining Loss: 3926.026978 \tValidation Loss: 810.724594\n","  Epoch: 300 \tTraining Loss: 3853.121789 \tValidation Loss: 1012.533641\n","Best Validation Loss: 666.759197 in epoch 219\n","Best Train Loss: 3199.296312 in epoch 134\n","  Test Loss: 9.135899\n","\n","  Mean Abs Loss: 5.180952\n","\n","  Test Loss: 8.728999\n","\n","  Mean Abs Loss: 4.800000\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3867.690159 \tValidation Loss: 933.257617\n","  Epoch: 100 \tTraining Loss: 3787.292533 \tValidation Loss: 934.997656\n","  Epoch: 150 \tTraining Loss: 3625.740573 \tValidation Loss: 981.214925\n","  Epoch: 200 \tTraining Loss: 3591.117609 \tValidation Loss: 985.595756\n","  Epoch: 250 \tTraining Loss: 3607.400105 \tValidation Loss: 996.223451\n","  Epoch: 300 \tTraining Loss: 3521.033335 \tValidation Loss: 967.741979\n","Best Validation Loss: 542.437666 in epoch 295\n","Best Train Loss: 2904.059492 in epoch 95\n","  Test Loss: 8.781460\n","\n","  Mean Abs Loss: 4.761905\n","\n","  Test Loss: 8.752745\n","\n","  Mean Abs Loss: 4.761905\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4889.268156 \tValidation Loss: 1070.590848\n","  Epoch: 100 \tTraining Loss: 4321.816073 \tValidation Loss: 1096.298464\n","  Epoch: 150 \tTraining Loss: 4010.305038 \tValidation Loss: 1030.460629\n","  Epoch: 200 \tTraining Loss: 3980.042771 \tValidation Loss: 1027.615983\n","  Epoch: 250 \tTraining Loss: 3610.794434 \tValidation Loss: 846.263601\n","  Epoch: 300 \tTraining Loss: 3036.285518 \tValidation Loss: 1006.905186\n","Best Validation Loss: 702.298328 in epoch 246\n","Best Train Loss: 3036.285518 in epoch 300\n","  Test Loss: 9.592378\n","\n","  Mean Abs Loss: 5.361905\n","\n","  Test Loss: 9.664073\n","\n","  Mean Abs Loss: 5.380952\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3839.874499 \tValidation Loss: 1051.957900\n","  Epoch: 100 \tTraining Loss: 3728.366599 \tValidation Loss: 893.961163\n","  Epoch: 150 \tTraining Loss: 3626.934515 \tValidation Loss: 718.817746\n","  Epoch: 200 \tTraining Loss: 3574.797809 \tValidation Loss: 979.041116\n","  Epoch: 250 \tTraining Loss: 3668.038066 \tValidation Loss: 976.248535\n","  Epoch: 300 \tTraining Loss: 2770.490530 \tValidation Loss: 975.741956\n","Best Validation Loss: 583.840805 in epoch 282\n","Best Train Loss: 2625.211491 in epoch 296\n","  Test Loss: 8.717829\n","\n","  Mean Abs Loss: 4.695238\n","\n","  Test Loss: 8.672441\n","\n","  Mean Abs Loss: 4.752381\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4823.531308 \tValidation Loss: 1346.979505\n","  Epoch: 100 \tTraining Loss: 4264.451404 \tValidation Loss: 1116.923564\n","  Epoch: 150 \tTraining Loss: 3942.955349 \tValidation Loss: 1093.970624\n","  Epoch: 200 \tTraining Loss: 3959.709435 \tValidation Loss: 1062.273067\n","  Epoch: 250 \tTraining Loss: 3843.815668 \tValidation Loss: 1063.703702\n","  Epoch: 300 \tTraining Loss: 3756.939060 \tValidation Loss: 1056.944131\n","Best Validation Loss: 730.505606 in epoch 159\n","Best Train Loss: 3448.919043 in epoch 284\n","  Test Loss: 8.693844\n","\n","  Mean Abs Loss: 4.714286\n","\n","  Test Loss: 8.778018\n","\n","  Mean Abs Loss: 4.752381\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3960.417712 \tValidation Loss: 1062.949151\n","  Epoch: 100 \tTraining Loss: 3635.728639 \tValidation Loss: 922.488593\n","  Epoch: 150 \tTraining Loss: 3595.840522 \tValidation Loss: 991.824140\n","  Epoch: 200 \tTraining Loss: 3592.081497 \tValidation Loss: 1003.110905\n","  Epoch: 250 \tTraining Loss: 3507.030731 \tValidation Loss: 1024.512045\n","  Epoch: 300 \tTraining Loss: 3523.848689 \tValidation Loss: 942.092470\n","Best Validation Loss: 725.545469 in epoch 296\n","Best Train Loss: 2622.799025 in epoch 202\n","  Test Loss: 8.748957\n","\n","  Mean Abs Loss: 4.704762\n","\n","  Test Loss: 8.798889\n","\n","  Mean Abs Loss: 4.723810\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4908.065347 \tValidation Loss: 1368.459554\n","  Epoch: 100 \tTraining Loss: 4494.575393 \tValidation Loss: 1204.640497\n","  Epoch: 150 \tTraining Loss: 4155.471248 \tValidation Loss: 875.913289\n","  Epoch: 200 \tTraining Loss: 4000.553010 \tValidation Loss: 1040.301986\n","  Epoch: 250 \tTraining Loss: 3965.164509 \tValidation Loss: 1018.222316\n","  Epoch: 300 \tTraining Loss: 3935.804589 \tValidation Loss: 1023.746404\n","Best Validation Loss: 718.169681 in epoch 108\n","Best Train Loss: 3172.065584 in epoch 149\n","  Test Loss: 9.110279\n","\n","  Mean Abs Loss: 5.057143\n","\n","  Test Loss: 9.522758\n","\n","  Mean Abs Loss: 5.323810\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3861.564211 \tValidation Loss: 1025.199973\n","  Epoch: 100 \tTraining Loss: 3736.748388 \tValidation Loss: 1003.185434\n","  Epoch: 150 \tTraining Loss: 3736.085454 \tValidation Loss: 960.810274\n","  Epoch: 200 \tTraining Loss: 3656.349119 \tValidation Loss: 961.905507\n","  Epoch: 250 \tTraining Loss: 3597.491640 \tValidation Loss: 729.997375\n","  Epoch: 300 \tTraining Loss: 3504.774979 \tValidation Loss: 956.893779\n","Best Validation Loss: 729.997375 in epoch 250\n","Best Train Loss: 2815.599683 in epoch 180\n","  Test Loss: 9.019398\n","\n","  Mean Abs Loss: 4.933333\n","\n","  Test Loss: 8.822501\n","\n","  Mean Abs Loss: 4.809524\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4921.762373 \tValidation Loss: 1158.608183\n","  Epoch: 100 \tTraining Loss: 4302.409258 \tValidation Loss: 939.372345\n","  Epoch: 150 \tTraining Loss: 3969.896035 \tValidation Loss: 1096.699398\n","  Epoch: 200 \tTraining Loss: 3936.159469 \tValidation Loss: 865.877948\n","  Epoch: 250 \tTraining Loss: 3966.986804 \tValidation Loss: 1069.931067\n","  Epoch: 300 \tTraining Loss: 3655.924198 \tValidation Loss: 1020.841053\n","Best Validation Loss: 796.183323 in epoch 262\n","Best Train Loss: 3393.700312 in epoch 114\n","  Test Loss: 9.310382\n","\n","  Mean Abs Loss: 5.228571\n","\n","  Test Loss: 8.968552\n","\n","  Mean Abs Loss: 5.142857\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3767.987888 \tValidation Loss: 1055.141039\n","  Epoch: 100 \tTraining Loss: 3791.833246 \tValidation Loss: 790.918574\n","  Epoch: 150 \tTraining Loss: 3615.318322 \tValidation Loss: 996.318065\n","  Epoch: 200 \tTraining Loss: 3570.077236 \tValidation Loss: 955.550462\n","  Epoch: 250 \tTraining Loss: 3594.516616 \tValidation Loss: 913.046226\n","  Epoch: 300 \tTraining Loss: 3516.683012 \tValidation Loss: 952.773843\n","Best Validation Loss: 557.299976 in epoch 240\n","Best Train Loss: 2714.716920 in epoch 259\n","  Test Loss: 8.840289\n","\n","  Mean Abs Loss: 4.752381\n","\n","  Test Loss: 8.902116\n","\n","  Mean Abs Loss: 4.857143\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 5111.405779 \tValidation Loss: 1414.914115\n","  Epoch: 100 \tTraining Loss: 4399.133000 \tValidation Loss: 1238.997000\n","  Epoch: 150 \tTraining Loss: 4056.558292 \tValidation Loss: 1089.506191\n","  Epoch: 200 \tTraining Loss: 3936.959521 \tValidation Loss: 1042.981413\n","  Epoch: 250 \tTraining Loss: 3862.706157 \tValidation Loss: 945.091096\n","  Epoch: 300 \tTraining Loss: 3871.654940 \tValidation Loss: 1024.884038\n","Best Validation Loss: 794.052718 in epoch 238\n","Best Train Loss: 3097.503115 in epoch 234\n","  Test Loss: 10.017285\n","\n","  Mean Abs Loss: 5.790476\n","\n","  Test Loss: 9.465217\n","\n","  Mean Abs Loss: 5.342857\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3745.174754 \tValidation Loss: 1024.361471\n","  Epoch: 100 \tTraining Loss: 3719.978555 \tValidation Loss: 999.489773\n","  Epoch: 150 \tTraining Loss: 3529.386999 \tValidation Loss: 993.104895\n","  Epoch: 200 \tTraining Loss: 3584.218784 \tValidation Loss: 951.505647\n","  Epoch: 250 \tTraining Loss: 3453.140594 \tValidation Loss: 989.670429\n","  Epoch: 300 \tTraining Loss: 3478.812441 \tValidation Loss: 999.852539\n","Best Validation Loss: 719.715566 in epoch 184\n","Best Train Loss: 3195.417706 in epoch 295\n","  Test Loss: 8.881776\n","\n","  Mean Abs Loss: 4.895238\n","\n","  Test Loss: 8.829433\n","\n","  Mean Abs Loss: 4.828571\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4857.340978 \tValidation Loss: 1386.643661\n","  Epoch: 100 \tTraining Loss: 4276.183853 \tValidation Loss: 1257.339531\n","  Epoch: 150 \tTraining Loss: 4099.484449 \tValidation Loss: 1175.191371\n","  Epoch: 200 \tTraining Loss: 3917.321909 \tValidation Loss: 1094.701217\n","  Epoch: 250 \tTraining Loss: 3972.798281 \tValidation Loss: 1049.825595\n","  Epoch: 300 \tTraining Loss: 3993.231503 \tValidation Loss: 1015.274756\n","Best Validation Loss: 746.411495 in epoch 298\n","Best Train Loss: 3616.570135 in epoch 146\n","  Test Loss: 9.118931\n","\n","  Mean Abs Loss: 5.114286\n","\n","  Test Loss: 8.771143\n","\n","  Mean Abs Loss: 5.000000\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3882.330323 \tValidation Loss: 958.998365\n","  Epoch: 100 \tTraining Loss: 3730.832995 \tValidation Loss: 980.951563\n","  Epoch: 150 \tTraining Loss: 3644.409320 \tValidation Loss: 1011.947227\n","  Epoch: 200 \tTraining Loss: 3566.135547 \tValidation Loss: 980.797445\n","  Epoch: 250 \tTraining Loss: 3525.292676 \tValidation Loss: 952.840187\n","  Epoch: 300 \tTraining Loss: 3562.668681 \tValidation Loss: 967.650450\n","Best Validation Loss: 594.991592 in epoch 47\n","Best Train Loss: 3007.415726 in epoch 132\n","  Test Loss: 9.125678\n","\n","  Mean Abs Loss: 5.095238\n","\n","  Test Loss: 8.930000\n","\n","  Mean Abs Loss: 4.933333\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4778.587688 \tValidation Loss: 1200.417516\n","  Epoch: 100 \tTraining Loss: 4333.572586 \tValidation Loss: 1074.372423\n","  Epoch: 150 \tTraining Loss: 4063.472791 \tValidation Loss: 990.593156\n","  Epoch: 200 \tTraining Loss: 3905.145246 \tValidation Loss: 1011.612941\n","  Epoch: 250 \tTraining Loss: 3985.430027 \tValidation Loss: 964.335041\n","  Epoch: 300 \tTraining Loss: 3910.076852 \tValidation Loss: 1001.581556\n","Best Validation Loss: 596.542907 in epoch 131\n","Best Train Loss: 3004.723310 in epoch 243\n","  Test Loss: 8.841586\n","\n","  Mean Abs Loss: 4.809524\n","\n","  Test Loss: 8.989404\n","\n","  Mean Abs Loss: 4.980952\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3805.073369 \tValidation Loss: 936.091105\n","  Epoch: 100 \tTraining Loss: 3726.927128 \tValidation Loss: 934.786382\n","  Epoch: 150 \tTraining Loss: 3633.864032 \tValidation Loss: 988.273032\n","  Epoch: 200 \tTraining Loss: 3375.033294 \tValidation Loss: 987.715788\n","  Epoch: 250 \tTraining Loss: 3472.832703 \tValidation Loss: 950.823633\n","  Epoch: 300 \tTraining Loss: 3406.989458 \tValidation Loss: 1002.533478\n","Best Validation Loss: 582.981097 in epoch 86\n","Best Train Loss: 3230.517606 in epoch 235\n","  Test Loss: 9.015323\n","\n","  Mean Abs Loss: 4.876190\n","\n","  Test Loss: 8.782541\n","\n","  Mean Abs Loss: 4.657143\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4748.856539 \tValidation Loss: 1280.690691\n","  Epoch: 100 \tTraining Loss: 4214.891853 \tValidation Loss: 1164.037156\n","  Epoch: 150 \tTraining Loss: 4043.405277 \tValidation Loss: 1118.253306\n","  Epoch: 200 \tTraining Loss: 3983.254850 \tValidation Loss: 1093.975922\n","  Epoch: 250 \tTraining Loss: 3917.207830 \tValidation Loss: 1075.167343\n","  Epoch: 300 \tTraining Loss: 3742.017014 \tValidation Loss: 1032.382866\n","Best Validation Loss: 800.976341 in epoch 282\n","Best Train Loss: 3472.838375 in epoch 244\n","  Test Loss: 8.671764\n","\n","  Mean Abs Loss: 4.914286\n","\n","  Test Loss: 8.657926\n","\n","  Mean Abs Loss: 5.066667\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3902.773690 \tValidation Loss: 627.696877\n","  Epoch: 100 \tTraining Loss: 3843.589463 \tValidation Loss: 1000.152437\n","  Epoch: 150 \tTraining Loss: 3616.934629 \tValidation Loss: 992.340513\n","  Epoch: 200 \tTraining Loss: 3641.903860 \tValidation Loss: 965.444718\n","  Epoch: 250 \tTraining Loss: 3667.462224 \tValidation Loss: 982.218399\n","  Epoch: 300 \tTraining Loss: 3603.388384 \tValidation Loss: 1001.852235\n","Best Validation Loss: 627.696877 in epoch 50\n","Best Train Loss: 3112.157431 in epoch 40\n","  Test Loss: 8.688926\n","\n","  Mean Abs Loss: 4.790476\n","\n","  Test Loss: 8.688117\n","\n","  Mean Abs Loss: 4.800000\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4850.465424 \tValidation Loss: 1334.965345\n","  Epoch: 100 \tTraining Loss: 4235.321121 \tValidation Loss: 1167.664437\n","  Epoch: 150 \tTraining Loss: 4146.986961 \tValidation Loss: 1120.857857\n","  Epoch: 200 \tTraining Loss: 3914.297982 \tValidation Loss: 1096.979069\n","  Epoch: 250 \tTraining Loss: 3879.816514 \tValidation Loss: 1044.235213\n","  Epoch: 300 \tTraining Loss: 3941.364523 \tValidation Loss: 1015.283566\n","Best Validation Loss: 744.766302 in epoch 182\n","Best Train Loss: 3547.841698 in epoch 208\n","  Test Loss: 8.993441\n","\n","  Mean Abs Loss: 5.085714\n","\n","  Test Loss: 8.978317\n","\n","  Mean Abs Loss: 5.133333\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3656.343816 \tValidation Loss: 786.566935\n","  Epoch: 100 \tTraining Loss: 3706.634306 \tValidation Loss: 954.886684\n","  Epoch: 150 \tTraining Loss: 3718.912498 \tValidation Loss: 963.570156\n","  Epoch: 200 \tTraining Loss: 3560.453322 \tValidation Loss: 996.279897\n","  Epoch: 250 \tTraining Loss: 3602.957499 \tValidation Loss: 956.031911\n","  Epoch: 300 \tTraining Loss: 3561.744645 \tValidation Loss: 958.903963\n","Best Validation Loss: 545.656494 in epoch 137\n","Best Train Loss: 2911.571855 in epoch 249\n","  Test Loss: 8.839766\n","\n","  Mean Abs Loss: 4.828571\n","\n","  Test Loss: 8.993875\n","\n","  Mean Abs Loss: 4.914286\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4962.996340 \tValidation Loss: 1364.095720\n","  Epoch: 100 \tTraining Loss: 4430.648004 \tValidation Loss: 1216.584466\n","  Epoch: 150 \tTraining Loss: 4141.494840 \tValidation Loss: 1059.917482\n","  Epoch: 200 \tTraining Loss: 3958.327136 \tValidation Loss: 1126.035371\n","  Epoch: 250 \tTraining Loss: 3985.304000 \tValidation Loss: 1113.951014\n","  Epoch: 300 \tTraining Loss: 3606.503123 \tValidation Loss: 1065.419125\n","Best Validation Loss: 862.891177 in epoch 276\n","Best Train Loss: 3534.423441 in epoch 289\n","  Test Loss: 9.039867\n","\n","  Mean Abs Loss: 5.238095\n","\n","  Test Loss: 8.977217\n","\n","  Mean Abs Loss: 5.076190\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3764.377237 \tValidation Loss: 825.444737\n","  Epoch: 100 \tTraining Loss: 3727.521067 \tValidation Loss: 1024.790002\n","  Epoch: 150 \tTraining Loss: 3560.848583 \tValidation Loss: 962.917846\n","  Epoch: 200 \tTraining Loss: 3525.853837 \tValidation Loss: 964.420737\n","  Epoch: 250 \tTraining Loss: 3502.791902 \tValidation Loss: 949.383245\n","  Epoch: 300 \tTraining Loss: 3535.847994 \tValidation Loss: 825.619837\n","Best Validation Loss: 715.385673 in epoch 181\n","Best Train Loss: 3132.877762 in epoch 283\n","  Test Loss: 8.710892\n","\n","  Mean Abs Loss: 4.647619\n","\n","  Test Loss: 8.745355\n","\n","  Mean Abs Loss: 4.590476\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4957.078882 \tValidation Loss: 1328.289052\n","  Epoch: 100 \tTraining Loss: 4414.123411 \tValidation Loss: 1231.766345\n","  Epoch: 150 \tTraining Loss: 4132.548272 \tValidation Loss: 1160.120555\n","  Epoch: 200 \tTraining Loss: 3969.485829 \tValidation Loss: 1091.895575\n","  Epoch: 250 \tTraining Loss: 3779.753455 \tValidation Loss: 903.214008\n","  Epoch: 300 \tTraining Loss: 3773.002697 \tValidation Loss: 1064.283824\n","Best Validation Loss: 642.289566 in epoch 213\n","Best Train Loss: 3568.761357 in epoch 245\n","  Test Loss: 8.840308\n","\n","  Mean Abs Loss: 4.819048\n","\n","  Test Loss: 8.874241\n","\n","  Mean Abs Loss: 4.847619\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 4069.561157 \tValidation Loss: 1002.004057\n","  Epoch: 100 \tTraining Loss: 3778.920380 \tValidation Loss: 963.378528\n","  Epoch: 150 \tTraining Loss: 3574.092067 \tValidation Loss: 1014.929627\n","  Epoch: 200 \tTraining Loss: 3776.995302 \tValidation Loss: 1028.637707\n","  Epoch: 250 \tTraining Loss: 3595.174753 \tValidation Loss: 811.221089\n","  Epoch: 300 \tTraining Loss: 3626.046476 \tValidation Loss: 975.486755\n","Best Validation Loss: 567.161119 in epoch 245\n","Best Train Loss: 3135.348793 in epoch 287\n","  Test Loss: 8.799576\n","\n","  Mean Abs Loss: 4.809524\n","\n","  Test Loss: 9.016638\n","\n","  Mean Abs Loss: 4.990476\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4979.917843 \tValidation Loss: 1259.907303\n","  Epoch: 100 \tTraining Loss: 4347.721846 \tValidation Loss: 1109.615111\n","  Epoch: 150 \tTraining Loss: 4020.426565 \tValidation Loss: 1032.453636\n","  Epoch: 200 \tTraining Loss: 4179.563785 \tValidation Loss: 949.536344\n","  Epoch: 250 \tTraining Loss: 4068.548130 \tValidation Loss: 995.428430\n","  Epoch: 300 \tTraining Loss: 3836.772914 \tValidation Loss: 838.196314\n","Best Validation Loss: 616.523303 in epoch 234\n","Best Train Loss: 3632.451417 in epoch 281\n","  Test Loss: 9.293923\n","\n","  Mean Abs Loss: 5.209524\n","\n","  Test Loss: 9.532065\n","\n","  Mean Abs Loss: 5.342857\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3920.917282 \tValidation Loss: 1016.910056\n","  Epoch: 100 \tTraining Loss: 3749.866263 \tValidation Loss: 997.016658\n","  Epoch: 150 \tTraining Loss: 3610.811196 \tValidation Loss: 798.761920\n","  Epoch: 200 \tTraining Loss: 3546.838858 \tValidation Loss: 976.033612\n","  Epoch: 250 \tTraining Loss: 3564.500686 \tValidation Loss: 963.195058\n","  Epoch: 300 \tTraining Loss: 3576.185591 \tValidation Loss: 930.565220\n","Best Validation Loss: 767.386469 in epoch 292\n","Best Train Loss: 2844.811549 in epoch 193\n","  Test Loss: 8.747646\n","\n","  Mean Abs Loss: 4.780952\n","\n","  Test Loss: 8.900221\n","\n","  Mean Abs Loss: 4.819048\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4880.424327 \tValidation Loss: 1380.183799\n","  Epoch: 100 \tTraining Loss: 4388.962109 \tValidation Loss: 1226.996993\n","  Epoch: 150 \tTraining Loss: 4155.226411 \tValidation Loss: 1192.336432\n","  Epoch: 200 \tTraining Loss: 4040.938749 \tValidation Loss: 1151.873883\n","  Epoch: 250 \tTraining Loss: 4091.866498 \tValidation Loss: 1084.776340\n","  Epoch: 300 \tTraining Loss: 4075.724045 \tValidation Loss: 1091.813996\n","Best Validation Loss: 700.542077 in epoch 260\n","Best Train Loss: 3687.220807 in epoch 296\n","  Test Loss: 9.114803\n","\n","  Mean Abs Loss: 5.161905\n","\n","  Test Loss: 9.195760\n","\n","  Mean Abs Loss: 5.200000\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3789.506200 \tValidation Loss: 1033.362578\n","  Epoch: 100 \tTraining Loss: 3648.571708 \tValidation Loss: 1009.677493\n","  Epoch: 150 \tTraining Loss: 3601.434576 \tValidation Loss: 998.697579\n","  Epoch: 200 \tTraining Loss: 3593.628818 \tValidation Loss: 975.250611\n","  Epoch: 250 \tTraining Loss: 3582.537205 \tValidation Loss: 623.961077\n","  Epoch: 300 \tTraining Loss: 3462.917201 \tValidation Loss: 987.289827\n","Best Validation Loss: 623.961077 in epoch 250\n","Best Train Loss: 2386.358292 in epoch 209\n","  Test Loss: 9.057221\n","\n","  Mean Abs Loss: 5.028571\n","\n","  Test Loss: 8.921564\n","\n","  Mean Abs Loss: 4.904762\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4994.208538 \tValidation Loss: 1468.616042\n","  Epoch: 100 \tTraining Loss: 4458.269760 \tValidation Loss: 1335.161408\n","  Epoch: 150 \tTraining Loss: 3837.790019 \tValidation Loss: 1156.752535\n","  Epoch: 200 \tTraining Loss: 4049.267296 \tValidation Loss: 1152.140958\n","  Epoch: 250 \tTraining Loss: 3904.862333 \tValidation Loss: 1068.730107\n","  Epoch: 300 \tTraining Loss: 3769.208714 \tValidation Loss: 1082.474171\n","Best Validation Loss: 835.605839 in epoch 251\n","Best Train Loss: 3509.874474 in epoch 222\n","  Test Loss: 9.565739\n","\n","  Mean Abs Loss: 5.304762\n","\n","  Test Loss: 9.357719\n","\n","  Mean Abs Loss: 5.161905\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3781.493505 \tValidation Loss: 1032.920134\n","  Epoch: 100 \tTraining Loss: 3583.808154 \tValidation Loss: 966.641781\n","  Epoch: 150 \tTraining Loss: 3721.330638 \tValidation Loss: 985.804578\n","  Epoch: 200 \tTraining Loss: 3663.607956 \tValidation Loss: 980.638001\n","  Epoch: 250 \tTraining Loss: 2732.301104 \tValidation Loss: 882.228056\n","  Epoch: 300 \tTraining Loss: 3444.351874 \tValidation Loss: 954.226242\n","Best Validation Loss: 562.167086 in epoch 174\n","Best Train Loss: 2613.745311 in epoch 64\n","  Test Loss: 8.926221\n","\n","  Mean Abs Loss: 5.000000\n","\n","  Test Loss: 8.926042\n","\n","  Mean Abs Loss: 5.000000\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4838.061907 \tValidation Loss: 1284.204638\n","  Epoch: 100 \tTraining Loss: 4358.488241 \tValidation Loss: 1107.593647\n","  Epoch: 150 \tTraining Loss: 4120.280439 \tValidation Loss: 1069.265485\n","  Epoch: 200 \tTraining Loss: 4018.711150 \tValidation Loss: 956.317717\n","  Epoch: 250 \tTraining Loss: 3939.746854 \tValidation Loss: 998.366884\n","  Epoch: 300 \tTraining Loss: 3944.538944 \tValidation Loss: 764.071083\n","Best Validation Loss: 764.071083 in epoch 300\n","Best Train Loss: 3498.832694 in epoch 273\n","  Test Loss: 8.684331\n","\n","  Mean Abs Loss: 4.895238\n","\n","  Test Loss: 8.760609\n","\n","  Mean Abs Loss: 5.000000\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3953.551861 \tValidation Loss: 935.601294\n","  Epoch: 100 \tTraining Loss: 3825.206250 \tValidation Loss: 984.632122\n","  Epoch: 150 \tTraining Loss: 3586.036837 \tValidation Loss: 630.687542\n","  Epoch: 200 \tTraining Loss: 3670.645745 \tValidation Loss: 993.240589\n","  Epoch: 250 \tTraining Loss: 3676.545703 \tValidation Loss: 974.996374\n","  Epoch: 300 \tTraining Loss: 3454.713398 \tValidation Loss: 960.973285\n","Best Validation Loss: 543.773374 in epoch 265\n","Best Train Loss: 2826.427477 in epoch 193\n","  Test Loss: 8.853657\n","\n","  Mean Abs Loss: 4.771429\n","\n","  Test Loss: 8.813449\n","\n","  Mean Abs Loss: 4.771429\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 5119.562520 \tValidation Loss: 1434.612421\n","  Epoch: 100 \tTraining Loss: 4454.713414 \tValidation Loss: 1154.595400\n","  Epoch: 150 \tTraining Loss: 4142.548472 \tValidation Loss: 1034.853479\n","  Epoch: 200 \tTraining Loss: 3944.934378 \tValidation Loss: 992.815501\n","  Epoch: 250 \tTraining Loss: 3870.979751 \tValidation Loss: 829.817740\n","  Epoch: 300 \tTraining Loss: 3898.875918 \tValidation Loss: 996.021481\n","Best Validation Loss: 726.750225 in epoch 166\n","Best Train Loss: 3557.530342 in epoch 255\n","  Test Loss: 8.852425\n","\n","  Mean Abs Loss: 4.771429\n","\n","  Test Loss: 9.289320\n","\n","  Mean Abs Loss: 5.276190\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3773.678843 \tValidation Loss: 999.793987\n","  Epoch: 100 \tTraining Loss: 3659.851905 \tValidation Loss: 1003.410499\n","  Epoch: 150 \tTraining Loss: 3640.326801 \tValidation Loss: 1008.457602\n","  Epoch: 200 \tTraining Loss: 3346.220416 \tValidation Loss: 997.178180\n","  Epoch: 250 \tTraining Loss: 3528.849663 \tValidation Loss: 977.815651\n","  Epoch: 300 \tTraining Loss: 3481.324057 \tValidation Loss: 930.297285\n","Best Validation Loss: 554.196737 in epoch 256\n","Best Train Loss: 2676.980205 in epoch 280\n","  Test Loss: 8.806730\n","\n","  Mean Abs Loss: 4.771429\n","\n","  Test Loss: 8.807403\n","\n","  Mean Abs Loss: 4.885714\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 5099.671920 \tValidation Loss: 1472.244716\n","  Epoch: 100 \tTraining Loss: 4341.817132 \tValidation Loss: 1236.817166\n","  Epoch: 150 \tTraining Loss: 4039.515542 \tValidation Loss: 926.768350\n","  Epoch: 200 \tTraining Loss: 3894.947785 \tValidation Loss: 1075.909793\n","  Epoch: 250 \tTraining Loss: 3819.900820 \tValidation Loss: 1064.559611\n","  Epoch: 300 \tTraining Loss: 3762.343450 \tValidation Loss: 1047.611595\n","Best Validation Loss: 763.277784 in epoch 273\n","Best Train Loss: 2989.697424 in epoch 182\n","  Test Loss: 8.898577\n","\n","  Mean Abs Loss: 4.761905\n","\n","  Test Loss: 8.885485\n","\n","  Mean Abs Loss: 4.742857\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3762.132319 \tValidation Loss: 987.534546\n","  Epoch: 100 \tTraining Loss: 3666.010664 \tValidation Loss: 999.591712\n","  Epoch: 150 \tTraining Loss: 3573.066830 \tValidation Loss: 1012.571699\n","  Epoch: 200 \tTraining Loss: 3600.667977 \tValidation Loss: 1007.433145\n","  Epoch: 250 \tTraining Loss: 3404.333293 \tValidation Loss: 989.935730\n","  Epoch: 300 \tTraining Loss: 3494.681482 \tValidation Loss: 991.704220\n","Best Validation Loss: 556.663804 in epoch 231\n","Best Train Loss: 2740.546756 in epoch 216\n","  Test Loss: 8.779833\n","\n","  Mean Abs Loss: 4.828571\n","\n","  Test Loss: 8.916730\n","\n","  Mean Abs Loss: 4.838095\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4989.015910 \tValidation Loss: 1380.057739\n","  Epoch: 100 \tTraining Loss: 4388.894416 \tValidation Loss: 1052.707896\n","  Epoch: 150 \tTraining Loss: 4173.306334 \tValidation Loss: 1107.018926\n","  Epoch: 200 \tTraining Loss: 4001.091077 \tValidation Loss: 1078.914776\n","  Epoch: 250 \tTraining Loss: 4004.769871 \tValidation Loss: 1006.665134\n","  Epoch: 300 \tTraining Loss: 3914.318378 \tValidation Loss: 1040.207403\n","Best Validation Loss: 813.049733 in epoch 225\n","Best Train Loss: 3059.862429 in epoch 232\n","  Test Loss: 9.393448\n","\n","  Mean Abs Loss: 5.209524\n","\n","  Test Loss: 9.563790\n","\n","  Mean Abs Loss: 5.333333\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3843.625826 \tValidation Loss: 996.640433\n","  Epoch: 100 \tTraining Loss: 3685.017397 \tValidation Loss: 985.790033\n","  Epoch: 150 \tTraining Loss: 3627.471441 \tValidation Loss: 1008.052393\n","  Epoch: 200 \tTraining Loss: 3619.834515 \tValidation Loss: 994.858345\n","  Epoch: 250 \tTraining Loss: 3664.061314 \tValidation Loss: 974.089551\n","  Epoch: 300 \tTraining Loss: 3652.251239 \tValidation Loss: 989.328797\n","Best Validation Loss: 646.972324 in epoch 217\n","Best Train Loss: 2791.116168 in epoch 288\n","  Test Loss: 8.733796\n","\n","  Mean Abs Loss: 4.619048\n","\n","  Test Loss: 8.958195\n","\n","  Mean Abs Loss: 4.780952\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4786.283164 \tValidation Loss: 1065.719627\n","  Epoch: 100 \tTraining Loss: 4292.579454 \tValidation Loss: 1125.180828\n","  Epoch: 150 \tTraining Loss: 4053.611676 \tValidation Loss: 1042.730799\n","  Epoch: 200 \tTraining Loss: 3931.054339 \tValidation Loss: 1030.346940\n","  Epoch: 250 \tTraining Loss: 3990.593193 \tValidation Loss: 1005.846293\n","  Epoch: 300 \tTraining Loss: 3941.344252 \tValidation Loss: 1018.459340\n","Best Validation Loss: 558.975610 in epoch 293\n","Best Train Loss: 3213.218826 in epoch 140\n","  Test Loss: 9.534700\n","\n","  Mean Abs Loss: 5.314286\n","\n","  Test Loss: 8.954392\n","\n","  Mean Abs Loss: 5.028571\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3843.063424 \tValidation Loss: 801.059459\n","  Epoch: 100 \tTraining Loss: 3668.077730 \tValidation Loss: 1004.450343\n","  Epoch: 150 \tTraining Loss: 3536.607237 \tValidation Loss: 832.401086\n","  Epoch: 200 \tTraining Loss: 3640.880831 \tValidation Loss: 979.855218\n","  Epoch: 250 \tTraining Loss: 3515.754428 \tValidation Loss: 992.160596\n","  Epoch: 300 \tTraining Loss: 3443.947140 \tValidation Loss: 991.810228\n","Best Validation Loss: 746.844432 in epoch 187\n","Best Train Loss: 2975.703167 in epoch 81\n","  Test Loss: 8.856982\n","\n","  Mean Abs Loss: 5.057143\n","\n","  Test Loss: 8.742994\n","\n","  Mean Abs Loss: 4.761905\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 5083.237994 \tValidation Loss: 1363.670928\n","  Epoch: 100 \tTraining Loss: 4404.368198 \tValidation Loss: 1166.639723\n","  Epoch: 150 \tTraining Loss: 4146.651786 \tValidation Loss: 939.281708\n","  Epoch: 200 \tTraining Loss: 4026.750959 \tValidation Loss: 948.107967\n","  Epoch: 250 \tTraining Loss: 3945.277755 \tValidation Loss: 1015.844600\n","  Epoch: 300 \tTraining Loss: 3816.336023 \tValidation Loss: 854.672006\n","Best Validation Loss: 674.317995 in epoch 269\n","Best Train Loss: 3003.471121 in epoch 261\n","  Test Loss: 8.995464\n","\n","  Mean Abs Loss: 5.009524\n","\n","  Test Loss: 9.200251\n","\n","  Mean Abs Loss: 5.161905\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3853.903839 \tValidation Loss: 996.039464\n","  Epoch: 100 \tTraining Loss: 3827.572546 \tValidation Loss: 997.473240\n","  Epoch: 150 \tTraining Loss: 3603.090944 \tValidation Loss: 954.391940\n","  Epoch: 200 \tTraining Loss: 3558.650731 \tValidation Loss: 812.442888\n","  Epoch: 250 \tTraining Loss: 3538.097765 \tValidation Loss: 943.559716\n","  Epoch: 300 \tTraining Loss: 3497.005080 \tValidation Loss: 942.181845\n","Best Validation Loss: 721.220986 in epoch 297\n","Best Train Loss: 2765.148169 in epoch 176\n","  Test Loss: 8.698183\n","\n","  Mean Abs Loss: 4.628571\n","\n","  Test Loss: 8.666261\n","\n","  Mean Abs Loss: 4.609524\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4935.502516 \tValidation Loss: 1350.076187\n","  Epoch: 100 \tTraining Loss: 4401.478647 \tValidation Loss: 1212.202512\n","  Epoch: 150 \tTraining Loss: 4086.938413 \tValidation Loss: 1199.538736\n","  Epoch: 200 \tTraining Loss: 4053.471963 \tValidation Loss: 1088.146785\n","  Epoch: 250 \tTraining Loss: 4044.183879 \tValidation Loss: 1124.492821\n","  Epoch: 300 \tTraining Loss: 3990.986069 \tValidation Loss: 1085.951716\n","Best Validation Loss: 755.109277 in epoch 257\n","Best Train Loss: 3197.478821 in epoch 241\n","  Test Loss: 9.604550\n","\n","  Mean Abs Loss: 5.600000\n","\n","  Test Loss: 9.547225\n","\n","  Mean Abs Loss: 5.438095\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3791.680613 \tValidation Loss: 983.148788\n","  Epoch: 100 \tTraining Loss: 3809.234485 \tValidation Loss: 1123.945923\n","  Epoch: 150 \tTraining Loss: 3694.680782 \tValidation Loss: 756.775290\n","  Epoch: 200 \tTraining Loss: 3584.317262 \tValidation Loss: 995.353651\n","  Epoch: 250 \tTraining Loss: 3502.508014 \tValidation Loss: 976.518792\n","  Epoch: 300 \tTraining Loss: 3496.544748 \tValidation Loss: 971.991014\n","Best Validation Loss: 638.827025 in epoch 90\n","Best Train Loss: 2820.271532 in epoch 227\n","  Test Loss: 8.899997\n","\n","  Mean Abs Loss: 4.866667\n","\n","  Test Loss: 9.132994\n","\n","  Mean Abs Loss: 5.123810\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4975.274379 \tValidation Loss: 1303.694993\n","  Epoch: 100 \tTraining Loss: 4418.175485 \tValidation Loss: 1233.135894\n","  Epoch: 150 \tTraining Loss: 4207.069224 \tValidation Loss: 993.986407\n","  Epoch: 200 \tTraining Loss: 3941.555785 \tValidation Loss: 1063.788181\n","  Epoch: 250 \tTraining Loss: 3837.680984 \tValidation Loss: 1045.971008\n","  Epoch: 300 \tTraining Loss: 3846.904061 \tValidation Loss: 766.434109\n","Best Validation Loss: 766.434109 in epoch 300\n","Best Train Loss: 3309.538795 in epoch 145\n","  Test Loss: 9.087278\n","\n","  Mean Abs Loss: 5.238095\n","\n","  Test Loss: 8.970543\n","\n","  Mean Abs Loss: 5.161905\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3738.069883 \tValidation Loss: 1013.802186\n","  Epoch: 100 \tTraining Loss: 3721.660161 \tValidation Loss: 1008.805251\n","  Epoch: 150 \tTraining Loss: 3499.161164 \tValidation Loss: 990.571108\n","  Epoch: 200 \tTraining Loss: 3656.205238 \tValidation Loss: 999.788958\n","  Epoch: 250 \tTraining Loss: 3639.432595 \tValidation Loss: 981.779621\n","  Epoch: 300 \tTraining Loss: 3414.906784 \tValidation Loss: 714.332745\n","Best Validation Loss: 714.332745 in epoch 300\n","Best Train Loss: 2986.491719 in epoch 73\n","  Test Loss: 9.260877\n","\n","  Mean Abs Loss: 4.942857\n","\n","  Test Loss: 9.058472\n","\n","  Mean Abs Loss: 4.952381\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4875.694351 \tValidation Loss: 1363.536569\n","  Epoch: 100 \tTraining Loss: 4458.797361 \tValidation Loss: 971.913186\n","  Epoch: 150 \tTraining Loss: 4210.961880 \tValidation Loss: 1194.905915\n","  Epoch: 200 \tTraining Loss: 4124.134214 \tValidation Loss: 1107.449479\n","  Epoch: 250 \tTraining Loss: 3921.762235 \tValidation Loss: 930.546231\n","  Epoch: 300 \tTraining Loss: 3925.602173 \tValidation Loss: 1044.032961\n","Best Validation Loss: 785.395461 in epoch 136\n","Best Train Loss: 3766.235038 in epoch 229\n","  Test Loss: 9.292970\n","\n","  Mean Abs Loss: 5.219048\n","\n","  Test Loss: 9.938449\n","\n","  Mean Abs Loss: 5.676190\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3775.343679 \tValidation Loss: 1011.774471\n","  Epoch: 100 \tTraining Loss: 3653.417147 \tValidation Loss: 931.360460\n","  Epoch: 150 \tTraining Loss: 3561.954587 \tValidation Loss: 981.830664\n","  Epoch: 200 \tTraining Loss: 3526.822707 \tValidation Loss: 967.225736\n","  Epoch: 250 \tTraining Loss: 3542.626848 \tValidation Loss: 923.646103\n","  Epoch: 300 \tTraining Loss: 3442.297680 \tValidation Loss: 930.780442\n","Best Validation Loss: 702.777814 in epoch 258\n","Best Train Loss: 2686.911613 in epoch 248\n","  Test Loss: 8.957127\n","\n","  Mean Abs Loss: 4.942857\n","\n","  Test Loss: 8.893484\n","\n","  Mean Abs Loss: 4.857143\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4895.546618 \tValidation Loss: 1369.567787\n","  Epoch: 100 \tTraining Loss: 4390.308630 \tValidation Loss: 1164.607178\n","  Epoch: 150 \tTraining Loss: 4076.874992 \tValidation Loss: 1114.898484\n","  Epoch: 200 \tTraining Loss: 4047.385615 \tValidation Loss: 1032.830677\n","  Epoch: 250 \tTraining Loss: 3978.158330 \tValidation Loss: 1035.115662\n","  Epoch: 300 \tTraining Loss: 3843.437803 \tValidation Loss: 958.426525\n","Best Validation Loss: 713.072660 in epoch 174\n","Best Train Loss: 3024.081440 in epoch 222\n","  Test Loss: 9.638296\n","\n","  Mean Abs Loss: 5.447619\n","\n","  Test Loss: 9.440479\n","\n","  Mean Abs Loss: 5.257143\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3903.167832 \tValidation Loss: 730.600183\n","  Epoch: 100 \tTraining Loss: 3708.044556 \tValidation Loss: 580.691337\n","  Epoch: 150 \tTraining Loss: 3622.863688 \tValidation Loss: 1010.452864\n","  Epoch: 200 \tTraining Loss: 3492.850366 \tValidation Loss: 898.592107\n","  Epoch: 250 \tTraining Loss: 3412.887227 \tValidation Loss: 771.733999\n","  Epoch: 300 \tTraining Loss: 3421.426796 \tValidation Loss: 985.314491\n","Best Validation Loss: 551.466037 in epoch 186\n","Best Train Loss: 2710.012446 in epoch 180\n","  Test Loss: 8.882390\n","\n","  Mean Abs Loss: 4.933333\n","\n","  Test Loss: 8.839393\n","\n","  Mean Abs Loss: 4.790476\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4810.530163 \tValidation Loss: 1175.728228\n","  Epoch: 100 \tTraining Loss: 4354.972090 \tValidation Loss: 1184.269467\n","  Epoch: 150 \tTraining Loss: 4092.943965 \tValidation Loss: 1136.726149\n","  Epoch: 200 \tTraining Loss: 4102.400567 \tValidation Loss: 1093.439986\n","  Epoch: 250 \tTraining Loss: 4055.829928 \tValidation Loss: 1046.812568\n","  Epoch: 300 \tTraining Loss: 3899.293672 \tValidation Loss: 1030.661225\n","Best Validation Loss: 722.324147 in epoch 120\n","Best Train Loss: 3801.673050 in epoch 206\n","  Test Loss: 9.401125\n","\n","  Mean Abs Loss: 5.390476\n","\n","  Test Loss: 9.929929\n","\n","  Mean Abs Loss: 5.714286\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3756.603384 \tValidation Loss: 985.017869\n","  Epoch: 100 \tTraining Loss: 3628.683486 \tValidation Loss: 979.034062\n","  Epoch: 150 \tTraining Loss: 3766.755105 \tValidation Loss: 995.946773\n","  Epoch: 200 \tTraining Loss: 3545.700709 \tValidation Loss: 953.936324\n","  Epoch: 250 \tTraining Loss: 3590.368193 \tValidation Loss: 975.290076\n","  Epoch: 300 \tTraining Loss: 3494.386284 \tValidation Loss: 967.707247\n","Best Validation Loss: 594.753290 in epoch 257\n","Best Train Loss: 2752.959420 in epoch 194\n","  Test Loss: 8.687104\n","\n","  Mean Abs Loss: 4.609524\n","\n","  Test Loss: 8.795215\n","\n","  Mean Abs Loss: 4.761905\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4914.585307 \tValidation Loss: 1373.834886\n","  Epoch: 100 \tTraining Loss: 4267.809670 \tValidation Loss: 1178.236008\n","  Epoch: 150 \tTraining Loss: 3961.563704 \tValidation Loss: 901.935011\n","  Epoch: 200 \tTraining Loss: 3936.439684 \tValidation Loss: 1052.129847\n","  Epoch: 250 \tTraining Loss: 3792.817964 \tValidation Loss: 1058.782463\n","  Epoch: 300 \tTraining Loss: 3767.516620 \tValidation Loss: 1011.037883\n","Best Validation Loss: 718.409829 in epoch 251\n","Best Train Loss: 3440.419567 in epoch 93\n","  Test Loss: 9.214693\n","\n","  Mean Abs Loss: 5.228571\n","\n","  Test Loss: 8.857442\n","\n","  Mean Abs Loss: 4.914286\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3853.333622 \tValidation Loss: 991.556558\n","  Epoch: 100 \tTraining Loss: 3688.494082 \tValidation Loss: 987.939238\n","  Epoch: 150 \tTraining Loss: 3590.670433 \tValidation Loss: 980.028958\n","  Epoch: 200 \tTraining Loss: 3587.667798 \tValidation Loss: 963.362322\n","  Epoch: 250 \tTraining Loss: 3546.670164 \tValidation Loss: 734.560350\n","  Epoch: 300 \tTraining Loss: 3480.086619 \tValidation Loss: 974.978779\n","Best Validation Loss: 726.625948 in epoch 263\n","Best Train Loss: 2693.601576 in epoch 280\n","  Test Loss: 8.838582\n","\n","  Mean Abs Loss: 4.809524\n","\n","  Test Loss: 8.728439\n","\n","  Mean Abs Loss: 4.733333\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4825.512922 \tValidation Loss: 1289.031234\n","  Epoch: 100 \tTraining Loss: 4408.771883 \tValidation Loss: 934.990607\n","  Epoch: 150 \tTraining Loss: 4130.065201 \tValidation Loss: 1108.180031\n","  Epoch: 200 \tTraining Loss: 4118.642786 \tValidation Loss: 1062.330861\n","  Epoch: 250 \tTraining Loss: 3943.992145 \tValidation Loss: 1030.140018\n","  Epoch: 300 \tTraining Loss: 3952.928999 \tValidation Loss: 784.995643\n","Best Validation Loss: 638.992458 in epoch 296\n","Best Train Loss: 3104.939020 in epoch 297\n","  Test Loss: 9.737062\n","\n","  Mean Abs Loss: 5.733333\n","\n","  Test Loss: 9.907729\n","\n","  Mean Abs Loss: 5.904762\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3795.905973 \tValidation Loss: 837.681914\n","  Epoch: 100 \tTraining Loss: 3764.229832 \tValidation Loss: 997.027565\n","  Epoch: 150 \tTraining Loss: 3618.349643 \tValidation Loss: 948.431020\n","  Epoch: 200 \tTraining Loss: 3616.018446 \tValidation Loss: 947.411128\n","  Epoch: 250 \tTraining Loss: 3555.896815 \tValidation Loss: 916.542153\n","  Epoch: 300 \tTraining Loss: 3506.129249 \tValidation Loss: 782.129647\n","Best Validation Loss: 625.969809 in epoch 221\n","Best Train Loss: 3231.320916 in epoch 217\n","  Test Loss: 8.951189\n","\n","  Mean Abs Loss: 4.904762\n","\n","  Test Loss: 8.860257\n","\n","  Mean Abs Loss: 4.885714\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4904.887577 \tValidation Loss: 1345.600077\n","  Epoch: 100 \tTraining Loss: 4407.900523 \tValidation Loss: 1102.359842\n","  Epoch: 150 \tTraining Loss: 4126.558518 \tValidation Loss: 1075.600397\n","  Epoch: 200 \tTraining Loss: 3889.233519 \tValidation Loss: 1053.818344\n","  Epoch: 250 \tTraining Loss: 3926.682535 \tValidation Loss: 1045.879397\n","  Epoch: 300 \tTraining Loss: 3783.255637 \tValidation Loss: 1050.971186\n","Best Validation Loss: 804.522045 in epoch 229\n","Best Train Loss: 3085.765855 in epoch 239\n","  Test Loss: 9.555022\n","\n","  Mean Abs Loss: 5.266667\n","\n","  Test Loss: 9.567396\n","\n","  Mean Abs Loss: 5.314286\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3822.978083 \tValidation Loss: 1039.105946\n","  Epoch: 100 \tTraining Loss: 3666.181643 \tValidation Loss: 875.957543\n","  Epoch: 150 \tTraining Loss: 3629.629620 \tValidation Loss: 990.741901\n","  Epoch: 200 \tTraining Loss: 3461.320346 \tValidation Loss: 850.204739\n","  Epoch: 250 \tTraining Loss: 3570.360450 \tValidation Loss: 959.194457\n","  Epoch: 300 \tTraining Loss: 3454.028032 \tValidation Loss: 951.977853\n","Best Validation Loss: 607.175312 in epoch 288\n","Best Train Loss: 2724.364788 in epoch 248\n","  Test Loss: 8.898828\n","\n","  Mean Abs Loss: 4.800000\n","\n","  Test Loss: 8.653683\n","\n","  Mean Abs Loss: 4.590476\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4820.336599 \tValidation Loss: 1360.854177\n","  Epoch: 100 \tTraining Loss: 4303.347464 \tValidation Loss: 1089.998424\n","  Epoch: 150 \tTraining Loss: 4112.049887 \tValidation Loss: 1132.113633\n","  Epoch: 200 \tTraining Loss: 4086.533232 \tValidation Loss: 1088.567717\n","  Epoch: 250 \tTraining Loss: 3959.476865 \tValidation Loss: 1050.853270\n","  Epoch: 300 \tTraining Loss: 3886.262219 \tValidation Loss: 1077.091546\n","Best Validation Loss: 666.263568 in epoch 219\n","Best Train Loss: 3031.984978 in epoch 272\n","  Test Loss: 9.256379\n","\n","  Mean Abs Loss: 5.200000\n","\n","  Test Loss: 9.439549\n","\n","  Mean Abs Loss: 5.171429\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3807.231943 \tValidation Loss: 1025.290339\n","  Epoch: 100 \tTraining Loss: 3698.922450 \tValidation Loss: 996.293165\n","  Epoch: 150 \tTraining Loss: 3630.736236 \tValidation Loss: 993.677972\n","  Epoch: 200 \tTraining Loss: 3398.220352 \tValidation Loss: 908.637432\n","  Epoch: 250 \tTraining Loss: 3487.923895 \tValidation Loss: 952.486021\n","  Epoch: 300 \tTraining Loss: 3530.789426 \tValidation Loss: 960.842352\n","Best Validation Loss: 627.529686 in epoch 116\n","Best Train Loss: 3110.970524 in epoch 291\n","  Test Loss: 8.816223\n","\n","  Mean Abs Loss: 4.828571\n","\n","  Test Loss: 8.949314\n","\n","  Mean Abs Loss: 4.923810\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4917.664047 \tValidation Loss: 1361.915264\n","  Epoch: 100 \tTraining Loss: 4402.695662 \tValidation Loss: 1273.502480\n","  Epoch: 150 \tTraining Loss: 4066.878754 \tValidation Loss: 1124.219274\n","  Epoch: 200 \tTraining Loss: 3974.222328 \tValidation Loss: 870.338757\n","  Epoch: 250 \tTraining Loss: 3923.114974 \tValidation Loss: 1070.548738\n","  Epoch: 300 \tTraining Loss: 3914.980010 \tValidation Loss: 1100.364779\n","Best Validation Loss: 677.293007 in epoch 281\n","Best Train Loss: 3144.254140 in epoch 206\n","  Test Loss: 9.730526\n","\n","  Mean Abs Loss: 5.457143\n","\n","  Test Loss: 9.473079\n","\n","  Mean Abs Loss: 5.400000\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3844.840203 \tValidation Loss: 830.476640\n","  Epoch: 100 \tTraining Loss: 3702.905835 \tValidation Loss: 1001.388574\n","  Epoch: 150 \tTraining Loss: 3711.678234 \tValidation Loss: 967.600728\n","  Epoch: 200 \tTraining Loss: 3595.894263 \tValidation Loss: 979.604886\n","  Epoch: 250 \tTraining Loss: 3543.368832 \tValidation Loss: 996.150409\n","  Epoch: 300 \tTraining Loss: 3510.317074 \tValidation Loss: 987.098287\n","Best Validation Loss: 726.706808 in epoch 231\n","Best Train Loss: 2844.968588 in epoch 153\n","  Test Loss: 8.690787\n","\n","  Mean Abs Loss: 4.657143\n","\n","  Test Loss: 8.700519\n","\n","  Mean Abs Loss: 4.723810\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4983.589919 \tValidation Loss: 1399.988767\n","  Epoch: 100 \tTraining Loss: 4623.517155 \tValidation Loss: 1159.340010\n","  Epoch: 150 \tTraining Loss: 4240.876425 \tValidation Loss: 1002.367962\n","  Epoch: 200 \tTraining Loss: 4135.999761 \tValidation Loss: 887.549161\n","  Epoch: 250 \tTraining Loss: 3932.405032 \tValidation Loss: 1040.054345\n","  Epoch: 300 \tTraining Loss: 3878.571495 \tValidation Loss: 987.652480\n","Best Validation Loss: 766.067598 in epoch 283\n","Best Train Loss: 3554.350966 in epoch 208\n","  Test Loss: 9.345741\n","\n","  Mean Abs Loss: 5.476190\n","\n","  Test Loss: 8.836463\n","\n","  Mean Abs Loss: 4.952381\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3849.301109 \tValidation Loss: 855.999061\n","  Epoch: 100 \tTraining Loss: 3665.949284 \tValidation Loss: 1044.193644\n","  Epoch: 150 \tTraining Loss: 3563.695364 \tValidation Loss: 955.267957\n","  Epoch: 200 \tTraining Loss: 3534.358561 \tValidation Loss: 999.163402\n","  Epoch: 250 \tTraining Loss: 3537.990152 \tValidation Loss: 791.185225\n","  Epoch: 300 \tTraining Loss: 3529.868395 \tValidation Loss: 987.729005\n","Best Validation Loss: 752.195430 in epoch 204\n","Best Train Loss: 3186.928605 in epoch 261\n","  Test Loss: 8.762936\n","\n","  Mean Abs Loss: 4.780952\n","\n","  Test Loss: 8.834265\n","\n","  Mean Abs Loss: 4.800000\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4804.696237 \tValidation Loss: 1278.257731\n","  Epoch: 100 \tTraining Loss: 4311.131074 \tValidation Loss: 1177.786882\n","  Epoch: 150 \tTraining Loss: 3952.024616 \tValidation Loss: 1046.447815\n","  Epoch: 200 \tTraining Loss: 3954.910160 \tValidation Loss: 1047.915185\n","  Epoch: 250 \tTraining Loss: 3888.828958 \tValidation Loss: 879.658306\n","  Epoch: 300 \tTraining Loss: 3861.750985 \tValidation Loss: 1059.692674\n","Best Validation Loss: 684.829134 in epoch 272\n","Best Train Loss: 3541.152118 in epoch 284\n","  Test Loss: 8.936587\n","\n","  Mean Abs Loss: 5.066667\n","\n","  Test Loss: 8.983153\n","\n","  Mean Abs Loss: 5.114286\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3849.200339 \tValidation Loss: 856.282729\n","  Epoch: 100 \tTraining Loss: 3761.109600 \tValidation Loss: 988.294444\n","  Epoch: 150 \tTraining Loss: 3829.680719 \tValidation Loss: 982.930615\n","  Epoch: 200 \tTraining Loss: 3672.147932 \tValidation Loss: 755.562622\n","  Epoch: 250 \tTraining Loss: 3509.077042 \tValidation Loss: 971.936980\n","  Epoch: 300 \tTraining Loss: 3536.519079 \tValidation Loss: 992.469453\n","Best Validation Loss: 724.973771 in epoch 57\n","Best Train Loss: 2693.938188 in epoch 275\n","  Test Loss: 8.782309\n","\n","  Mean Abs Loss: 4.723810\n","\n","  Test Loss: 9.565250\n","\n","  Mean Abs Loss: 5.409524\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4804.830136 \tValidation Loss: 1170.238339\n","  Epoch: 100 \tTraining Loss: 4366.984499 \tValidation Loss: 1105.834323\n","  Epoch: 150 \tTraining Loss: 3971.702964 \tValidation Loss: 743.350708\n","  Epoch: 200 \tTraining Loss: 3941.965153 \tValidation Loss: 1041.099270\n","  Epoch: 250 \tTraining Loss: 3855.296883 \tValidation Loss: 1029.675204\n","  Epoch: 300 \tTraining Loss: 3749.549895 \tValidation Loss: 939.665611\n","Best Validation Loss: 743.350708 in epoch 150\n","Best Train Loss: 2998.840186 in epoch 259\n","  Test Loss: 8.850101\n","\n","  Mean Abs Loss: 4.895238\n","\n","  Test Loss: 8.745325\n","\n","  Mean Abs Loss: 4.828571\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3726.327453 \tValidation Loss: 1004.532990\n","  Epoch: 100 \tTraining Loss: 3577.315920 \tValidation Loss: 1020.470409\n","  Epoch: 150 \tTraining Loss: 3607.346011 \tValidation Loss: 985.682877\n","  Epoch: 200 \tTraining Loss: 3593.626714 \tValidation Loss: 985.176877\n","  Epoch: 250 \tTraining Loss: 3550.721382 \tValidation Loss: 986.006816\n","  Epoch: 300 \tTraining Loss: 3499.173926 \tValidation Loss: 995.407064\n","Best Validation Loss: 564.688509 in epoch 231\n","Best Train Loss: 2761.392364 in epoch 189\n","  Test Loss: 8.856827\n","\n","  Mean Abs Loss: 4.790476\n","\n","  Test Loss: 8.773493\n","\n","  Mean Abs Loss: 4.742857\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4853.681221 \tValidation Loss: 1332.376421\n","  Epoch: 100 \tTraining Loss: 4254.747317 \tValidation Loss: 1154.584048\n","  Epoch: 150 \tTraining Loss: 4032.809674 \tValidation Loss: 1035.852502\n","  Epoch: 200 \tTraining Loss: 4099.985073 \tValidation Loss: 1067.686093\n","  Epoch: 250 \tTraining Loss: 3921.454969 \tValidation Loss: 1079.894780\n","  Epoch: 300 \tTraining Loss: 3958.793380 \tValidation Loss: 854.330013\n","Best Validation Loss: 706.378308 in epoch 235\n","Best Train Loss: 3077.835323 in epoch 283\n","  Test Loss: 9.341712\n","\n","  Mean Abs Loss: 5.295238\n","\n","  Test Loss: 9.471041\n","\n","  Mean Abs Loss: 5.457143\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3977.110935 \tValidation Loss: 1022.373101\n","  Epoch: 100 \tTraining Loss: 3687.438110 \tValidation Loss: 984.733022\n","  Epoch: 150 \tTraining Loss: 2804.978122 \tValidation Loss: 1018.161539\n","  Epoch: 200 \tTraining Loss: 3544.911503 \tValidation Loss: 1005.885940\n","  Epoch: 250 \tTraining Loss: 3523.830895 \tValidation Loss: 939.159441\n","  Epoch: 300 \tTraining Loss: 3490.020120 \tValidation Loss: 981.779363\n","Best Validation Loss: 733.071585 in epoch 180\n","Best Train Loss: 2704.349089 in epoch 282\n","  Test Loss: 8.812714\n","\n","  Mean Abs Loss: 4.733333\n","\n","  Test Loss: 9.017456\n","\n","  Mean Abs Loss: 4.895238\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4926.496256 \tValidation Loss: 1236.369737\n","  Epoch: 100 \tTraining Loss: 4330.563419 \tValidation Loss: 1038.534873\n","  Epoch: 150 \tTraining Loss: 3996.292347 \tValidation Loss: 1019.867430\n","  Epoch: 200 \tTraining Loss: 3950.815258 \tValidation Loss: 1032.405611\n","  Epoch: 250 \tTraining Loss: 3859.103529 \tValidation Loss: 1018.224955\n","  Epoch: 300 \tTraining Loss: 3794.967998 \tValidation Loss: 883.144936\n","Best Validation Loss: 772.861071 in epoch 252\n","Best Train Loss: 3130.749525 in epoch 190\n","  Test Loss: 8.768172\n","\n","  Mean Abs Loss: 5.000000\n","\n","  Test Loss: 8.723330\n","\n","  Mean Abs Loss: 4.885714\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3932.412427 \tValidation Loss: 888.957916\n","  Epoch: 100 \tTraining Loss: 3717.034113 \tValidation Loss: 985.227484\n","  Epoch: 150 \tTraining Loss: 3670.799250 \tValidation Loss: 990.209883\n","  Epoch: 200 \tTraining Loss: 3623.881536 \tValidation Loss: 824.891038\n","  Epoch: 250 \tTraining Loss: 3469.437579 \tValidation Loss: 894.862604\n","  Epoch: 300 \tTraining Loss: 3484.987185 \tValidation Loss: 992.286316\n","Best Validation Loss: 657.236994 in epoch 90\n","Best Train Loss: 2745.750552 in epoch 186\n","  Test Loss: 8.832911\n","\n","  Mean Abs Loss: 4.676190\n","\n","  Test Loss: 9.204314\n","\n","  Mean Abs Loss: 5.057143\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4605.482227 \tValidation Loss: 1288.018489\n","  Epoch: 100 \tTraining Loss: 4148.651639 \tValidation Loss: 1155.168161\n","  Epoch: 150 \tTraining Loss: 3952.713606 \tValidation Loss: 1069.466587\n","  Epoch: 200 \tTraining Loss: 3903.400998 \tValidation Loss: 980.095155\n","  Epoch: 250 \tTraining Loss: 3922.110243 \tValidation Loss: 865.243657\n","  Epoch: 300 \tTraining Loss: 3827.668538 \tValidation Loss: 959.705225\n","Best Validation Loss: 791.468176 in epoch 233\n","Best Train Loss: 3541.431921 in epoch 243\n","  Test Loss: 8.938048\n","\n","  Mean Abs Loss: 5.038095\n","\n","  Test Loss: 8.911553\n","\n","  Mean Abs Loss: 5.076190\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3904.531387 \tValidation Loss: 1038.447497\n","  Epoch: 100 \tTraining Loss: 3773.402639 \tValidation Loss: 983.947766\n","  Epoch: 150 \tTraining Loss: 3835.646062 \tValidation Loss: 1031.397909\n","  Epoch: 200 \tTraining Loss: 3571.100521 \tValidation Loss: 976.590440\n","  Epoch: 250 \tTraining Loss: 3548.415442 \tValidation Loss: 971.799321\n","  Epoch: 300 \tTraining Loss: 3507.070253 \tValidation Loss: 963.867235\n","Best Validation Loss: 709.706119 in epoch 209\n","Best Train Loss: 2778.463248 in epoch 274\n","  Test Loss: 9.039197\n","\n","  Mean Abs Loss: 4.980952\n","\n","  Test Loss: 8.945648\n","\n","  Mean Abs Loss: 4.914286\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4737.285948 \tValidation Loss: 1324.851290\n","  Epoch: 100 \tTraining Loss: 4205.911802 \tValidation Loss: 963.407337\n","  Epoch: 150 \tTraining Loss: 3985.144158 \tValidation Loss: 1072.471317\n","  Epoch: 200 \tTraining Loss: 3917.903658 \tValidation Loss: 1037.991302\n","  Epoch: 250 \tTraining Loss: 3660.298306 \tValidation Loss: 886.880345\n","  Epoch: 300 \tTraining Loss: 3855.081119 \tValidation Loss: 1012.547928\n","Best Validation Loss: 803.947081 in epoch 290\n","Best Train Loss: 3428.024441 in epoch 279\n","  Test Loss: 8.700792\n","\n","  Mean Abs Loss: 4.761905\n","\n","  Test Loss: 8.686994\n","\n","  Mean Abs Loss: 4.847619\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3800.331586 \tValidation Loss: 1004.098997\n","  Epoch: 100 \tTraining Loss: 3720.548769 \tValidation Loss: 790.414006\n","  Epoch: 150 \tTraining Loss: 3621.712360 \tValidation Loss: 998.286133\n","  Epoch: 200 \tTraining Loss: 3510.715980 \tValidation Loss: 984.990541\n","  Epoch: 250 \tTraining Loss: 3493.505400 \tValidation Loss: 990.805766\n","  Epoch: 300 \tTraining Loss: 3499.941312 \tValidation Loss: 1005.288903\n","Best Validation Loss: 750.187229 in epoch 154\n","Best Train Loss: 2642.223211 in epoch 257\n","  Test Loss: 8.808743\n","\n","  Mean Abs Loss: 4.628571\n","\n","  Test Loss: 8.923330\n","\n","  Mean Abs Loss: 4.819048\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4863.911950 \tValidation Loss: 1330.749171\n","  Epoch: 100 \tTraining Loss: 4412.702940 \tValidation Loss: 1181.707129\n","  Epoch: 150 \tTraining Loss: 4221.078179 \tValidation Loss: 1180.720111\n","  Epoch: 200 \tTraining Loss: 3926.912024 \tValidation Loss: 1092.949634\n","  Epoch: 250 \tTraining Loss: 3983.075276 \tValidation Loss: 1076.565630\n","  Epoch: 300 \tTraining Loss: 4010.187355 \tValidation Loss: 945.388628\n","Best Validation Loss: 833.265005 in epoch 292\n","Best Train Loss: 3225.808089 in epoch 169\n","  Test Loss: 9.321872\n","\n","  Mean Abs Loss: 5.238095\n","\n","  Test Loss: 9.210777\n","\n","  Mean Abs Loss: 5.209524\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3860.535607 \tValidation Loss: 1046.226703\n","  Epoch: 100 \tTraining Loss: 3862.915306 \tValidation Loss: 1006.968519\n","  Epoch: 150 \tTraining Loss: 3651.466518 \tValidation Loss: 974.626130\n","  Epoch: 200 \tTraining Loss: 3617.116631 \tValidation Loss: 971.737846\n","  Epoch: 250 \tTraining Loss: 3481.797047 \tValidation Loss: 958.465859\n","  Epoch: 300 \tTraining Loss: 3471.683660 \tValidation Loss: 970.596885\n","Best Validation Loss: 735.606357 in epoch 18\n","Best Train Loss: 2720.496378 in epoch 249\n","  Test Loss: 8.735397\n","\n","  Mean Abs Loss: 4.780952\n","\n","  Test Loss: 8.558692\n","\n","  Mean Abs Loss: 4.809524\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4795.306863 \tValidation Loss: 1272.406623\n","  Epoch: 100 \tTraining Loss: 4346.089194 \tValidation Loss: 1101.507372\n","  Epoch: 150 \tTraining Loss: 4041.726920 \tValidation Loss: 1071.490285\n","  Epoch: 200 \tTraining Loss: 3870.217858 \tValidation Loss: 803.117593\n","  Epoch: 250 \tTraining Loss: 3936.269179 \tValidation Loss: 1013.738332\n","  Epoch: 300 \tTraining Loss: 3983.533603 \tValidation Loss: 1000.671472\n","Best Validation Loss: 759.432217 in epoch 290\n","Best Train Loss: 3354.490263 in epoch 93\n","  Test Loss: 9.652454\n","\n","  Mean Abs Loss: 5.523810\n","\n","  Test Loss: 8.938122\n","\n","  Mean Abs Loss: 5.133333\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3779.116442 \tValidation Loss: 1061.719679\n","  Epoch: 100 \tTraining Loss: 3824.921655 \tValidation Loss: 1000.837463\n","  Epoch: 150 \tTraining Loss: 3615.745067 \tValidation Loss: 978.173359\n","  Epoch: 200 \tTraining Loss: 3594.431967 \tValidation Loss: 955.321054\n","  Epoch: 250 \tTraining Loss: 3493.316649 \tValidation Loss: 903.310175\n","  Epoch: 300 \tTraining Loss: 3194.240166 \tValidation Loss: 973.961474\n","Best Validation Loss: 726.479677 in epoch 248\n","Best Train Loss: 2698.178966 in epoch 263\n","  Test Loss: 8.918958\n","\n","  Mean Abs Loss: 4.828571\n","\n","  Test Loss: 8.928703\n","\n","  Mean Abs Loss: 4.857143\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4806.474713 \tValidation Loss: 1286.395369\n","  Epoch: 100 \tTraining Loss: 4250.896359 \tValidation Loss: 1101.126740\n","  Epoch: 150 \tTraining Loss: 4046.820326 \tValidation Loss: 1068.877501\n","  Epoch: 200 \tTraining Loss: 3865.448295 \tValidation Loss: 1019.397451\n","  Epoch: 250 \tTraining Loss: 3814.995652 \tValidation Loss: 1011.908043\n","  Epoch: 300 \tTraining Loss: 3861.987102 \tValidation Loss: 805.855660\n","Best Validation Loss: 651.895124 in epoch 107\n","Best Train Loss: 3028.727537 in epoch 211\n","  Test Loss: 8.805744\n","\n","  Mean Abs Loss: 5.133333\n","\n","  Test Loss: 9.407963\n","\n","  Mean Abs Loss: 5.333333\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3667.523705 \tValidation Loss: 1020.684615\n","  Epoch: 100 \tTraining Loss: 3710.390093 \tValidation Loss: 1014.083867\n","  Epoch: 150 \tTraining Loss: 3600.128437 \tValidation Loss: 980.016256\n","  Epoch: 200 \tTraining Loss: 3551.126430 \tValidation Loss: 1002.244377\n","  Epoch: 250 \tTraining Loss: 3390.067556 \tValidation Loss: 1039.710048\n","  Epoch: 300 \tTraining Loss: 3549.388280 \tValidation Loss: 777.799987\n","Best Validation Loss: 579.254119 in epoch 135\n","Best Train Loss: 2741.352534 in epoch 95\n","  Test Loss: 9.026357\n","\n","  Mean Abs Loss: 5.142857\n","\n","  Test Loss: 8.811091\n","\n","  Mean Abs Loss: 4.971429\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4854.155612 \tValidation Loss: 1464.261078\n","  Epoch: 100 \tTraining Loss: 4386.710182 \tValidation Loss: 1247.019137\n","  Epoch: 150 \tTraining Loss: 4136.966363 \tValidation Loss: 1142.723489\n","  Epoch: 200 \tTraining Loss: 4003.658367 \tValidation Loss: 1020.519932\n","  Epoch: 250 \tTraining Loss: 3886.681257 \tValidation Loss: 1024.869731\n","  Epoch: 300 \tTraining Loss: 3932.619411 \tValidation Loss: 1017.590408\n","Best Validation Loss: 794.443930 in epoch 263\n","Best Train Loss: 3183.911818 in epoch 156\n","  Test Loss: 9.184970\n","\n","  Mean Abs Loss: 5.161905\n","\n","  Test Loss: 9.017941\n","\n","  Mean Abs Loss: 5.095238\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3800.021012 \tValidation Loss: 1006.859522\n","  Epoch: 100 \tTraining Loss: 3685.054672 \tValidation Loss: 974.502303\n","  Epoch: 150 \tTraining Loss: 3610.964821 \tValidation Loss: 978.671055\n","  Epoch: 200 \tTraining Loss: 3546.466847 \tValidation Loss: 966.685122\n","  Epoch: 250 \tTraining Loss: 3543.628618 \tValidation Loss: 945.454637\n","  Epoch: 300 \tTraining Loss: 3491.850932 \tValidation Loss: 925.478556\n","Best Validation Loss: 602.142472 in epoch 242\n","Best Train Loss: 3248.736174 in epoch 162\n","  Test Loss: 8.773776\n","\n","  Mean Abs Loss: 4.790476\n","\n","  Test Loss: 8.798973\n","\n","  Mean Abs Loss: 4.742857\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4923.845413 \tValidation Loss: 1250.926989\n","  Epoch: 100 \tTraining Loss: 4444.409155 \tValidation Loss: 1066.845277\n","  Epoch: 150 \tTraining Loss: 4232.137368 \tValidation Loss: 1016.799352\n","  Epoch: 200 \tTraining Loss: 3880.105615 \tValidation Loss: 1003.971226\n","  Epoch: 250 \tTraining Loss: 4046.871181 \tValidation Loss: 994.218968\n","  Epoch: 300 \tTraining Loss: 3820.168153 \tValidation Loss: 997.042513\n","Best Validation Loss: 740.834587 in epoch 273\n","Best Train Loss: 3091.669397 in epoch 197\n","  Test Loss: 8.901900\n","\n","  Mean Abs Loss: 4.828571\n","\n","  Test Loss: 8.750116\n","\n","  Mean Abs Loss: 4.828571\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3739.480069 \tValidation Loss: 846.367410\n","  Epoch: 100 \tTraining Loss: 3705.428877 \tValidation Loss: 982.068563\n","  Epoch: 150 \tTraining Loss: 3626.608566 \tValidation Loss: 961.619154\n","  Epoch: 200 \tTraining Loss: 3572.336676 \tValidation Loss: 985.210782\n","  Epoch: 250 \tTraining Loss: 3466.779400 \tValidation Loss: 979.018621\n","  Epoch: 300 \tTraining Loss: 3429.235352 \tValidation Loss: 718.941416\n","Best Validation Loss: 475.680452 in epoch 124\n","Best Train Loss: 2768.552947 in epoch 253\n","  Test Loss: 8.841902\n","\n","  Mean Abs Loss: 4.790476\n","\n","  Test Loss: 8.702524\n","\n","  Mean Abs Loss: 4.742857\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4011.613170 \tValidation Loss: 1332.453322\n","  Epoch: 100 \tTraining Loss: 4432.144726 \tValidation Loss: 1193.434935\n","  Epoch: 150 \tTraining Loss: 4049.092597 \tValidation Loss: 1121.857067\n","  Epoch: 200 \tTraining Loss: 3900.940200 \tValidation Loss: 1105.690556\n","  Epoch: 250 \tTraining Loss: 3853.884872 \tValidation Loss: 1060.965208\n","  Epoch: 300 \tTraining Loss: 3861.545962 \tValidation Loss: 1059.700160\n","Best Validation Loss: 815.811941 in epoch 241\n","Best Train Loss: 3065.916376 in epoch 245\n","  Test Loss: 9.271858\n","\n","  Mean Abs Loss: 5.085714\n","\n","  Test Loss: 9.467908\n","\n","  Mean Abs Loss: 5.142857\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3342.604496 \tValidation Loss: 1031.388508\n","  Epoch: 100 \tTraining Loss: 3623.970769 \tValidation Loss: 958.828058\n","  Epoch: 150 \tTraining Loss: 3685.373995 \tValidation Loss: 997.073146\n","  Epoch: 200 \tTraining Loss: 3607.328890 \tValidation Loss: 781.181587\n","  Epoch: 250 \tTraining Loss: 3520.360492 \tValidation Loss: 981.735342\n","  Epoch: 300 \tTraining Loss: 3495.254400 \tValidation Loss: 967.148200\n","Best Validation Loss: 605.540950 in epoch 175\n","Best Train Loss: 2825.677838 in epoch 133\n","  Test Loss: 8.855047\n","\n","  Mean Abs Loss: 4.866667\n","\n","  Test Loss: 9.146154\n","\n","  Mean Abs Loss: 5.009524\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4883.542950 \tValidation Loss: 1314.324859\n","  Epoch: 100 \tTraining Loss: 4338.725721 \tValidation Loss: 998.948287\n","  Epoch: 150 \tTraining Loss: 4051.384708 \tValidation Loss: 1063.300274\n","  Epoch: 200 \tTraining Loss: 3915.269284 \tValidation Loss: 992.680569\n","  Epoch: 250 \tTraining Loss: 3789.287500 \tValidation Loss: 1039.719763\n","  Epoch: 300 \tTraining Loss: 3754.177923 \tValidation Loss: 1060.050223\n","Best Validation Loss: 792.896514 in epoch 227\n","Best Train Loss: 3124.464025 in epoch 182\n","  Test Loss: 9.660079\n","\n","  Mean Abs Loss: 5.466667\n","\n","  Test Loss: 9.053541\n","\n","  Mean Abs Loss: 5.123810\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3803.763514 \tValidation Loss: 1001.208807\n","  Epoch: 100 \tTraining Loss: 3600.485462 \tValidation Loss: 985.235866\n","  Epoch: 150 \tTraining Loss: 3614.024549 \tValidation Loss: 966.516814\n","  Epoch: 200 \tTraining Loss: 3566.881700 \tValidation Loss: 730.177775\n","  Epoch: 250 \tTraining Loss: 3472.650011 \tValidation Loss: 974.884314\n","  Epoch: 300 \tTraining Loss: 3529.582520 \tValidation Loss: 959.998662\n","Best Validation Loss: 686.124412 in epoch 108\n","Best Train Loss: 2758.349541 in epoch 161\n","  Test Loss: 8.866746\n","\n","  Mean Abs Loss: 4.733333\n","\n","  Test Loss: 8.957898\n","\n","  Mean Abs Loss: 4.866667\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4781.589288 \tValidation Loss: 1286.211624\n","  Epoch: 100 \tTraining Loss: 4313.734979 \tValidation Loss: 1153.650860\n","  Epoch: 150 \tTraining Loss: 4081.453364 \tValidation Loss: 1062.999457\n","  Epoch: 200 \tTraining Loss: 3982.698910 \tValidation Loss: 1055.865798\n","  Epoch: 250 \tTraining Loss: 3942.687550 \tValidation Loss: 1030.030497\n","  Epoch: 300 \tTraining Loss: 3850.753222 \tValidation Loss: 1035.555307\n","Best Validation Loss: 713.501470 in epoch 128\n","Best Train Loss: 3406.945440 in epoch 87\n","  Test Loss: 9.369068\n","\n","  Mean Abs Loss: 5.295238\n","\n","  Test Loss: 8.652063\n","\n","  Mean Abs Loss: 4.923810\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3811.809964 \tValidation Loss: 1056.942047\n","  Epoch: 100 \tTraining Loss: 3618.948415 \tValidation Loss: 980.920474\n","  Epoch: 150 \tTraining Loss: 3687.284867 \tValidation Loss: 994.518219\n","  Epoch: 200 \tTraining Loss: 3563.169693 \tValidation Loss: 963.250650\n","  Epoch: 250 \tTraining Loss: 3455.798370 \tValidation Loss: 956.501106\n","  Epoch: 300 \tTraining Loss: 3555.450543 \tValidation Loss: 967.602416\n","Best Validation Loss: 715.624323 in epoch 232\n","Best Train Loss: 2823.389767 in epoch 155\n","  Test Loss: 8.944736\n","\n","  Mean Abs Loss: 4.990476\n","\n","  Test Loss: 8.755297\n","\n","  Mean Abs Loss: 4.857143\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4948.239463 \tValidation Loss: 1340.519182\n","  Epoch: 100 \tTraining Loss: 4273.530340 \tValidation Loss: 1196.420490\n","  Epoch: 150 \tTraining Loss: 3655.465648 \tValidation Loss: 1067.246278\n","  Epoch: 200 \tTraining Loss: 3926.318899 \tValidation Loss: 845.826206\n","  Epoch: 250 \tTraining Loss: 3939.137787 \tValidation Loss: 1073.741779\n","  Epoch: 300 \tTraining Loss: 3908.705443 \tValidation Loss: 1048.316345\n","Best Validation Loss: 795.981962 in epoch 247\n","Best Train Loss: 3506.224749 in epoch 275\n","  Test Loss: 8.923221\n","\n","  Mean Abs Loss: 4.819048\n","\n","  Test Loss: 8.856378\n","\n","  Mean Abs Loss: 4.780952\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3871.296655 \tValidation Loss: 998.614098\n","  Epoch: 100 \tTraining Loss: 3591.944913 \tValidation Loss: 833.244695\n","  Epoch: 150 \tTraining Loss: 3667.230345 \tValidation Loss: 1002.029573\n","  Epoch: 200 \tTraining Loss: 3555.551621 \tValidation Loss: 982.306429\n","  Epoch: 250 \tTraining Loss: 3506.308932 \tValidation Loss: 951.157585\n","  Epoch: 300 \tTraining Loss: 3429.742039 \tValidation Loss: 956.686611\n","Best Validation Loss: 720.444447 in epoch 237\n","Best Train Loss: 2650.086113 in epoch 202\n","  Test Loss: 8.603100\n","\n","  Mean Abs Loss: 4.666667\n","\n","  Test Loss: 8.865402\n","\n","  Mean Abs Loss: 4.704762\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 5108.637790 \tValidation Loss: 1498.334017\n","  Epoch: 100 \tTraining Loss: 4512.699012 \tValidation Loss: 1255.896870\n","  Epoch: 150 \tTraining Loss: 4111.851084 \tValidation Loss: 1170.149265\n","  Epoch: 200 \tTraining Loss: 4091.658902 \tValidation Loss: 1131.756862\n","  Epoch: 250 \tTraining Loss: 3874.575667 \tValidation Loss: 1074.158416\n","  Epoch: 300 \tTraining Loss: 3799.450318 \tValidation Loss: 1041.317529\n","Best Validation Loss: 706.812941 in epoch 192\n","Best Train Loss: 3643.810677 in epoch 280\n","  Test Loss: 9.291857\n","\n","  Mean Abs Loss: 5.257143\n","\n","  Test Loss: 9.736137\n","\n","  Mean Abs Loss: 5.666667\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3782.674723 \tValidation Loss: 990.166780\n","  Epoch: 100 \tTraining Loss: 3639.602772 \tValidation Loss: 926.383689\n","  Epoch: 150 \tTraining Loss: 3564.864514 \tValidation Loss: 996.139279\n","  Epoch: 200 \tTraining Loss: 3478.900730 \tValidation Loss: 1033.193730\n","  Epoch: 250 \tTraining Loss: 3621.176424 \tValidation Loss: 1005.376432\n","  Epoch: 300 \tTraining Loss: 3568.026394 \tValidation Loss: 898.389047\n","Best Validation Loss: 724.353814 in epoch 210\n","Best Train Loss: 2703.136184 in epoch 190\n","  Test Loss: 8.724707\n","\n","  Mean Abs Loss: 4.714286\n","\n","  Test Loss: 8.808097\n","\n","  Mean Abs Loss: 4.780952\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 5071.793903 \tValidation Loss: 1524.054028\n","  Epoch: 100 \tTraining Loss: 4382.058252 \tValidation Loss: 1198.515704\n","  Epoch: 150 \tTraining Loss: 4234.756693 \tValidation Loss: 1086.584308\n","  Epoch: 200 \tTraining Loss: 3935.723464 \tValidation Loss: 1080.779275\n","  Epoch: 250 \tTraining Loss: 3892.120241 \tValidation Loss: 1072.608191\n","  Epoch: 300 \tTraining Loss: 3936.147242 \tValidation Loss: 1006.669666\n","Best Validation Loss: 822.871455 in epoch 215\n","Best Train Loss: 3169.597890 in epoch 189\n","  Test Loss: 9.328411\n","\n","  Mean Abs Loss: 5.457143\n","\n","  Test Loss: 9.200954\n","\n","  Mean Abs Loss: 5.371429\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3858.573297 \tValidation Loss: 1003.485622\n","  Epoch: 100 \tTraining Loss: 3679.661432 \tValidation Loss: 1042.246970\n","  Epoch: 150 \tTraining Loss: 3605.430381 \tValidation Loss: 1009.828391\n","  Epoch: 200 \tTraining Loss: 3605.464165 \tValidation Loss: 964.137846\n","  Epoch: 250 \tTraining Loss: 3604.420654 \tValidation Loss: 956.348018\n","  Epoch: 300 \tTraining Loss: 3527.053511 \tValidation Loss: 954.817927\n","Best Validation Loss: 705.422200 in epoch 287\n","Best Train Loss: 2711.698284 in epoch 299\n","  Test Loss: 8.701516\n","\n","  Mean Abs Loss: 4.895238\n","\n","  Test Loss: 8.859516\n","\n","  Mean Abs Loss: 4.847619\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4748.412774 \tValidation Loss: 1221.203419\n","  Epoch: 100 \tTraining Loss: 4268.328852 \tValidation Loss: 1085.696977\n","  Epoch: 150 \tTraining Loss: 4054.727796 \tValidation Loss: 1037.911567\n","  Epoch: 200 \tTraining Loss: 4026.482459 \tValidation Loss: 1013.670677\n","  Epoch: 250 \tTraining Loss: 3848.084963 \tValidation Loss: 929.178189\n","  Epoch: 300 \tTraining Loss: 3885.210280 \tValidation Loss: 1015.571459\n","Best Validation Loss: 727.100929 in epoch 249\n","Best Train Loss: 3439.556451 in epoch 72\n","  Test Loss: 10.347824\n","\n","  Mean Abs Loss: 6.133333\n","\n","  Test Loss: 9.014607\n","\n","  Mean Abs Loss: 5.085714\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3820.960239 \tValidation Loss: 1004.907252\n","  Epoch: 100 \tTraining Loss: 3570.145436 \tValidation Loss: 810.499078\n","  Epoch: 150 \tTraining Loss: 3485.028402 \tValidation Loss: 957.152545\n","  Epoch: 200 \tTraining Loss: 3588.778668 \tValidation Loss: 941.451879\n","  Epoch: 250 \tTraining Loss: 3297.038966 \tValidation Loss: 961.039584\n","  Epoch: 300 \tTraining Loss: 3524.030399 \tValidation Loss: 811.706274\n","Best Validation Loss: 645.246625 in epoch 82\n","Best Train Loss: 2824.414169 in epoch 96\n","  Test Loss: 8.812097\n","\n","  Mean Abs Loss: 4.733333\n","\n","  Test Loss: 8.978956\n","\n","  Mean Abs Loss: 4.980952\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4802.909685 \tValidation Loss: 1164.287809\n","  Epoch: 100 \tTraining Loss: 4192.062638 \tValidation Loss: 1235.687632\n","  Epoch: 150 \tTraining Loss: 3963.730222 \tValidation Loss: 1094.889146\n","  Epoch: 200 \tTraining Loss: 3915.711802 \tValidation Loss: 1091.464969\n","  Epoch: 250 \tTraining Loss: 3782.224528 \tValidation Loss: 1068.461340\n","  Epoch: 300 \tTraining Loss: 3734.978978 \tValidation Loss: 1031.818870\n","Best Validation Loss: 787.623962 in epoch 171\n","Best Train Loss: 2965.534856 in epoch 294\n","  Test Loss: 8.969544\n","\n","  Mean Abs Loss: 4.990476\n","\n","  Test Loss: 9.563672\n","\n","  Mean Abs Loss: 5.390476\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3803.988167 \tValidation Loss: 1010.054527\n","  Epoch: 100 \tTraining Loss: 3640.617125 \tValidation Loss: 809.155749\n","  Epoch: 150 \tTraining Loss: 3696.876076 \tValidation Loss: 971.273991\n","  Epoch: 200 \tTraining Loss: 3524.007981 \tValidation Loss: 980.914059\n","  Epoch: 250 \tTraining Loss: 3652.251883 \tValidation Loss: 951.708787\n","  Epoch: 300 \tTraining Loss: 3549.764865 \tValidation Loss: 763.559259\n","Best Validation Loss: 617.711397 in epoch 108\n","Best Train Loss: 2819.981844 in epoch 151\n","  Test Loss: 8.756572\n","\n","  Mean Abs Loss: 4.571429\n","\n"," Test loss decreased (4.580952 --> 4.571429).  Saving model ...\n","  Test Loss: 8.967108\n","\n","  Mean Abs Loss: 5.085714\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4533.278316 \tValidation Loss: 1208.629278\n","  Epoch: 100 \tTraining Loss: 4080.527575 \tValidation Loss: 1110.527185\n","  Epoch: 150 \tTraining Loss: 3981.725271 \tValidation Loss: 1061.841451\n","  Epoch: 200 \tTraining Loss: 3857.737867 \tValidation Loss: 1075.420854\n","  Epoch: 250 \tTraining Loss: 3871.151102 \tValidation Loss: 1030.155911\n","  Epoch: 300 \tTraining Loss: 3831.448477 \tValidation Loss: 1025.516488\n","Best Validation Loss: 782.032981 in epoch 198\n","Best Train Loss: 3514.515405 in epoch 290\n","  Test Loss: 8.758646\n","\n","  Mean Abs Loss: 4.780952\n","\n","  Test Loss: 8.645214\n","\n","  Mean Abs Loss: 4.733333\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3910.295631 \tValidation Loss: 986.587430\n","  Epoch: 100 \tTraining Loss: 3998.462970 \tValidation Loss: 986.123905\n","  Epoch: 150 \tTraining Loss: 3572.833083 \tValidation Loss: 996.144124\n","  Epoch: 200 \tTraining Loss: 3600.733083 \tValidation Loss: 972.359239\n","  Epoch: 250 \tTraining Loss: 3570.141915 \tValidation Loss: 972.967006\n","  Epoch: 300 \tTraining Loss: 3471.535915 \tValidation Loss: 978.324233\n","Best Validation Loss: 560.972759 in epoch 159\n","Best Train Loss: 2823.297023 in epoch 148\n","  Test Loss: 8.804116\n","\n","  Mean Abs Loss: 4.742857\n","\n","  Test Loss: 8.708322\n","\n","  Mean Abs Loss: 4.800000\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4968.080239 \tValidation Loss: 1261.575519\n","  Epoch: 100 \tTraining Loss: 4378.232535 \tValidation Loss: 1033.006546\n","  Epoch: 150 \tTraining Loss: 4072.247961 \tValidation Loss: 968.751667\n","  Epoch: 200 \tTraining Loss: 3969.576451 \tValidation Loss: 1079.154388\n","  Epoch: 250 \tTraining Loss: 3916.064553 \tValidation Loss: 1068.196560\n","  Epoch: 300 \tTraining Loss: 3966.914513 \tValidation Loss: 1015.903864\n","Best Validation Loss: 792.560092 in epoch 186\n","Best Train Loss: 3119.601115 in epoch 176\n","  Test Loss: 9.523734\n","\n","  Mean Abs Loss: 5.447619\n","\n","  Test Loss: 9.265165\n","\n","  Mean Abs Loss: 5.219048\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3832.162665 \tValidation Loss: 995.317668\n","  Epoch: 100 \tTraining Loss: 3691.713698 \tValidation Loss: 990.057570\n","  Epoch: 150 \tTraining Loss: 3652.206719 \tValidation Loss: 1002.132887\n","  Epoch: 200 \tTraining Loss: 3573.444193 \tValidation Loss: 1008.493627\n","  Epoch: 250 \tTraining Loss: 3540.169831 \tValidation Loss: 877.949825\n","  Epoch: 300 \tTraining Loss: 3472.495939 \tValidation Loss: 972.123796\n","Best Validation Loss: 695.278312 in epoch 159\n","Best Train Loss: 2730.887145 in epoch 212\n","  Test Loss: 8.748326\n","\n","  Mean Abs Loss: 4.666667\n","\n","  Test Loss: 8.746318\n","\n","  Mean Abs Loss: 4.666667\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4750.586335 \tValidation Loss: 1235.442775\n","  Epoch: 100 \tTraining Loss: 4187.599919 \tValidation Loss: 1119.882556\n","  Epoch: 150 \tTraining Loss: 4148.218573 \tValidation Loss: 868.640688\n","  Epoch: 200 \tTraining Loss: 3992.830088 \tValidation Loss: 1030.131303\n","  Epoch: 250 \tTraining Loss: 3868.854567 \tValidation Loss: 1035.720086\n","  Epoch: 300 \tTraining Loss: 3745.486011 \tValidation Loss: 1018.876251\n","Best Validation Loss: 753.335670 in epoch 190\n","Best Train Loss: 3362.913103 in epoch 113\n","  Test Loss: 8.947873\n","\n","  Mean Abs Loss: 4.761905\n","\n","  Test Loss: 8.806468\n","\n","  Mean Abs Loss: 4.895238\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3752.311416 \tValidation Loss: 1041.739284\n","  Epoch: 100 \tTraining Loss: 3718.698216 \tValidation Loss: 992.902645\n","  Epoch: 150 \tTraining Loss: 3321.158260 \tValidation Loss: 964.301209\n","  Epoch: 200 \tTraining Loss: 3495.631747 \tValidation Loss: 969.314307\n","  Epoch: 250 \tTraining Loss: 3512.775686 \tValidation Loss: 970.854894\n","  Epoch: 300 \tTraining Loss: 3487.621167 \tValidation Loss: 957.870509\n","Best Validation Loss: 664.101894 in epoch 253\n","Best Train Loss: 2630.749175 in epoch 269\n","  Test Loss: 8.824915\n","\n","  Mean Abs Loss: 4.857143\n","\n","  Test Loss: 8.657287\n","\n","  Mean Abs Loss: 4.590476\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4824.127669 \tValidation Loss: 1089.077872\n","  Epoch: 100 \tTraining Loss: 4363.189550 \tValidation Loss: 1039.521622\n","  Epoch: 150 \tTraining Loss: 4204.269402 \tValidation Loss: 1090.031766\n","  Epoch: 200 \tTraining Loss: 3927.043485 \tValidation Loss: 1053.143457\n","  Epoch: 250 \tTraining Loss: 3882.125382 \tValidation Loss: 1061.280266\n","  Epoch: 300 \tTraining Loss: 3876.409419 \tValidation Loss: 1027.850962\n","Best Validation Loss: 731.272418 in epoch 293\n","Best Train Loss: 3651.555957 in epoch 207\n","  Test Loss: 8.817694\n","\n","  Mean Abs Loss: 4.752381\n","\n","  Test Loss: 9.049678\n","\n","  Mean Abs Loss: 5.190476\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3936.077639 \tValidation Loss: 1024.175481\n","  Epoch: 100 \tTraining Loss: 3781.652644 \tValidation Loss: 1031.141954\n","  Epoch: 150 \tTraining Loss: 3642.022805 \tValidation Loss: 819.427092\n","  Epoch: 200 \tTraining Loss: 3563.096986 \tValidation Loss: 975.423046\n","  Epoch: 250 \tTraining Loss: 3542.423881 \tValidation Loss: 973.284608\n","  Epoch: 300 \tTraining Loss: 3486.803080 \tValidation Loss: 951.106847\n","Best Validation Loss: 645.885873 in epoch 204\n","Best Train Loss: 2689.480553 in epoch 258\n","  Test Loss: 8.919721\n","\n","  Mean Abs Loss: 4.809524\n","\n","  Test Loss: 8.894670\n","\n","  Mean Abs Loss: 4.790476\n","\n","\n","Combined Dimension: 20\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 5129.010893 \tValidation Loss: 1439.950447\n","  Epoch: 100 \tTraining Loss: 4592.048163 \tValidation Loss: 1216.077071\n","  Epoch: 150 \tTraining Loss: 4113.956097 \tValidation Loss: 849.506542\n","  Epoch: 200 \tTraining Loss: 3884.367517 \tValidation Loss: 1083.544093\n","  Epoch: 250 \tTraining Loss: 3980.795164 \tValidation Loss: 1051.277821\n","  Epoch: 300 \tTraining Loss: 3891.362478 \tValidation Loss: 1042.453666\n","Best Validation Loss: 808.874585 in epoch 225\n","Best Train Loss: 3060.604767 in epoch 242\n","  Test Loss: 9.129167\n","\n","  Mean Abs Loss: 5.019048\n","\n","  Test Loss: 9.144429\n","\n","  Mean Abs Loss: 5.104762\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Liic3BnEcpyt","colab_type":"code","colab":{}},"source":["Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.001"],"execution_count":0,"outputs":[]}]}