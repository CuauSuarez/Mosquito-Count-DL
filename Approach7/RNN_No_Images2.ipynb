{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RNN_No_Images2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOaIpoxKKoyLWdOHZp4OmRK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"FUEm9QBvYyXr","colab_type":"code","colab":{}},"source":["import argparse\n","import yaml\n","import time\n","import datetime\n","import cv2\n","import numpy as np\n","import pandas as pd\n","import random\n","\n","from skimage import io, transform\n","import matplotlib.pyplot as plt\n","from scipy.ndimage import zoom\n","from scipy import ndimage, misc\n","\n","import torch\n","import torch.utils.data\n","from torch import nn, optim\n","from torch.nn import functional as F\n","from torchvision import datasets, transforms, utils\n","from torchvision.utils import save_image\n","from torch.utils.data import Dataset, DataLoader, TensorDataset\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import torchvision\n","import torch\n","from torch.autograd import Variable\n","import torch.optim as optim\n","\n","from google.colab import drive\n","import os\n","\n","import warnings\n","from collections import defaultdict\n","\n","from scipy import stats\n","from sklearn.metrics import mean_absolute_error"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vkVVfsA_Y9x5","colab_type":"code","outputId":"874e2726-07d7-43d8-9c13-7fbe7bd1f639","executionInfo":{"status":"ok","timestamp":1586982093135,"user_tz":300,"elapsed":71047,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":128}},"source":["drive.mount('/content/drive', force_remount = True)\n","os.chdir('/content/drive/My Drive/Mosquito-Tec/DA-RNN')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mAFzqb7tZAwM","colab_type":"code","colab":{}},"source":["# Open Data\n","\n","train_csv_file = '/content/drive/My Drive/Colab/mosquito/Final_Mosquito_train6_W.csv'\n","train_frame = pd.read_csv(train_csv_file)\n","\n","train_frame[\"TRAPSET\"] = pd.to_datetime(train_frame[\"TRAPSET\"])\n","train_frame[\"TRAPCOLLECT\"] = pd.to_datetime(train_frame[\"TRAPCOLLECT\"])\n","\n","train_frame[\"TRAPDAYS\"] = (train_frame[\"TRAPCOLLECT\"] - train_frame[\"TRAPSET\"]).dt.days\n","\n","train_frame[\"TRAPSET\"] = train_frame[\"TRAPSET\"].dt.week\n","train_frame[\"TRAPCOLLECT\"] = train_frame[\"TRAPCOLLECT\"].dt.week\n","\n","# Test data\n","\n","test_csv_file = '/content/drive/My Drive/Colab/mosquito/Final_Mosquito_test6_W.csv'\n","test_frame = pd.read_csv(test_csv_file)\n","\n","test_frame[\"TRAPSET\"] = pd.to_datetime(test_frame[\"TRAPSET\"])\n","test_frame[\"TRAPCOLLECT\"] = pd.to_datetime(test_frame[\"TRAPCOLLECT\"])\n","\n","test_frame[\"TRAPDAYS\"] = (test_frame[\"TRAPCOLLECT\"] - test_frame[\"TRAPSET\"]).dt.days\n","\n","test_frame[\"TRAPSET\"] = test_frame[\"TRAPSET\"].dt.week\n","test_frame[\"TRAPCOLLECT\"] = test_frame[\"TRAPCOLLECT\"].dt.week"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W8m5PuFCRrfe","colab_type":"code","colab":{}},"source":["non_temporal_columns = [\"OBJECTID\", \"X\", \"Y\", \"TRAPTYPE\", \"ATTRACTANTUSED\",\n","                        \"TRAPID\", \"LATITUDE\", \"LONGITUDE\", \"ADDRESS\", \"TOWN\",\n","                        \"STATE\", \"COUNTY\", \"TRAPSITE\", \"TRAPSET\", \"SETTIMEOFDAY\",\n","                        \"YEAR\", \"TRAPCOLLECT\", \"COLLECTTIMEOFDAY\", \"GENUS\",\n","                        \"SPECIES\", \"LIFESTAGE\", \"EGGSCOLLECTED\", \"LARVAECOLLECTED\",\n","                        \"PUPAECOLLECTED\", \"REPORTDATE\", \"TRAPDAYS\"]\n","\n","non_num_columns = [\"LATITUDE\", \"LONGITUDE\", \"TRAPDAYS\", \"TRAPSET\"]\n","\n","categorical_columnsT = [\"TRAPTYPE\", \"ATTRACTANTSUSED\", \"TRAPID\", \"ADDRESS\", \"TOWN\",\n","                        \"STATE\", \"COUNTY\", \"TRAPSITE\", \"TRAPSET\", \"SETTIMEOFDAY\",\n","                        \"TRAPCOLLECT\", \"COLLECTTIMEOFDAY\", \"GENUS\",\n","                        \"SPECIES\", \"LIFESTAGE\", \"EGGSCOLLECTED\", \"LARVAECOLLECTED\",\n","                        \"PUPAECOLLECTED\", \"REPORTDATE\"]\n","\n","categorical_columns = [\"TRAPTYPE\", \"ATTRACTANTSUSED\"]\n","\n","temporal_columns = [\"sunriseTime\", \"sunsetTime\", \"moonPhase\", \"precipIntensity\",\n","                    \"precipIntensityMax\", \"precipProbability\", \"temperatureHigh\",\n","                    \"temperatureHighTime\", \"temperatureLow\", \"temperatureLowTime\",\n","                    \"apparentTemperatureHigh\", \"apparentTemperatureHighTime\",\n","                    \"apparentTemperatureLow\", \"apparentTemperatureLowTime\",\n","                    \"dewPoint\", \"humidity\", \"pressure\", \"windSpeed\", \"windGust\",\n","                    \"windGustTime\", \"windBearing\", \"cloudCover\", \"uvIndex\", \n","                    \"uvIndexTime\", \"visibility\", \"temperatureMin\", \"temperatureMinTime\",\n","                    \"temperatureMax\", \"temperatureMaxTime\", \"apparentTemperatureMin\",\n","                    \"apparentTemperatureMinTime\", \"apparentTemperatureMax\", \"apparentTemperatureMaxTime\",\n","                    \"icon\", \"time\", \"precipIntensityMaxTime\", \"precipType\", \"summary\"] \n","\n","numerical_columns = [\"temperatureHigh\", \"uvIndex\", \"precipIntensityMaxTime\", \n","                     \"sunriseTime\", \"sunsetTime\", \"temperatureLow\", \"temperatureLowTime\",\n","                     \"temperatureHighTime\", \"humidity\"]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eKTqERJZEAvP","colab_type":"code","colab":{}},"source":["SEQ_LENGTH = 14\n","NUM_ENTRIES = train_frame.shape[0]\n","\n","column_means = {}\n","column_maxs = {}\n","column_mins = {}\n","\n","for col in numerical_columns:\n","    column_means[col] = 0\n","    column_maxs[col] = 0\n","    column_mins[col] = np.inf\n","\n","    for i in range(1, SEQ_LENGTH + 1):\n","        train_frame[col + str(i)].replace(to_replace=r'No*', value=np.nan, regex=True, inplace=True)\n","        test_frame[col + str(i)].replace(to_replace=r'No*', value=np.nan, regex=True, inplace=True)\n","\n","        if(train_frame[col + str(i)].dtype == np.dtype(object)):\n","            train_frame[col + str(i)] = train_frame[col + str(i)].astype(\"float64\", copy=\"False\")\n","\n","        if(test_frame[col + str(i)].dtype == np.dtype(object)):\n","            test_frame[col + str(i)] = test_frame[col + str(i)].astype(\"float64\", copy=\"False\")\n","\n","        cur_col = train_frame[col + str(i)]\n","\n","        col_mean = cur_col.mean()\n","\n","        train_frame[col + str(i)].replace(to_replace=np.nan, value=col_mean, inplace=True)\n","        test_frame[col + str(i)].replace(to_replace=np.nan, value=col_mean, inplace=True)\n","\n","        column_means[col] += col_mean * NUM_ENTRIES\n","        if cur_col.max() > column_maxs[col]:\n","            column_maxs[col] = cur_col.max()\n","\n","        if cur_col.min() < column_mins[col]:\n","            column_mins[col] = cur_col.min()\n","\n","    column_means[col] /= SEQ_LENGTH * NUM_ENTRIES\n","\n","    for i in range(1, SEQ_LENGTH + 1):\n","        test_frame[col + str(i)] = (test_frame[col + str(i)] - column_means[col]) / (column_maxs[col] - column_mins[col])\n","        train_frame[col + str(i)] = (train_frame[col + str(i)] - column_means[col]) / (column_maxs[col] - column_mins[col])\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aoXrda1jLe0z","colab_type":"code","colab":{}},"source":["# Standardize Numerical Columns (not Temporal)\n","\n","num_means = {}\n","num_maxs = {}\n","num_mins = {}\n","\n","for col in non_num_columns:\n","    cur_col = train_frame[col]\n","    num_means[col] = cur_col.mean()\n","    num_maxs[col] = cur_col.max()\n","    num_mins[col] = cur_col.min()\n","\n","    train_frame[col] = (cur_col - num_means[col]) / (num_maxs[col] - num_mins[col])\n","    test_frame[col] = (test_frame[col] - num_means[col]) / (num_maxs[col] - num_mins[col])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fT25ApU8vdWW","colab_type":"code","outputId":"2da3f132-6fd0-41e7-b58f-6ae80fbde0be","executionInfo":{"status":"ok","timestamp":1586982100577,"user_tz":300,"elapsed":643,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Convert categories\n","ats_train = []\n","ats_test = []\n","\n","for category in categorical_columns:\n","    train_frame[category] = train_frame[category].astype('category')\n","    test_frame[category] = test_frame[category].astype('category')\n","    ats_train.append(train_frame[category].cat.codes.values)\n","    ats_test.append(test_frame[category].cat.codes.values)\n","\n","train_cat = np.stack(ats_train, 1)\n","test_cat = np.stack(ats_test, 1)\n","\n","categorical_column_sizes = [len(train_frame[column].cat.categories) for column in categorical_columns]\n","embedding_sizes = [(col_size, min(50, (col_size + 1) // 2)) for col_size in categorical_column_sizes]\n","\n","print(embedding_sizes)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["[(3, 2), (4, 2)]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KOy39SYAGsgy","colab_type":"code","colab":{}},"source":["# Organize info by batches/sequences\n","# Shape [784, 14, 35] -> Train\n","\n","#train_features = np.zeros(shape=(train_frame.shape[0], SEQ_LENGTH, len(numerical_columns)+len(non_num_columns)))\n","#test_features = np.zeros(shape=(test_frame.shape[0], SEQ_LENGTH, len(numerical_columns)+len(non_num_columns)))\n","\n","train_features = np.zeros(shape=(train_frame.shape[0], SEQ_LENGTH, len(numerical_columns)))\n","test_features = np.zeros(shape=(test_frame.shape[0], SEQ_LENGTH, len(numerical_columns)))\n","\n","for i in range(1, SEQ_LENGTH + 1):\n","    for row, col in enumerate(numerical_columns):\n","        train_features[:, i-1, row] = train_frame[col + str(i)]\n","        test_features[:, i-1, row] = test_frame[col + str(i)]\n","\n","    '''    \n","    for row, col in enumerate(non_num_columns):\n","        train_features[:, i-1, row+len(numerical_columns)] = train_frame[col]\n","        test_features[:, i-1, row+len(numerical_columns)] = test_frame[col]\n","    '''"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J0aIP-SKMQ9R","colab_type":"code","colab":{}},"source":["# Organize info that is not temporal\n","train_y = train_frame[\"TOTAL\"].to_numpy()\n","test_y = test_frame[\"TOTAL\"].to_numpy()\n","\n","train_X = np.zeros(shape=(train_frame.shape[0], len(non_num_columns)))\n","test_X = np.zeros(shape=(test_frame.shape[0], len(non_num_columns)))\n","\n","for row, col in enumerate(non_num_columns):\n","    train_X[:, row] = train_frame[col]\n","    test_X[:, row] = test_frame[col]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4SLM3B2ZtcU4","colab_type":"text"},"source":["### Danger! Cuau Validation: DNR"]},{"cell_type":"code","metadata":{"id":"ICC4CaoOy4eh","colab_type":"code","outputId":"19551a01-6446-404a-fc35-40e69580c133","executionInfo":{"status":"ok","timestamp":1586982101983,"user_tz":300,"elapsed":412,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Create DataLoaders\n","# number of subprocesses to use for data loading\n","num_workers = 0\n","# how many samples per batch to load\n","batch_size = 16\n","# percentage of training set to use as validation\n","valid_size = 0.2\n","\n","\n","# obtain training indices that will be used for validation\n","num_train = len(train_X)\n","indices = list(range(num_train))\n","np.random.shuffle(indices)\n","split = int(np.floor(valid_size * num_train))\n","train_idx, valid_idx = indices[split:], indices[:split]\n","\n","print(len(train_idx), len(valid_idx))\n","\n","# define samplers for obtaining training and validation batches\n","train_sampler = SubsetRandomSampler(train_idx)\n","valid_sampler = SubsetRandomSampler(valid_idx)\n","\n","train_data = TensorDataset(torch.from_numpy(train_features), \n","                           torch.from_numpy(train_X), \n","                           torch.from_numpy(train_cat),\n","                           torch.from_numpy(train_y))\n","\n","test_data = TensorDataset(torch.from_numpy(test_features), \n","                          torch.from_numpy(test_X), \n","                          torch.from_numpy(test_cat),\n","                          torch.from_numpy(test_y))\n","\n","\n","# prepare data loaders (combine dataset and sampler)\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size,\n","    sampler = train_sampler, num_workers = num_workers, drop_last=True)\n","valid_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, \n","    sampler = valid_sampler, num_workers = num_workers, drop_last=True)\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size, \n","    num_workers = num_workers, shuffle = False, drop_last=True)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["596 149\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"i1VwYmtPtgGO","colab_type":"code","outputId":"19ab58ae-5bed-448a-867e-253019c42182","executionInfo":{"status":"ok","timestamp":1586981681621,"user_tz":300,"elapsed":596,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["# Create DataLoaders\n","# number of subprocesses to use for data loading\n","num_workers = 0\n","# how many samples per batch to load\n","batch_size = 4\n","# percentage of training set to use as validation\n","valid_size = 0.15\n","\n","\n","# obtain training indices that will be used for validation\n","num_train = len(train_X)\n","indices = list(range(num_train))\n","np.random.shuffle(indices)\n","split = int(np.floor(valid_size * num_train))\n","train_idx, valid_idx = indices[split:], indices[:split]\n","\n","print(len(train_idx), len(valid_idx))\n","\n","# define samplers for obtaining training and validation batches\n","train_sampler = SubsetRandomSampler(train_idx)\n","valid_sampler = SubsetRandomSampler(valid_idx)\n","\n","train_data = TensorDataset(torch.from_numpy(train_features[train_idx]), \n","                           torch.from_numpy(train_X[train_idx]), \n","                           torch.from_numpy(train_cat[train_idx]),\n","                           torch.from_numpy(train_y[train_idx]))\n","\n","valid_data = TensorDataset(torch.from_numpy(train_features[valid_idx]), \n","                           torch.from_numpy(train_X[valid_idx]), \n","                           torch.from_numpy(train_cat[valid_idx]),\n","                           torch.from_numpy(train_y[valid_idx]))\n","\n","test_data = TensorDataset(torch.from_numpy(test_features), \n","                          torch.from_numpy(test_X), \n","                          torch.from_numpy(test_cat),\n","                          torch.from_numpy(test_y))\n","\n","print(len(train_data), len(valid_data))\n","\n","# prepare data loaders (combine dataset and sampler)\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n","    num_workers=num_workers, shuffle=True, drop_last=True)\n","valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, \n","    num_workers=num_workers, shuffle=True, drop_last=True)\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n","    num_workers=num_workers, shuffle=False, drop_last=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["634 111\n","634 111\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JBf5h7sS3nVc","colab_type":"code","colab":{}},"source":["dataiter = iter(train_loader)\n","sample_features, sample_X, sample_cat, sample_y = dataiter.next()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M9CE7bQE36vD","colab_type":"code","outputId":"9315b42a-e342-4ef3-a6ca-49bb2556ecb6","executionInfo":{"status":"ok","timestamp":1586904168421,"user_tz":300,"elapsed":381,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":90}},"source":["print(sample_features.shape)\n","print(sample_X.shape)\n","print(sample_cat.shape)\n","print(sample_y.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["torch.Size([16, 14, 5])\n","torch.Size([16, 3])\n","torch.Size([16, 2])\n","torch.Size([16])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uwzut-CseGO3","colab_type":"code","outputId":"78899864-2af6-4c86-a9c6-8e3b1ed405f8","executionInfo":{"status":"ok","timestamp":1586982104853,"user_tz":300,"elapsed":430,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# First checking if GPU is available\n","train_on_gpu = torch.cuda.is_available()\n","\n","if(train_on_gpu):\n","    print('Training on GPU.')\n","else:\n","    print('No GPU available, training on CPU.')\n","\n","#train_on_gpu = False"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Training on GPU.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fKaB5Hx1377j","colab_type":"code","colab":{}},"source":["class MosquitoLSTM(nn.Module):\n","    def __init__(self, num_numerical_columns, hidden_dim, n_layers):\n","        super(MosquitoLSTM, self).__init__()\n","\n","        self.n_layers = n_layers\n","        self.hidden_dim = hidden_dim\n","\n","        '''\n","        embed_size = 0\n","        for embed in embedding_sizes:\n","            embed_size += embed[1]\n","\n","        self.all_embeddings = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in embedding_sizes])\n","        '''\n","\n","        # For LSTM\n","        self.lstm = nn.LSTM(num_numerical_columns, hidden_dim, n_layers,\n","                            dropout=0.3, batch_first=True)\n","        \n","        # dropout layer\n","        self.dropout = nn.Dropout(0.3)\n","\n","        self.dense = nn.Sequential(\n","            \n","            nn.Linear(hidden_dim, 1)\n","        )\n","        self.ReLU = nn.ReLU()\n","\n","    def forward(self, X_temp, hidden):\n","        lstm_out, hidden = self.lstm(X_temp, hidden)\n","    \n","        # stack up lstm outputs\n","        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n","        print(lstm_out.shape)\n","        # dropout and fully-connected layer\n","        out = self.dense(lstm_out)\n","        # sigmoid function\n","        out = self.ReLU(out)\n","        \n","        print(out.shape)\n","        # reshape to be batch_size first\n","        out = out.view(batch_size, -1)\n","        print(out.shape)\n","        out = out[:, -1]\n","        print(out.shape)\n","\n","\n","\n","        return out, hidden\n","\n","    def init_hidden(self, batch_size):\n","        ''' Initializes hidden state '''\n","        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n","        # initialized to zero, for hidden state and cell state of LSTM\n","        weight = next(self.parameters()).data\n","        \n","        if (train_on_gpu):\n","            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n","                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n","        else:\n","            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n","                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n","        \n","        return hidden\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MO7LlGaj7oIe","colab_type":"code","colab":{}},"source":["class MosquitoRedux(nn.Module):\n","    def __init__(self, \n","                 lstm_columns, lstm_hidden_dim, lstm_n_layers,\n","                 num_columns, num_hidden_dim,\n","                 embedding_sizes, cat_hidden_dim):\n","        super(MosquitoRedux, self).__init__()\n","\n","        self.lstm_n_layers = lstm_n_layers\n","        self.lstm_hidden_dim = lstm_hidden_dim\n","\n","        self.num_hidden_dim = num_hidden_dim\n","        self.cat_hidden_dim = cat_hidden_dim\n","\n","        embed_size = 0\n","        for embed in embedding_sizes:\n","            embed_size += embed[1]\n","\n","        self.all_embeddings = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in embedding_sizes])\n","\n","        # For LSTM\n","        self.lstm = nn.LSTM(lstm_columns, lstm_hidden_dim, lstm_n_layers,\n","                            dropout=0.3, batch_first=True)\n","        \n","        # LSTM dense layer\n","        self.lstm_dense = nn.Sequential(\n","            nn.BatchNorm1d(lstm_hidden_dim),\n","            nn.Dropout(0.3),\n","            nn.Linear(lstm_hidden_dim, 1),\n","            nn.ReLU()\n","        )\n","        \n","        self.num_dense = nn.Sequential(\n","            nn.Dropout(0.3),\n","            nn.Linear(num_columns, num_hidden_dim),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm1d(num_hidden_dim),\n","            nn.Dropout(0.3),\n","            nn.Linear(num_hidden_dim, num_hidden_dim),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm1d(num_hidden_dim),\n","            nn.Dropout(0.3),\n","            nn.Linear(num_hidden_dim, num_hidden_dim),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm1d(num_hidden_dim),\n","            nn.Linear(num_hidden_dim, 1),\n","            nn.ReLU()\n","        )\n","\n","        self.cat_dense = nn.Sequential(\n","            nn.Dropout(0.3),\n","            nn.Linear(embed_size, cat_hidden_dim),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm1d(cat_hidden_dim),\n","            nn.Dropout(0.3),\n","            nn.Linear(cat_hidden_dim, cat_hidden_dim),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm1d(cat_hidden_dim),\n","            nn.Dropout(0.3),\n","            nn.Linear(cat_hidden_dim, cat_hidden_dim),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm1d(cat_hidden_dim),\n","            nn.Linear(cat_hidden_dim, 1),\n","            nn.ReLU()\n","        )\n","\n","    def forward(self, X_temp, hidden, X_num, X_cat):\n","        # Forward lstm\n","        lstm_out, hidden = self.lstm(X_temp, hidden)\n","        # stack up lstm outputs\n","        lstm_out = lstm_out.contiguous().view(-1, self.lstm_hidden_dim)\n","        # dropout and fully-connected layer\n","        lstm_out = self.lstm_dense(lstm_out)\n","      \n","        # reshape to be batch_size first\n","        lstm_out = lstm_out.view(batch_size, -1)\n","        lstm_out = lstm_out[:, -1]\n","        lstm_out = lstm_out.view(batch_size, -1)\n","\n","        # Forward num\n","        num_out = self.num_dense(X_num)\n","\n","        # Forward cat\n","        embeddings = []\n","        for i, e in enumerate(self.all_embeddings):\n","            embed = e(X_cat[:, i])\n","            embeddings.append(embed)\n","\n","        cat_out = torch.cat(embeddings, 1)\n","        cat_out = self.cat_dense(cat_out)\n","\n","        return lstm_out, hidden, num_out, cat_out\n","\n","    def init_hidden(self, batch_size):\n","        ''' Initializes hidden state '''\n","        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n","        # initialized to zero, for hidden state and cell state of LSTM\n","        weight = next(self.parameters()).data\n","        \n","        if (train_on_gpu):\n","            hidden = (weight.new(self.lstm_n_layers, batch_size, self.lstm_hidden_dim).zero_().cuda(),\n","                  weight.new(self.lstm_n_layers, batch_size, self.lstm_hidden_dim).zero_().cuda())\n","        else:\n","            hidden = (weight.new(self.lstm_n_layers, batch_size, self.lstm_hidden_dim).zero_(),\n","                      weight.new(self.lstm_n_layers, batch_size, self.lstm_hidden_dim).zero_())\n","        \n","        return hidden"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MjjvR7ARfbN5","colab_type":"code","outputId":"a5b56b1b-30e6-4da6-c2f7-e72d484b8f19","executionInfo":{"status":"ok","timestamp":1586933793908,"user_tz":300,"elapsed":813,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Instantiate the model w/ hyperparams\n","a_h = 3\n","\n","lstm_hidden_dim = np.int(len(train_idx) / (a_h * len(numerical_columns) + 1))\n","print(lstm_hidden_dim)\n","lstm_n_layers = 2\n","\n","num_hidden_dim = 15\n","cat_hidden_dim = 15\n","\n","#net = MosquitoLSTM(len(numerical_columns), hidden_dim, n_layers).double()\n","net = MosquitoRedux(\n","    len(numerical_columns), lstm_hidden_dim, lstm_n_layers,\n","    len(non_num_columns), num_hidden_dim,\n","    embedding_sizes, cat_hidden_dim\n",").double()\n","\n","weight_lstm = 1.0\n","weight_num = 2.0\n","weight_cat = 1.0"],"execution_count":0,"outputs":[{"output_type":"stream","text":["21\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"caOIYuZ5foOw","colab_type":"code","colab":{}},"source":["lr = 0.001\n","\n","criterion = nn.MSELoss()\n","#optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n","optimizer = torch.optim.RMSprop(net.parameters(), lr=lr)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jzkXPhqufsic","colab_type":"code","outputId":"efaf72ba-3e55-48c5-9b5d-9948f9ed06ef","executionInfo":{"status":"ok","timestamp":1586923700434,"user_tz":300,"elapsed":168759,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# training params\n","\n","epochs = 400 # 3-4 is approx where I noticed the validation loss stop decreasing\n","\n","counter = 0\n","print_every = 10\n","clip = 10 # gradient clipping\n","\n","# move model to GPU, if available\n","if(train_on_gpu):\n","    net.cuda()\n","\n","net.train()\n","# train for some number of epochs\n","valid_loss_min = np.Inf\n","\n","for epoch in range(epochs):\n","    # initialize hidden state\n","    h = net.init_hidden(batch_size)\n","\n","    train_loss = 0.0\n","    valid_loss = 0.0\n","\n","    net.train()\n","\n","    # batch loop\n","    for X_temp, X_num, X_cat, labels in train_loader:\n","        counter += 1\n","\n","        if(train_on_gpu):\n","            X_temp, X_num, X_cat, labels = X_temp.cuda(), X_num.cuda(), X_cat.type(torch.LongTensor).cuda(), labels.cuda()\n","\n","        # Creating new variables for the hidden state, otherwise\n","        # we'd backprop through the entire training history\n","        h = tuple([each.data for each in h])\n","        # zero accumulated gradients\n","        net.zero_grad()\n","\n","        # get the output from the model\n","        lstm_output, h, num_output, cat_output = net(X_temp, h, X_num, X_cat)\n","\n","        output = (weight_lstm*lstm_output + weight_num*num_output + weight_cat*cat_output) \n","\n","        # calculate the loss and perform backprop\n","        loss = criterion(output.squeeze(), labels.double())\n","        loss.backward()\n","        train_loss += loss.item()\n","        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","        nn.utils.clip_grad_norm_(net.parameters(), clip)\n","        optimizer.step()\n","\n","    net.eval()\n","\n","    val_h = net.init_hidden(batch_size)\n","\n","    for X_temp, X_num, X_cat, labels in valid_loader:\n","        \n","        \n","        if(train_on_gpu):\n","            X_temp, X_num, X_cat, labels = X_temp.cuda(), X_num.cuda(), X_cat.type(torch.LongTensor).cuda(), labels.cuda()\n","\n","        # Creating new variables for the hidden state, otherwise\n","        # we'd backprop through the entire training history\n","        val_h = tuple([each.data for each in val_h])\n","\n","        # get the output from the model\n","        lstm_output, h, num_output, cat_output = net(X_temp, h, X_num, X_cat)\n","\n","        # calculate the loss and perform backprop\n","        output = (weight_lstm*lstm_output + weight_num*num_output + weight_cat*cat_output) \n","        loss = criterion(output.squeeze(), labels.double())\n","        valid_loss += loss.item()\n","\n","    # save model if validation loss has decreased\n","    if valid_loss <= valid_loss_min:\n","        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","        valid_loss_min,\n","        valid_loss))\n","        torch.save(net.state_dict(), 'lstm_test.pt')\n","        valid_loss_min = valid_loss\n","        \n","    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n","            epoch, train_loss, valid_loss))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Validation loss decreased (inf --> 1063.810599).  Saving model ...\n","Epoch: 0 \tTraining Loss: 4902.986458 \tValidation Loss: 1063.810599\n","Validation loss decreased (1063.810599 --> 1010.740524).  Saving model ...\n","Epoch: 1 \tTraining Loss: 4528.126603 \tValidation Loss: 1010.740524\n","Epoch: 2 \tTraining Loss: 4321.250742 \tValidation Loss: 1019.153390\n","Validation loss decreased (1010.740524 --> 790.559787).  Saving model ...\n","Epoch: 3 \tTraining Loss: 4298.738146 \tValidation Loss: 790.559787\n","Validation loss decreased (790.559787 --> 788.113032).  Saving model ...\n","Epoch: 4 \tTraining Loss: 4151.630487 \tValidation Loss: 788.113032\n","Epoch: 5 \tTraining Loss: 4001.181914 \tValidation Loss: 942.267813\n","Epoch: 6 \tTraining Loss: 4183.231023 \tValidation Loss: 953.540055\n","Epoch: 7 \tTraining Loss: 4021.229664 \tValidation Loss: 910.900171\n","Epoch: 8 \tTraining Loss: 4069.782589 \tValidation Loss: 942.354421\n","Epoch: 9 \tTraining Loss: 4039.964342 \tValidation Loss: 968.322827\n","Epoch: 10 \tTraining Loss: 3960.657928 \tValidation Loss: 859.746226\n","Epoch: 11 \tTraining Loss: 3992.804889 \tValidation Loss: 962.385893\n","Epoch: 12 \tTraining Loss: 4073.931558 \tValidation Loss: 916.914474\n","Epoch: 13 \tTraining Loss: 4020.756905 \tValidation Loss: 945.938688\n","Epoch: 14 \tTraining Loss: 4063.930741 \tValidation Loss: 976.674579\n","Epoch: 15 \tTraining Loss: 4040.273404 \tValidation Loss: 892.743500\n","Epoch: 16 \tTraining Loss: 3139.971909 \tValidation Loss: 954.365069\n","Epoch: 17 \tTraining Loss: 4060.167099 \tValidation Loss: 953.029332\n","Epoch: 18 \tTraining Loss: 4059.023678 \tValidation Loss: 915.728428\n","Epoch: 19 \tTraining Loss: 3963.972188 \tValidation Loss: 956.412105\n","Epoch: 20 \tTraining Loss: 3917.386838 \tValidation Loss: 892.234124\n","Epoch: 21 \tTraining Loss: 3974.464383 \tValidation Loss: 939.849364\n","Epoch: 22 \tTraining Loss: 3922.569558 \tValidation Loss: 951.745127\n","Epoch: 23 \tTraining Loss: 3838.124238 \tValidation Loss: 907.878958\n","Epoch: 24 \tTraining Loss: 3973.836096 \tValidation Loss: 928.306695\n","Epoch: 25 \tTraining Loss: 3925.872197 \tValidation Loss: 1010.771514\n","Epoch: 26 \tTraining Loss: 4072.696164 \tValidation Loss: 966.208509\n","Epoch: 27 \tTraining Loss: 3929.462006 \tValidation Loss: 936.945429\n","Epoch: 28 \tTraining Loss: 3881.088143 \tValidation Loss: 935.232045\n","Epoch: 29 \tTraining Loss: 3815.092979 \tValidation Loss: 933.868704\n","Validation loss decreased (788.113032 --> 778.794350).  Saving model ...\n","Epoch: 30 \tTraining Loss: 3926.549424 \tValidation Loss: 778.794350\n","Epoch: 31 \tTraining Loss: 3892.851435 \tValidation Loss: 911.501090\n","Epoch: 32 \tTraining Loss: 3829.624486 \tValidation Loss: 936.745191\n","Epoch: 33 \tTraining Loss: 3779.653069 \tValidation Loss: 931.099982\n","Epoch: 34 \tTraining Loss: 3516.694271 \tValidation Loss: 930.604605\n","Epoch: 35 \tTraining Loss: 3835.122952 \tValidation Loss: 962.641713\n","Epoch: 36 \tTraining Loss: 3825.273889 \tValidation Loss: 915.044680\n","Epoch: 37 \tTraining Loss: 3844.533442 \tValidation Loss: 922.052588\n","Epoch: 38 \tTraining Loss: 3808.159224 \tValidation Loss: 897.883912\n","Epoch: 39 \tTraining Loss: 3793.423553 \tValidation Loss: 924.209021\n","Epoch: 40 \tTraining Loss: 3769.365939 \tValidation Loss: 816.985080\n","Epoch: 41 \tTraining Loss: 3787.416049 \tValidation Loss: 887.653672\n","Epoch: 42 \tTraining Loss: 3850.656385 \tValidation Loss: 920.602154\n","Epoch: 43 \tTraining Loss: 3738.353130 \tValidation Loss: 896.850612\n","Validation loss decreased (778.794350 --> 591.720069).  Saving model ...\n","Epoch: 44 \tTraining Loss: 3823.879824 \tValidation Loss: 591.720069\n","Epoch: 45 \tTraining Loss: 3697.565320 \tValidation Loss: 941.674602\n","Epoch: 46 \tTraining Loss: 3837.695961 \tValidation Loss: 924.278358\n","Epoch: 47 \tTraining Loss: 3778.464097 \tValidation Loss: 896.809567\n","Epoch: 48 \tTraining Loss: 3555.551868 \tValidation Loss: 899.409208\n","Epoch: 49 \tTraining Loss: 3806.429891 \tValidation Loss: 710.097916\n","Epoch: 50 \tTraining Loss: 3845.138778 \tValidation Loss: 886.939085\n","Epoch: 51 \tTraining Loss: 3713.275382 \tValidation Loss: 906.733160\n","Epoch: 52 \tTraining Loss: 3830.486929 \tValidation Loss: 801.166579\n","Epoch: 53 \tTraining Loss: 3789.678678 \tValidation Loss: 910.083177\n","Epoch: 54 \tTraining Loss: 3805.242171 \tValidation Loss: 904.318094\n","Epoch: 55 \tTraining Loss: 3712.499105 \tValidation Loss: 939.002967\n","Epoch: 56 \tTraining Loss: 3706.938634 \tValidation Loss: 822.772953\n","Epoch: 57 \tTraining Loss: 3799.861291 \tValidation Loss: 877.054421\n","Epoch: 58 \tTraining Loss: 3727.523757 \tValidation Loss: 855.292781\n","Epoch: 59 \tTraining Loss: 3637.569505 \tValidation Loss: 739.856807\n","Epoch: 60 \tTraining Loss: 3746.457208 \tValidation Loss: 894.757579\n","Epoch: 61 \tTraining Loss: 3761.585555 \tValidation Loss: 830.044012\n","Epoch: 62 \tTraining Loss: 3706.307010 \tValidation Loss: 907.800647\n","Epoch: 63 \tTraining Loss: 3728.783246 \tValidation Loss: 921.097553\n","Epoch: 64 \tTraining Loss: 3666.874648 \tValidation Loss: 931.827210\n","Epoch: 65 \tTraining Loss: 3773.551674 \tValidation Loss: 909.992542\n","Epoch: 66 \tTraining Loss: 3702.316191 \tValidation Loss: 958.139127\n","Epoch: 67 \tTraining Loss: 3722.387655 \tValidation Loss: 959.816195\n","Epoch: 68 \tTraining Loss: 3672.287460 \tValidation Loss: 923.672115\n","Epoch: 69 \tTraining Loss: 3701.387339 \tValidation Loss: 912.007884\n","Epoch: 70 \tTraining Loss: 3638.101025 \tValidation Loss: 910.381558\n","Epoch: 71 \tTraining Loss: 3709.616767 \tValidation Loss: 919.665182\n","Epoch: 72 \tTraining Loss: 3649.086501 \tValidation Loss: 919.311700\n","Epoch: 73 \tTraining Loss: 3653.564676 \tValidation Loss: 925.249300\n","Epoch: 74 \tTraining Loss: 3677.197416 \tValidation Loss: 804.082561\n","Epoch: 75 \tTraining Loss: 3618.983928 \tValidation Loss: 924.193861\n","Epoch: 76 \tTraining Loss: 3699.573488 \tValidation Loss: 924.935913\n","Epoch: 77 \tTraining Loss: 3642.728597 \tValidation Loss: 921.201771\n","Epoch: 78 \tTraining Loss: 3652.893454 \tValidation Loss: 902.159388\n","Epoch: 79 \tTraining Loss: 3667.545241 \tValidation Loss: 903.247526\n","Epoch: 80 \tTraining Loss: 3691.709887 \tValidation Loss: 921.190952\n","Epoch: 81 \tTraining Loss: 3748.633995 \tValidation Loss: 910.733033\n","Epoch: 82 \tTraining Loss: 3799.552397 \tValidation Loss: 922.346110\n","Epoch: 83 \tTraining Loss: 3670.691268 \tValidation Loss: 911.055476\n","Epoch: 84 \tTraining Loss: 3759.329491 \tValidation Loss: 831.196758\n","Epoch: 85 \tTraining Loss: 3715.411835 \tValidation Loss: 913.852533\n","Epoch: 86 \tTraining Loss: 3282.234081 \tValidation Loss: 927.197403\n","Epoch: 87 \tTraining Loss: 3660.966221 \tValidation Loss: 937.666703\n","Epoch: 88 \tTraining Loss: 3705.784096 \tValidation Loss: 843.550976\n","Epoch: 89 \tTraining Loss: 3625.987921 \tValidation Loss: 948.693986\n","Epoch: 90 \tTraining Loss: 3690.189483 \tValidation Loss: 912.741569\n","Epoch: 91 \tTraining Loss: 3633.442380 \tValidation Loss: 793.465999\n","Epoch: 92 \tTraining Loss: 3630.126814 \tValidation Loss: 929.162555\n","Epoch: 93 \tTraining Loss: 3645.351647 \tValidation Loss: 921.577764\n","Epoch: 94 \tTraining Loss: 3648.943836 \tValidation Loss: 894.533381\n","Epoch: 95 \tTraining Loss: 3693.441114 \tValidation Loss: 921.648583\n","Epoch: 96 \tTraining Loss: 3738.272412 \tValidation Loss: 920.143394\n","Epoch: 97 \tTraining Loss: 3658.535249 \tValidation Loss: 905.608811\n","Epoch: 98 \tTraining Loss: 3666.129216 \tValidation Loss: 905.274010\n","Epoch: 99 \tTraining Loss: 3441.777583 \tValidation Loss: 819.735320\n","Epoch: 100 \tTraining Loss: 3679.638255 \tValidation Loss: 851.920358\n","Epoch: 101 \tTraining Loss: 3705.595721 \tValidation Loss: 926.534809\n","Epoch: 102 \tTraining Loss: 3706.040590 \tValidation Loss: 916.556420\n","Epoch: 103 \tTraining Loss: 3640.544424 \tValidation Loss: 925.670826\n","Epoch: 104 \tTraining Loss: 3694.157592 \tValidation Loss: 902.804939\n","Epoch: 105 \tTraining Loss: 3671.265869 \tValidation Loss: 911.449376\n","Epoch: 106 \tTraining Loss: 3600.695581 \tValidation Loss: 917.226483\n","Epoch: 107 \tTraining Loss: 3597.463141 \tValidation Loss: 777.557816\n","Epoch: 108 \tTraining Loss: 3564.609682 \tValidation Loss: 913.159427\n","Epoch: 109 \tTraining Loss: 3536.765363 \tValidation Loss: 908.219703\n","Epoch: 110 \tTraining Loss: 3574.456269 \tValidation Loss: 927.243309\n","Epoch: 111 \tTraining Loss: 3695.334839 \tValidation Loss: 917.889419\n","Epoch: 112 \tTraining Loss: 3617.250422 \tValidation Loss: 906.111012\n","Epoch: 113 \tTraining Loss: 3602.880812 \tValidation Loss: 915.932495\n","Epoch: 114 \tTraining Loss: 3576.340550 \tValidation Loss: 926.671038\n","Epoch: 115 \tTraining Loss: 3595.682456 \tValidation Loss: 775.426441\n","Epoch: 116 \tTraining Loss: 3640.864624 \tValidation Loss: 909.583169\n","Epoch: 117 \tTraining Loss: 3621.551306 \tValidation Loss: 927.255655\n","Epoch: 118 \tTraining Loss: 3664.787632 \tValidation Loss: 914.830602\n","Epoch: 119 \tTraining Loss: 3637.295170 \tValidation Loss: 798.658374\n","Epoch: 120 \tTraining Loss: 3557.988209 \tValidation Loss: 925.550273\n","Epoch: 121 \tTraining Loss: 3600.511852 \tValidation Loss: 928.277747\n","Epoch: 122 \tTraining Loss: 3625.307896 \tValidation Loss: 929.659994\n","Epoch: 123 \tTraining Loss: 3534.787615 \tValidation Loss: 921.181980\n","Epoch: 124 \tTraining Loss: 3636.903017 \tValidation Loss: 892.886237\n","Epoch: 125 \tTraining Loss: 3649.309664 \tValidation Loss: 930.386726\n","Epoch: 126 \tTraining Loss: 3643.022649 \tValidation Loss: 931.689690\n","Epoch: 127 \tTraining Loss: 3672.332533 \tValidation Loss: 950.917113\n","Epoch: 128 \tTraining Loss: 3610.542003 \tValidation Loss: 915.306162\n","Epoch: 129 \tTraining Loss: 3744.210562 \tValidation Loss: 841.656105\n","Epoch: 130 \tTraining Loss: 3613.791214 \tValidation Loss: 921.087533\n","Epoch: 131 \tTraining Loss: 3545.076699 \tValidation Loss: 925.706388\n","Epoch: 132 \tTraining Loss: 3618.269981 \tValidation Loss: 920.080927\n","Epoch: 133 \tTraining Loss: 3633.871237 \tValidation Loss: 904.006784\n","Epoch: 134 \tTraining Loss: 3601.798511 \tValidation Loss: 922.647791\n","Epoch: 135 \tTraining Loss: 3617.329790 \tValidation Loss: 882.643336\n","Epoch: 136 \tTraining Loss: 3695.112376 \tValidation Loss: 699.886881\n","Epoch: 137 \tTraining Loss: 3656.851439 \tValidation Loss: 920.755421\n","Epoch: 138 \tTraining Loss: 3658.539342 \tValidation Loss: 930.604008\n","Epoch: 139 \tTraining Loss: 3529.643260 \tValidation Loss: 932.745322\n","Epoch: 140 \tTraining Loss: 3642.430436 \tValidation Loss: 866.465129\n","Epoch: 141 \tTraining Loss: 3613.374312 \tValidation Loss: 904.432699\n","Epoch: 142 \tTraining Loss: 3650.061819 \tValidation Loss: 784.380610\n","Epoch: 143 \tTraining Loss: 3633.042436 \tValidation Loss: 927.562125\n","Epoch: 144 \tTraining Loss: 3660.086430 \tValidation Loss: 922.936115\n","Epoch: 145 \tTraining Loss: 3605.468107 \tValidation Loss: 909.079174\n","Epoch: 146 \tTraining Loss: 3617.740311 \tValidation Loss: 891.483170\n","Epoch: 147 \tTraining Loss: 3615.915308 \tValidation Loss: 841.758215\n","Epoch: 148 \tTraining Loss: 3600.912477 \tValidation Loss: 935.860477\n","Epoch: 149 \tTraining Loss: 3599.621786 \tValidation Loss: 933.734369\n","Epoch: 150 \tTraining Loss: 3534.424035 \tValidation Loss: 939.470172\n","Epoch: 151 \tTraining Loss: 3561.862491 \tValidation Loss: 833.217541\n","Epoch: 152 \tTraining Loss: 3658.471355 \tValidation Loss: 849.866639\n","Epoch: 153 \tTraining Loss: 3557.228939 \tValidation Loss: 944.469956\n","Epoch: 154 \tTraining Loss: 3637.361514 \tValidation Loss: 924.508053\n","Epoch: 155 \tTraining Loss: 3653.466488 \tValidation Loss: 923.251413\n","Epoch: 156 \tTraining Loss: 3665.492621 \tValidation Loss: 918.796751\n","Epoch: 157 \tTraining Loss: 3564.617360 \tValidation Loss: 933.976653\n","Epoch: 158 \tTraining Loss: 3625.671366 \tValidation Loss: 903.856580\n","Epoch: 159 \tTraining Loss: 3680.497660 \tValidation Loss: 924.245049\n","Epoch: 160 \tTraining Loss: 3566.035442 \tValidation Loss: 930.347567\n","Epoch: 161 \tTraining Loss: 3559.542651 \tValidation Loss: 925.730617\n","Epoch: 162 \tTraining Loss: 3566.301508 \tValidation Loss: 897.633383\n","Epoch: 163 \tTraining Loss: 3579.783542 \tValidation Loss: 925.028039\n","Epoch: 164 \tTraining Loss: 3580.390539 \tValidation Loss: 890.190170\n","Epoch: 165 \tTraining Loss: 3576.304464 \tValidation Loss: 936.868676\n","Epoch: 166 \tTraining Loss: 3508.348380 \tValidation Loss: 934.628205\n","Epoch: 167 \tTraining Loss: 3597.737559 \tValidation Loss: 923.391940\n","Epoch: 168 \tTraining Loss: 3540.709296 \tValidation Loss: 914.745624\n","Epoch: 169 \tTraining Loss: 3541.578382 \tValidation Loss: 932.293748\n","Epoch: 170 \tTraining Loss: 3465.667943 \tValidation Loss: 928.255773\n","Epoch: 171 \tTraining Loss: 3614.827921 \tValidation Loss: 912.127889\n","Epoch: 172 \tTraining Loss: 3683.318276 \tValidation Loss: 930.414844\n","Epoch: 173 \tTraining Loss: 3572.880304 \tValidation Loss: 939.581809\n","Epoch: 174 \tTraining Loss: 3592.019166 \tValidation Loss: 930.452961\n","Epoch: 175 \tTraining Loss: 3624.637177 \tValidation Loss: 859.312566\n","Epoch: 176 \tTraining Loss: 3642.327468 \tValidation Loss: 929.095772\n","Epoch: 177 \tTraining Loss: 3559.140243 \tValidation Loss: 740.271827\n","Epoch: 178 \tTraining Loss: 3622.336794 \tValidation Loss: 865.574912\n","Epoch: 179 \tTraining Loss: 3561.864278 \tValidation Loss: 921.433308\n","Epoch: 180 \tTraining Loss: 3564.221080 \tValidation Loss: 918.825146\n","Epoch: 181 \tTraining Loss: 3560.351687 \tValidation Loss: 730.900245\n","Epoch: 182 \tTraining Loss: 3593.602856 \tValidation Loss: 933.365504\n","Epoch: 183 \tTraining Loss: 3574.040950 \tValidation Loss: 944.175354\n","Epoch: 184 \tTraining Loss: 3592.038857 \tValidation Loss: 929.715188\n","Epoch: 185 \tTraining Loss: 3627.410544 \tValidation Loss: 940.450092\n","Epoch: 186 \tTraining Loss: 3502.958156 \tValidation Loss: 892.954046\n","Epoch: 187 \tTraining Loss: 3252.482286 \tValidation Loss: 884.178012\n","Epoch: 188 \tTraining Loss: 3556.089174 \tValidation Loss: 887.475454\n","Epoch: 189 \tTraining Loss: 3508.234551 \tValidation Loss: 936.333267\n","Epoch: 190 \tTraining Loss: 3532.562039 \tValidation Loss: 903.107510\n","Epoch: 191 \tTraining Loss: 3550.708963 \tValidation Loss: 902.419963\n","Epoch: 192 \tTraining Loss: 3584.889843 \tValidation Loss: 821.747359\n","Epoch: 193 \tTraining Loss: 3498.720190 \tValidation Loss: 930.884865\n","Epoch: 194 \tTraining Loss: 3649.535693 \tValidation Loss: 931.419733\n","Epoch: 195 \tTraining Loss: 3527.073101 \tValidation Loss: 885.525025\n","Epoch: 196 \tTraining Loss: 3572.973989 \tValidation Loss: 939.524022\n","Epoch: 197 \tTraining Loss: 3594.204518 \tValidation Loss: 906.663603\n","Epoch: 198 \tTraining Loss: 3506.296814 \tValidation Loss: 920.428877\n","Epoch: 199 \tTraining Loss: 3496.952554 \tValidation Loss: 930.290084\n","Epoch: 200 \tTraining Loss: 3609.267553 \tValidation Loss: 926.738022\n","Epoch: 201 \tTraining Loss: 3482.888938 \tValidation Loss: 940.611845\n","Epoch: 202 \tTraining Loss: 3562.759937 \tValidation Loss: 933.040490\n","Epoch: 203 \tTraining Loss: 3547.635124 \tValidation Loss: 855.807829\n","Epoch: 204 \tTraining Loss: 3517.822646 \tValidation Loss: 911.917468\n","Epoch: 205 \tTraining Loss: 3605.878745 \tValidation Loss: 909.614995\n","Epoch: 206 \tTraining Loss: 3546.537768 \tValidation Loss: 929.701839\n","Epoch: 207 \tTraining Loss: 3610.378766 \tValidation Loss: 791.962379\n","Epoch: 208 \tTraining Loss: 3622.022953 \tValidation Loss: 898.405739\n","Epoch: 209 \tTraining Loss: 3586.801438 \tValidation Loss: 918.236922\n","Epoch: 210 \tTraining Loss: 3588.997553 \tValidation Loss: 928.797434\n","Epoch: 211 \tTraining Loss: 3627.758803 \tValidation Loss: 920.746462\n","Epoch: 212 \tTraining Loss: 3522.937199 \tValidation Loss: 933.412466\n","Epoch: 213 \tTraining Loss: 3636.579459 \tValidation Loss: 911.787753\n","Epoch: 214 \tTraining Loss: 3515.274791 \tValidation Loss: 908.500341\n","Epoch: 215 \tTraining Loss: 3594.251850 \tValidation Loss: 916.395140\n","Epoch: 216 \tTraining Loss: 3576.062302 \tValidation Loss: 823.676336\n","Epoch: 217 \tTraining Loss: 3377.704178 \tValidation Loss: 902.183786\n","Epoch: 218 \tTraining Loss: 3522.446070 \tValidation Loss: 915.248374\n","Epoch: 219 \tTraining Loss: 2834.752525 \tValidation Loss: 933.683235\n","Epoch: 220 \tTraining Loss: 3530.744246 \tValidation Loss: 909.363780\n","Epoch: 221 \tTraining Loss: 3536.541333 \tValidation Loss: 863.491244\n","Epoch: 222 \tTraining Loss: 3587.639797 \tValidation Loss: 924.324057\n","Epoch: 223 \tTraining Loss: 3518.447180 \tValidation Loss: 830.877162\n","Epoch: 224 \tTraining Loss: 3529.526092 \tValidation Loss: 937.569886\n","Epoch: 225 \tTraining Loss: 3514.612096 \tValidation Loss: 922.162652\n","Epoch: 226 \tTraining Loss: 3534.272257 \tValidation Loss: 926.776176\n","Epoch: 227 \tTraining Loss: 3565.739612 \tValidation Loss: 917.345682\n","Epoch: 228 \tTraining Loss: 3558.906870 \tValidation Loss: 921.325753\n","Epoch: 229 \tTraining Loss: 3551.964952 \tValidation Loss: 926.156432\n","Epoch: 230 \tTraining Loss: 3562.573392 \tValidation Loss: 938.158328\n","Epoch: 231 \tTraining Loss: 3553.140888 \tValidation Loss: 934.311676\n","Epoch: 232 \tTraining Loss: 3477.513671 \tValidation Loss: 927.772301\n","Epoch: 233 \tTraining Loss: 3542.012683 \tValidation Loss: 907.787771\n","Epoch: 234 \tTraining Loss: 3531.008364 \tValidation Loss: 924.130086\n","Epoch: 235 \tTraining Loss: 3678.930397 \tValidation Loss: 920.835540\n","Epoch: 236 \tTraining Loss: 3286.235081 \tValidation Loss: 928.862446\n","Epoch: 237 \tTraining Loss: 3501.167850 \tValidation Loss: 841.492812\n","Epoch: 238 \tTraining Loss: 3545.530477 \tValidation Loss: 904.198208\n","Epoch: 239 \tTraining Loss: 3542.518191 \tValidation Loss: 938.501626\n","Epoch: 240 \tTraining Loss: 3556.692622 \tValidation Loss: 927.031418\n","Epoch: 241 \tTraining Loss: 2740.846567 \tValidation Loss: 829.684371\n","Epoch: 242 \tTraining Loss: 3553.666767 \tValidation Loss: 939.783813\n","Epoch: 243 \tTraining Loss: 3476.817890 \tValidation Loss: 932.856721\n","Epoch: 244 \tTraining Loss: 3518.572078 \tValidation Loss: 921.263461\n","Epoch: 245 \tTraining Loss: 3515.197388 \tValidation Loss: 750.961822\n","Epoch: 246 \tTraining Loss: 3570.095749 \tValidation Loss: 925.542868\n","Epoch: 247 \tTraining Loss: 3514.677663 \tValidation Loss: 870.150265\n","Epoch: 248 \tTraining Loss: 3619.256531 \tValidation Loss: 939.872545\n","Epoch: 249 \tTraining Loss: 3578.544484 \tValidation Loss: 934.387380\n","Epoch: 250 \tTraining Loss: 3515.034420 \tValidation Loss: 929.320966\n","Epoch: 251 \tTraining Loss: 3433.462959 \tValidation Loss: 740.507774\n","Epoch: 252 \tTraining Loss: 2747.110172 \tValidation Loss: 926.653432\n","Epoch: 253 \tTraining Loss: 3538.146007 \tValidation Loss: 918.906383\n","Epoch: 254 \tTraining Loss: 3555.055538 \tValidation Loss: 888.230886\n","Epoch: 255 \tTraining Loss: 3668.477126 \tValidation Loss: 923.010669\n","Epoch: 256 \tTraining Loss: 3541.869734 \tValidation Loss: 945.703774\n","Epoch: 257 \tTraining Loss: 3532.875641 \tValidation Loss: 939.784248\n","Epoch: 258 \tTraining Loss: 3472.040553 \tValidation Loss: 747.790288\n","Epoch: 259 \tTraining Loss: 3554.999422 \tValidation Loss: 930.836487\n","Epoch: 260 \tTraining Loss: 3613.652263 \tValidation Loss: 932.918034\n","Epoch: 261 \tTraining Loss: 3584.654187 \tValidation Loss: 922.068555\n","Epoch: 262 \tTraining Loss: 3575.788616 \tValidation Loss: 918.153573\n","Epoch: 263 \tTraining Loss: 3533.476848 \tValidation Loss: 914.379650\n","Epoch: 264 \tTraining Loss: 3544.640355 \tValidation Loss: 909.547566\n","Epoch: 265 \tTraining Loss: 3350.425888 \tValidation Loss: 866.630635\n","Epoch: 266 \tTraining Loss: 3588.750375 \tValidation Loss: 933.638776\n","Epoch: 267 \tTraining Loss: 3201.513763 \tValidation Loss: 897.476934\n","Epoch: 268 \tTraining Loss: 3551.588865 \tValidation Loss: 928.833422\n","Epoch: 269 \tTraining Loss: 3486.201001 \tValidation Loss: 741.001543\n","Epoch: 270 \tTraining Loss: 3590.045775 \tValidation Loss: 723.780760\n","Epoch: 271 \tTraining Loss: 3502.415014 \tValidation Loss: 905.486182\n","Epoch: 272 \tTraining Loss: 3430.421707 \tValidation Loss: 934.792705\n","Epoch: 273 \tTraining Loss: 3537.707358 \tValidation Loss: 913.049911\n","Epoch: 274 \tTraining Loss: 3537.698226 \tValidation Loss: 942.249296\n","Epoch: 275 \tTraining Loss: 3503.560276 \tValidation Loss: 942.318570\n","Epoch: 276 \tTraining Loss: 3523.651075 \tValidation Loss: 908.575048\n","Epoch: 277 \tTraining Loss: 3529.852860 \tValidation Loss: 829.860839\n","Epoch: 278 \tTraining Loss: 3490.733716 \tValidation Loss: 900.897260\n","Epoch: 279 \tTraining Loss: 3588.211141 \tValidation Loss: 914.338661\n","Epoch: 280 \tTraining Loss: 3538.920874 \tValidation Loss: 910.549541\n","Epoch: 281 \tTraining Loss: 3504.293069 \tValidation Loss: 777.971103\n","Epoch: 282 \tTraining Loss: 3523.391863 \tValidation Loss: 905.322845\n","Epoch: 283 \tTraining Loss: 3471.704860 \tValidation Loss: 917.096929\n","Epoch: 284 \tTraining Loss: 3595.753612 \tValidation Loss: 876.856372\n","Epoch: 285 \tTraining Loss: 3522.998770 \tValidation Loss: 817.716830\n","Epoch: 286 \tTraining Loss: 3490.091545 \tValidation Loss: 924.462879\n","Epoch: 287 \tTraining Loss: 3554.481975 \tValidation Loss: 919.131029\n","Epoch: 288 \tTraining Loss: 3493.958041 \tValidation Loss: 890.117692\n","Epoch: 289 \tTraining Loss: 3495.433080 \tValidation Loss: 910.050931\n","Epoch: 290 \tTraining Loss: 3544.212359 \tValidation Loss: 919.063558\n","Epoch: 291 \tTraining Loss: 3486.703439 \tValidation Loss: 809.806686\n","Epoch: 292 \tTraining Loss: 3520.248590 \tValidation Loss: 905.598703\n","Epoch: 293 \tTraining Loss: 3346.872143 \tValidation Loss: 906.315243\n","Epoch: 294 \tTraining Loss: 3447.168624 \tValidation Loss: 912.813842\n","Epoch: 295 \tTraining Loss: 3569.588119 \tValidation Loss: 816.960743\n","Epoch: 296 \tTraining Loss: 3459.574438 \tValidation Loss: 913.128430\n","Epoch: 297 \tTraining Loss: 3518.641665 \tValidation Loss: 918.355897\n","Epoch: 298 \tTraining Loss: 3537.335137 \tValidation Loss: 825.234842\n","Epoch: 299 \tTraining Loss: 3530.889179 \tValidation Loss: 924.485149\n","Epoch: 300 \tTraining Loss: 3564.688755 \tValidation Loss: 898.419040\n","Epoch: 301 \tTraining Loss: 3535.211194 \tValidation Loss: 923.120488\n","Epoch: 302 \tTraining Loss: 3504.374916 \tValidation Loss: 928.047094\n","Epoch: 303 \tTraining Loss: 3569.550665 \tValidation Loss: 819.967022\n","Epoch: 304 \tTraining Loss: 3545.622057 \tValidation Loss: 918.052864\n","Epoch: 305 \tTraining Loss: 3533.018181 \tValidation Loss: 833.914965\n","Epoch: 306 \tTraining Loss: 3558.082347 \tValidation Loss: 926.355819\n","Epoch: 307 \tTraining Loss: 3334.821604 \tValidation Loss: 923.851172\n","Epoch: 308 \tTraining Loss: 3498.325347 \tValidation Loss: 915.341330\n","Epoch: 309 \tTraining Loss: 3463.961515 \tValidation Loss: 921.301985\n","Epoch: 310 \tTraining Loss: 3498.165682 \tValidation Loss: 771.130929\n","Epoch: 311 \tTraining Loss: 3617.923402 \tValidation Loss: 955.411444\n","Epoch: 312 \tTraining Loss: 3496.519169 \tValidation Loss: 848.998602\n","Epoch: 313 \tTraining Loss: 3519.588391 \tValidation Loss: 858.932856\n","Epoch: 314 \tTraining Loss: 3501.547808 \tValidation Loss: 934.711627\n","Epoch: 315 \tTraining Loss: 3445.668074 \tValidation Loss: 934.173167\n","Epoch: 316 \tTraining Loss: 3486.893839 \tValidation Loss: 920.493497\n","Epoch: 317 \tTraining Loss: 3418.969370 \tValidation Loss: 903.518924\n","Epoch: 318 \tTraining Loss: 3523.646911 \tValidation Loss: 920.367432\n","Epoch: 319 \tTraining Loss: 3538.592512 \tValidation Loss: 934.101044\n","Epoch: 320 \tTraining Loss: 3448.212166 \tValidation Loss: 938.258820\n","Epoch: 321 \tTraining Loss: 3492.122049 \tValidation Loss: 906.767192\n","Epoch: 322 \tTraining Loss: 3557.818696 \tValidation Loss: 814.374997\n","Epoch: 323 \tTraining Loss: 3584.544465 \tValidation Loss: 919.780658\n","Epoch: 324 \tTraining Loss: 3512.137365 \tValidation Loss: 844.148900\n","Epoch: 325 \tTraining Loss: 3495.675602 \tValidation Loss: 841.232426\n","Epoch: 326 \tTraining Loss: 3577.145614 \tValidation Loss: 917.169824\n","Epoch: 327 \tTraining Loss: 3538.884040 \tValidation Loss: 918.113174\n","Epoch: 328 \tTraining Loss: 3483.751905 \tValidation Loss: 910.373114\n","Epoch: 329 \tTraining Loss: 3478.914015 \tValidation Loss: 806.017214\n","Epoch: 330 \tTraining Loss: 3422.263017 \tValidation Loss: 888.532506\n","Epoch: 331 \tTraining Loss: 3220.005841 \tValidation Loss: 929.866173\n","Epoch: 332 \tTraining Loss: 3494.197872 \tValidation Loss: 938.046882\n","Epoch: 333 \tTraining Loss: 3438.821524 \tValidation Loss: 929.140040\n","Epoch: 334 \tTraining Loss: 3574.169910 \tValidation Loss: 928.314885\n","Epoch: 335 \tTraining Loss: 3488.687022 \tValidation Loss: 876.733389\n","Epoch: 336 \tTraining Loss: 3509.607013 \tValidation Loss: 928.233190\n","Epoch: 337 \tTraining Loss: 3460.922671 \tValidation Loss: 921.168419\n","Epoch: 338 \tTraining Loss: 3559.480618 \tValidation Loss: 940.733136\n","Epoch: 339 \tTraining Loss: 3593.068153 \tValidation Loss: 939.881101\n","Epoch: 340 \tTraining Loss: 3459.179738 \tValidation Loss: 930.674709\n","Epoch: 341 \tTraining Loss: 3361.750096 \tValidation Loss: 923.521361\n","Epoch: 342 \tTraining Loss: 3483.828239 \tValidation Loss: 919.942749\n","Epoch: 343 \tTraining Loss: 3549.401489 \tValidation Loss: 718.280966\n","Epoch: 344 \tTraining Loss: 3500.541889 \tValidation Loss: 938.551370\n","Epoch: 345 \tTraining Loss: 3532.771786 \tValidation Loss: 941.187888\n","Epoch: 346 \tTraining Loss: 3325.356657 \tValidation Loss: 770.190774\n","Epoch: 347 \tTraining Loss: 3550.352535 \tValidation Loss: 894.227854\n","Epoch: 348 \tTraining Loss: 3553.895247 \tValidation Loss: 906.681354\n","Epoch: 349 \tTraining Loss: 3589.353726 \tValidation Loss: 911.820081\n","Epoch: 350 \tTraining Loss: 3542.324086 \tValidation Loss: 901.751745\n","Epoch: 351 \tTraining Loss: 3358.239558 \tValidation Loss: 910.155222\n","Epoch: 352 \tTraining Loss: 3509.928012 \tValidation Loss: 917.133789\n","Epoch: 353 \tTraining Loss: 3568.927917 \tValidation Loss: 907.523000\n","Epoch: 354 \tTraining Loss: 3501.294112 \tValidation Loss: 880.015038\n","Epoch: 355 \tTraining Loss: 3458.937976 \tValidation Loss: 920.821282\n","Epoch: 356 \tTraining Loss: 3553.756964 \tValidation Loss: 899.216732\n","Epoch: 357 \tTraining Loss: 3410.294760 \tValidation Loss: 914.248715\n","Epoch: 358 \tTraining Loss: 3431.141519 \tValidation Loss: 919.492042\n","Epoch: 359 \tTraining Loss: 3584.627949 \tValidation Loss: 941.714132\n","Epoch: 360 \tTraining Loss: 3231.339148 \tValidation Loss: 811.287557\n","Epoch: 361 \tTraining Loss: 3559.523538 \tValidation Loss: 920.717265\n","Epoch: 362 \tTraining Loss: 3473.241519 \tValidation Loss: 928.438809\n","Epoch: 363 \tTraining Loss: 3563.504891 \tValidation Loss: 919.146927\n","Epoch: 364 \tTraining Loss: 3533.274045 \tValidation Loss: 917.761629\n","Epoch: 365 \tTraining Loss: 3497.469975 \tValidation Loss: 773.244192\n","Epoch: 366 \tTraining Loss: 3561.932573 \tValidation Loss: 926.564127\n","Epoch: 367 \tTraining Loss: 3513.555005 \tValidation Loss: 917.128508\n","Epoch: 368 \tTraining Loss: 3515.954560 \tValidation Loss: 934.565924\n","Epoch: 369 \tTraining Loss: 3515.256230 \tValidation Loss: 907.526021\n","Epoch: 370 \tTraining Loss: 3478.500462 \tValidation Loss: 899.024908\n","Epoch: 371 \tTraining Loss: 3481.789714 \tValidation Loss: 739.918334\n","Epoch: 372 \tTraining Loss: 3451.255990 \tValidation Loss: 928.195087\n","Epoch: 373 \tTraining Loss: 3441.543987 \tValidation Loss: 930.693787\n","Epoch: 374 \tTraining Loss: 3421.679936 \tValidation Loss: 914.088260\n","Epoch: 375 \tTraining Loss: 3417.513519 \tValidation Loss: 935.774387\n","Epoch: 376 \tTraining Loss: 3570.929799 \tValidation Loss: 892.317568\n","Epoch: 377 \tTraining Loss: 3565.762984 \tValidation Loss: 940.310238\n","Epoch: 378 \tTraining Loss: 3547.892386 \tValidation Loss: 939.861884\n","Epoch: 379 \tTraining Loss: 3487.577420 \tValidation Loss: 927.196007\n","Epoch: 380 \tTraining Loss: 3589.891445 \tValidation Loss: 907.528062\n","Epoch: 381 \tTraining Loss: 3435.696842 \tValidation Loss: 822.765490\n","Epoch: 382 \tTraining Loss: 3537.284392 \tValidation Loss: 946.509277\n","Epoch: 383 \tTraining Loss: 3571.371333 \tValidation Loss: 921.371467\n","Epoch: 384 \tTraining Loss: 3568.422438 \tValidation Loss: 885.416856\n","Epoch: 385 \tTraining Loss: 3531.806174 \tValidation Loss: 910.453249\n","Epoch: 386 \tTraining Loss: 3558.343727 \tValidation Loss: 921.700055\n","Epoch: 387 \tTraining Loss: 3440.433331 \tValidation Loss: 882.471776\n","Epoch: 388 \tTraining Loss: 3497.936126 \tValidation Loss: 919.756853\n","Epoch: 389 \tTraining Loss: 3565.650736 \tValidation Loss: 920.192272\n","Epoch: 390 \tTraining Loss: 3472.358243 \tValidation Loss: 930.493050\n","Epoch: 391 \tTraining Loss: 3510.644250 \tValidation Loss: 908.892687\n","Epoch: 392 \tTraining Loss: 3535.662079 \tValidation Loss: 891.944155\n","Epoch: 393 \tTraining Loss: 3544.798003 \tValidation Loss: 924.106674\n","Epoch: 394 \tTraining Loss: 3607.565091 \tValidation Loss: 919.231493\n","Epoch: 395 \tTraining Loss: 3499.620817 \tValidation Loss: 910.985127\n","Epoch: 396 \tTraining Loss: 3518.209177 \tValidation Loss: 893.146823\n","Epoch: 397 \tTraining Loss: 3442.841378 \tValidation Loss: 912.532562\n","Epoch: 398 \tTraining Loss: 3451.407067 \tValidation Loss: 917.585555\n","Epoch: 399 \tTraining Loss: 3526.182022 \tValidation Loss: 915.413056\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MPiYWv9UoFP_","colab_type":"code","outputId":"78b8212d-d385-4687-cd0e-34973134cfd3","executionInfo":{"status":"ok","timestamp":1586923766564,"user_tz":300,"elapsed":622,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["net.load_state_dict(torch.load('lstm_test.pt'))"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":381}]},{"cell_type":"code","metadata":{"id":"LOYJ_iJ0gCED","colab_type":"code","outputId":"74a43145-6f7b-4ab0-b710-9eb8993ed03e","executionInfo":{"status":"ok","timestamp":1586923766881,"user_tz":300,"elapsed":592,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":199}},"source":["\n","\n","# track test loss\n","test_loss = 0.0\n","mean_abs = 0.0\n","\n","net.eval()\n","# iterate over test data\n","#bin_op.binarization()\n","test_h = net.init_hidden(batch_size)\n","\n","for batch_idx, (X_temp, X_num, X_cat, labels) in enumerate(test_loader):\n","    if(train_on_gpu):\n","        X_temp, X_num, X_cat, labels = X_temp.cuda(), X_num.cuda(), X_cat.type(torch.LongTensor).cuda(), labels.cuda()\n","    \n","    test_h = tuple([each.data for each in test_h])\n","\n","    # get the output from the model\n","    lstm_output, h, num_output, cat_output = net(X_temp, h, X_num, X_cat)\n","\n","    output = (weight_lstm*lstm_output + weight_num*num_output + weight_cat*cat_output) \n","    # / (weight_lstm + weight_num + weight_cat)\n","\n","    # calculate the loss and perform backprop\n","    loss = criterion(output.squeeze(), labels.double())\n","\n","    out_np = output.detach().squeeze().cpu().numpy().astype(int).T\n","    target_np = labels.detach().cpu().numpy().astype(int).T\n","\n","    print([(out, tar) for out, tar in zip(out_np, target_np)])\n","\n","    mean_abs += mean_absolute_error(out_np, target_np) * batch_size\n","        \n","    test_loss += loss.item() * batch_size\n","\n","# calculate average losses\n","test_loss = np.sqrt(test_loss / len(test_loader.dataset))\n","mean_abs /= len(test_loader.dataset)\n","print('Test Loss: {:.6f}\\n'.format(test_loss))\n","print('Mean Abs Loss: {:.6f}\\n'.format(mean_abs))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[(4, 3), (4, 3), (3, 2), (6, 2), (5, 2), (5, 3), (6, 3), (2, 3), (2, 2), (2, 6), (2, 4), (2, 4), (2, 5), (2, 3), (2, 2), (5, 16)]\n","[(4, 4), (2, 10), (2, 5), (2, 3), (2, 8), (9, 2), (6, 9), (2, 66), (7, 3), (5, 9), (5, 11), (5, 8), (4, 25), (4, 4), (8, 3), (7, 2)]\n","[(7, 3), (6, 3), (7, 6), (8, 3), (8, 8), (8, 8), (7, 6), (7, 9), (8, 12), (8, 18), (7, 24), (7, 19), (7, 3), (7, 15), (8, 12), (8, 12)]\n","[(7, 17), (7, 9), (7, 3), (7, 12), (7, 33), (7, 11), (7, 6), (7, 2), (7, 7), (7, 5), (7, 2), (8, 11), (8, 23), (8, 4), (7, 7), (7, 5)]\n","[(6, 6), (7, 10), (7, 12), (6, 5), (7, 10), (5, 2), (7, 3), (7, 32), (7, 13), (5, 27), (8, 9), (8, 12), (5, 16), (8, 6), (5, 3), (7, 7)]\n","[(5, 2), (5, 6), (7, 10), (7, 6), (7, 2), (4, 3), (5, 5), (7, 2), (7, 5), (4, 17), (7, 18), (7, 3), (4, 8), (7, 2), (7, 15), (4, 5)]\n","Test Loss: 8.987268\n","\n","Mean Abs Loss: 4.809524\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fdmJvO0ZNXDu","colab_type":"text"},"source":["## Mixed Model"]},{"cell_type":"code","metadata":{"id":"iALvtBlwNZc0","colab_type":"code","colab":{}},"source":["class MosquitoMixed(nn.Module):\n","    def __init__(self, \n","                 lstm_columns, lstm_hidden_dim, lstm_n_layers, lstm_output,\n","                 num_columns, num_hidden_dim,\n","                 embedding_sizes, cat_hidden_dim,\n","                 combined_dim):\n","        super(MosquitoMixed, self).__init__()\n","\n","        self.lstm_n_layers = lstm_n_layers\n","        self.lstm_hidden_dim = lstm_hidden_dim\n","\n","        self.num_hidden_dim = num_hidden_dim\n","        self.cat_hidden_dim = cat_hidden_dim\n","\n","        embed_size = 0\n","        for embed in embedding_sizes:\n","            embed_size += embed[1]\n","\n","        self.all_embeddings = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in embedding_sizes])\n","\n","        # For LSTM\n","        self.lstm = nn.LSTM(lstm_columns, lstm_hidden_dim, lstm_n_layers,\n","                            dropout=0.3, batch_first=True)\n","        \n","        # LSTM dense layer\n","        self.lstm_dense = nn.Sequential(\n","            nn.BatchNorm1d(lstm_hidden_dim),\n","            nn.Dropout(0.3),\n","            nn.Linear(lstm_hidden_dim, lstm_output),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm1d(lstm_output)\n","        )\n","        \n","        self.num_dense = nn.Sequential(\n","            nn.Dropout(0.3),\n","            nn.Linear(num_columns, num_hidden_dim),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm1d(num_hidden_dim),\n","            nn.Dropout(0.3),\n","            nn.Linear(num_hidden_dim, num_hidden_dim),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm1d(num_hidden_dim)\n","        )\n","\n","        self.cat_dense = nn.Sequential(\n","            nn.Dropout(0.3),\n","            nn.Linear(embed_size, cat_hidden_dim),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm1d(cat_hidden_dim),\n","            nn.Dropout(0.3),\n","            nn.Linear(cat_hidden_dim, cat_hidden_dim),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm1d(cat_hidden_dim)\n","        )\n","\n","        combined_output = lstm_output + num_hidden_dim + cat_hidden_dim\n","\n","        self.combined_dense = nn.Sequential(\n","            nn.Dropout(0.3),\n","            nn.Linear(combined_output, combined_dim),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm1d(combined_dim),\n","            nn.Dropout(0.3),\n","            nn.Linear(combined_dim, combined_dim),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm1d(combined_dim),\n","            nn.Linear(combined_dim, 1),\n","            nn.ReLU()\n","        )\n","\n","    def forward(self, X_temp, hidden, X_num, X_cat):\n","        # Forward lstm\n","        lstm_out, hidden = self.lstm(X_temp, hidden)\n","        # stack up lstm outputs\n","        lstm_out = lstm_out.contiguous().view(-1, self.lstm_hidden_dim)\n","        # dropout and fully-connected layer\n","        lstm_out = self.lstm_dense(lstm_out)\n","      \n","        # reshape to be batch_size first\n","        lstm_out = lstm_out.view(batch_size, SEQ_LENGTH, -1)\n","        lstm_out = lstm_out[:, -1, :]\n","\n","        # Forward num\n","        num_out = self.num_dense(X_num)\n","\n","        # Forward cat\n","        embeddings = []\n","        for i, e in enumerate(self.all_embeddings):\n","            embed = e(X_cat[:, i])\n","            embeddings.append(embed)\n","\n","        cat_out = torch.cat(embeddings, 1)\n","        cat_out = self.cat_dense(cat_out)\n","\n","        combined_out = torch.cat((lstm_out, num_out, cat_out), dim=1)\n","        combined_out = self.combined_dense(combined_out)\n","\n","        return combined_out, hidden\n","\n","    def init_hidden(self, batch_size):\n","        ''' Initializes hidden state '''\n","        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n","        # initialized to zero, for hidden state and cell state of LSTM\n","        weight = next(self.parameters()).data\n","        \n","        if (train_on_gpu):\n","            hidden = (weight.new(self.lstm_n_layers, batch_size, self.lstm_hidden_dim).zero_().cuda(),\n","                  weight.new(self.lstm_n_layers, batch_size, self.lstm_hidden_dim).zero_().cuda())\n","        else:\n","            hidden = (weight.new(self.lstm_n_layers, batch_size, self.lstm_hidden_dim).zero_(),\n","                      weight.new(self.lstm_n_layers, batch_size, self.lstm_hidden_dim).zero_())\n","        \n","        return hidden"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nG0TTenANiCE","colab_type":"code","outputId":"7256b60c-7ff9-40c5-c3d4-a28f79c6604a","executionInfo":{"status":"ok","timestamp":1586927566063,"user_tz":300,"elapsed":623,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Instantiate the model w/ hyperparams\n","a_h = 3\n","\n","lstm_hidden_dim = np.int(len(train_idx) / (a_h * len(numerical_columns) + 1))\n","print(lstm_hidden_dim)\n","lstm_n_layers = 2\n","\n","num_hidden_dim = 15\n","cat_hidden_dim = 15\n","lstm_output = 15\n","combined_dim = 30\n","\n","#net = MosquitoLSTM(len(numerical_columns), hidden_dim, n_layers).double()\n","net = MosquitoMixed(\n","    len(numerical_columns), lstm_hidden_dim, lstm_n_layers, lstm_output,\n","    len(non_num_columns), num_hidden_dim,\n","    embedding_sizes, cat_hidden_dim,\n","    combined_dim\n",").double()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["23\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ke9eeDONNieR","colab_type":"code","colab":{}},"source":["lr = 0.0005\n","\n","criterion = nn.MSELoss()\n","#optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n","optimizer = torch.optim.RMSprop(net.parameters(), lr=lr)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QmqNAusjNlqH","colab_type":"code","outputId":"bd3410bb-5f90-4d72-8c08-7d39445e09cd","executionInfo":{"status":"ok","timestamp":1586927743453,"user_tz":300,"elapsed":171250,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# training params\n","\n","epochs = 400 # 3-4 is approx where I noticed the validation loss stop decreasing\n","\n","counter = 0\n","print_every = 10\n","clip = 8 # gradient clipping\n","\n","# move model to GPU, if available\n","if(train_on_gpu):\n","    net.cuda()\n","\n","net.train()\n","# train for some number of epochs\n","valid_loss_min = np.Inf\n","\n","for epoch in range(epochs):\n","    # initialize hidden state\n","    h = net.init_hidden(batch_size)\n","\n","    train_loss = 0.0\n","    valid_loss = 0.0\n","\n","    net.train()\n","\n","    # batch loop\n","    for X_temp, X_num, X_cat, labels in train_loader:\n","        counter += 1\n","\n","        if(train_on_gpu):\n","            X_temp, X_num, X_cat, labels = X_temp.cuda(), X_num.cuda(), X_cat.type(torch.LongTensor).cuda(), labels.cuda()\n","\n","        # Creating new variables for the hidden state, otherwise\n","        # we'd backprop through the entire training history\n","        h = tuple([each.data for each in h])\n","        # zero accumulated gradients\n","        net.zero_grad()\n","\n","        # get the output from the model\n","        output, h = net(X_temp, h, X_num, X_cat)\n","\n","        # calculate the loss and perform backprop\n","        loss = criterion(output.squeeze(), labels.double())\n","        loss.backward()\n","        train_loss += loss.item()\n","        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","        nn.utils.clip_grad_norm_(net.parameters(), clip)\n","        optimizer.step()\n","\n","    net.eval()\n","\n","    val_h = net.init_hidden(batch_size)\n","\n","    for X_temp, X_num, X_cat, labels in valid_loader:\n","        \n","        \n","        if(train_on_gpu):\n","            X_temp, X_num, X_cat, labels = X_temp.cuda(), X_num.cuda(), X_cat.type(torch.LongTensor).cuda(), labels.cuda()\n","\n","        # Creating new variables for the hidden state, otherwise\n","        # we'd backprop through the entire training history\n","        val_h = tuple([each.data for each in val_h])\n","\n","        # get the output from the model\n","        output, h = net(X_temp, h, X_num, X_cat)\n","\n","        # calculate the loss and perform backprop\n","        loss = criterion(output.squeeze(), labels.double())\n","        valid_loss += loss.item()\n","\n","    # save model if validation loss has decreased\n","    if valid_loss <= valid_loss_min:\n","        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","        valid_loss_min,\n","        valid_loss))\n","        torch.save(net.state_dict(), 'mixed_model.pt')\n","        valid_loss_min = valid_loss\n","        \n","    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n","            epoch, train_loss, valid_loss))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Validation loss decreased (inf --> 1408.388703).  Saving model ...\n","Epoch: 0 \tTraining Loss: 5613.927077 \tValidation Loss: 1408.388703\n","Epoch: 1 \tTraining Loss: 5464.741120 \tValidation Loss: 1425.810636\n","Validation loss decreased (1408.388703 --> 1398.965393).  Saving model ...\n","Epoch: 2 \tTraining Loss: 5332.130202 \tValidation Loss: 1398.965393\n","Validation loss decreased (1398.965393 --> 1121.663717).  Saving model ...\n","Epoch: 3 \tTraining Loss: 5184.529398 \tValidation Loss: 1121.663717\n","Epoch: 4 \tTraining Loss: 5103.049155 \tValidation Loss: 1327.744298\n","Epoch: 5 \tTraining Loss: 4991.016889 \tValidation Loss: 1251.445485\n","Epoch: 6 \tTraining Loss: 4956.225093 \tValidation Loss: 1255.050812\n","Epoch: 7 \tTraining Loss: 3920.977041 \tValidation Loss: 1225.111348\n","Epoch: 8 \tTraining Loss: 4758.061543 \tValidation Loss: 1242.986504\n","Epoch: 9 \tTraining Loss: 4698.551310 \tValidation Loss: 1223.246880\n","Epoch: 10 \tTraining Loss: 4575.154572 \tValidation Loss: 1181.207521\n","Epoch: 11 \tTraining Loss: 4562.272252 \tValidation Loss: 1187.592221\n","Validation loss decreased (1121.663717 --> 1003.328698).  Saving model ...\n","Epoch: 12 \tTraining Loss: 4601.256597 \tValidation Loss: 1003.328698\n","Validation loss decreased (1003.328698 --> 943.293971).  Saving model ...\n","Epoch: 13 \tTraining Loss: 4549.384667 \tValidation Loss: 943.293971\n","Epoch: 14 \tTraining Loss: 4475.240220 \tValidation Loss: 1087.989385\n","Epoch: 15 \tTraining Loss: 4314.748794 \tValidation Loss: 1172.764157\n","Epoch: 16 \tTraining Loss: 3619.717063 \tValidation Loss: 1137.183387\n","Epoch: 17 \tTraining Loss: 4464.598506 \tValidation Loss: 1044.737271\n","Epoch: 18 \tTraining Loss: 4416.828679 \tValidation Loss: 1106.009552\n","Epoch: 19 \tTraining Loss: 4378.109719 \tValidation Loss: 1099.915399\n","Epoch: 20 \tTraining Loss: 4353.555397 \tValidation Loss: 1113.140434\n","Epoch: 21 \tTraining Loss: 4338.986533 \tValidation Loss: 1082.377342\n","Epoch: 22 \tTraining Loss: 4439.005950 \tValidation Loss: 1143.776270\n","Epoch: 23 \tTraining Loss: 4360.307087 \tValidation Loss: 1100.741529\n","Epoch: 24 \tTraining Loss: 4274.916120 \tValidation Loss: 1080.669816\n","Epoch: 25 \tTraining Loss: 4344.885421 \tValidation Loss: 1060.975055\n","Epoch: 26 \tTraining Loss: 4303.224271 \tValidation Loss: 1110.640613\n","Epoch: 27 \tTraining Loss: 4202.681283 \tValidation Loss: 1082.347487\n","Epoch: 28 \tTraining Loss: 4199.208450 \tValidation Loss: 1077.824932\n","Epoch: 29 \tTraining Loss: 4234.628685 \tValidation Loss: 1056.267335\n","Epoch: 30 \tTraining Loss: 4155.500540 \tValidation Loss: 1012.655533\n","Epoch: 31 \tTraining Loss: 4194.901412 \tValidation Loss: 1044.444875\n","Epoch: 32 \tTraining Loss: 4219.328675 \tValidation Loss: 1063.745791\n","Epoch: 33 \tTraining Loss: 4105.243827 \tValidation Loss: 1042.981109\n","Epoch: 34 \tTraining Loss: 4181.943315 \tValidation Loss: 999.559516\n","Epoch: 35 \tTraining Loss: 4118.503295 \tValidation Loss: 976.768237\n","Validation loss decreased (943.293971 --> 826.242358).  Saving model ...\n","Epoch: 36 \tTraining Loss: 4201.622486 \tValidation Loss: 826.242358\n","Epoch: 37 \tTraining Loss: 4181.090819 \tValidation Loss: 897.210557\n","Epoch: 38 \tTraining Loss: 4130.551755 \tValidation Loss: 992.202108\n","Epoch: 39 \tTraining Loss: 3807.274130 \tValidation Loss: 1011.601095\n","Epoch: 40 \tTraining Loss: 3250.264448 \tValidation Loss: 970.678512\n","Epoch: 41 \tTraining Loss: 4046.993914 \tValidation Loss: 938.389730\n","Epoch: 42 \tTraining Loss: 4244.871480 \tValidation Loss: 963.981151\n","Epoch: 43 \tTraining Loss: 4076.617916 \tValidation Loss: 1004.278110\n","Epoch: 44 \tTraining Loss: 4047.879851 \tValidation Loss: 989.865758\n","Epoch: 45 \tTraining Loss: 4091.001101 \tValidation Loss: 880.768318\n","Epoch: 46 \tTraining Loss: 4060.057595 \tValidation Loss: 902.455675\n","Epoch: 47 \tTraining Loss: 4072.207376 \tValidation Loss: 969.150708\n","Epoch: 48 \tTraining Loss: 4073.112569 \tValidation Loss: 987.304027\n","Epoch: 49 \tTraining Loss: 4047.838639 \tValidation Loss: 861.442664\n","Epoch: 50 \tTraining Loss: 4177.974648 \tValidation Loss: 863.570199\n","Epoch: 51 \tTraining Loss: 3970.239132 \tValidation Loss: 988.074335\n","Epoch: 52 \tTraining Loss: 4123.502215 \tValidation Loss: 985.662650\n","Epoch: 53 \tTraining Loss: 4092.470368 \tValidation Loss: 890.169742\n","Epoch: 54 \tTraining Loss: 4069.390346 \tValidation Loss: 987.543667\n","Epoch: 55 \tTraining Loss: 3962.132708 \tValidation Loss: 944.412945\n","Epoch: 56 \tTraining Loss: 4077.997454 \tValidation Loss: 1052.463173\n","Epoch: 57 \tTraining Loss: 4078.555309 \tValidation Loss: 1112.451932\n","Epoch: 58 \tTraining Loss: 4064.598502 \tValidation Loss: 854.911056\n","Epoch: 59 \tTraining Loss: 4056.738984 \tValidation Loss: 957.020979\n","Epoch: 60 \tTraining Loss: 4116.089262 \tValidation Loss: 1017.513853\n","Epoch: 61 \tTraining Loss: 3963.247367 \tValidation Loss: 1073.132700\n","Epoch: 62 \tTraining Loss: 4138.824250 \tValidation Loss: 1014.207854\n","Epoch: 63 \tTraining Loss: 4150.220491 \tValidation Loss: 906.632745\n","Epoch: 64 \tTraining Loss: 4048.727386 \tValidation Loss: 961.837957\n","Epoch: 65 \tTraining Loss: 4102.823576 \tValidation Loss: 962.172904\n","Epoch: 66 \tTraining Loss: 4090.797924 \tValidation Loss: 926.069977\n","Epoch: 67 \tTraining Loss: 3795.896220 \tValidation Loss: 977.148407\n","Epoch: 68 \tTraining Loss: 4147.295290 \tValidation Loss: 967.199216\n","Epoch: 69 \tTraining Loss: 4034.613794 \tValidation Loss: 962.481669\n","Epoch: 70 \tTraining Loss: 4035.007639 \tValidation Loss: 939.667193\n","Epoch: 71 \tTraining Loss: 4125.533224 \tValidation Loss: 933.221110\n","Epoch: 72 \tTraining Loss: 4069.902220 \tValidation Loss: 942.480131\n","Epoch: 73 \tTraining Loss: 4088.686642 \tValidation Loss: 942.801775\n","Epoch: 74 \tTraining Loss: 4055.526095 \tValidation Loss: 840.282099\n","Epoch: 75 \tTraining Loss: 3991.432550 \tValidation Loss: 1008.420676\n","Epoch: 76 \tTraining Loss: 4152.975843 \tValidation Loss: 938.096740\n","Epoch: 77 \tTraining Loss: 4020.895529 \tValidation Loss: 965.944157\n","Epoch: 78 \tTraining Loss: 3110.366464 \tValidation Loss: 952.431310\n","Epoch: 79 \tTraining Loss: 3930.716965 \tValidation Loss: 942.969531\n","Epoch: 80 \tTraining Loss: 3866.670220 \tValidation Loss: 959.970517\n","Epoch: 81 \tTraining Loss: 4018.208919 \tValidation Loss: 949.652657\n","Epoch: 82 \tTraining Loss: 3921.949018 \tValidation Loss: 944.501743\n","Epoch: 83 \tTraining Loss: 3961.154764 \tValidation Loss: 958.912900\n","Epoch: 84 \tTraining Loss: 3983.718756 \tValidation Loss: 933.232008\n","Epoch: 85 \tTraining Loss: 4027.256970 \tValidation Loss: 948.677360\n","Epoch: 86 \tTraining Loss: 3874.818439 \tValidation Loss: 956.284596\n","Validation loss decreased (826.242358 --> 821.236630).  Saving model ...\n","Epoch: 87 \tTraining Loss: 3884.709025 \tValidation Loss: 821.236630\n","Epoch: 88 \tTraining Loss: 4031.747770 \tValidation Loss: 842.030395\n","Epoch: 89 \tTraining Loss: 3881.810312 \tValidation Loss: 958.215557\n","Epoch: 90 \tTraining Loss: 3968.084079 \tValidation Loss: 905.333580\n","Epoch: 91 \tTraining Loss: 3956.481463 \tValidation Loss: 954.600212\n","Epoch: 92 \tTraining Loss: 3788.388302 \tValidation Loss: 960.617429\n","Epoch: 93 \tTraining Loss: 3970.818295 \tValidation Loss: 933.132775\n","Epoch: 94 \tTraining Loss: 3976.993080 \tValidation Loss: 909.227254\n","Epoch: 95 \tTraining Loss: 3985.898502 \tValidation Loss: 918.199320\n","Epoch: 96 \tTraining Loss: 3948.008072 \tValidation Loss: 839.805573\n","Epoch: 97 \tTraining Loss: 3938.150116 \tValidation Loss: 960.602816\n","Epoch: 98 \tTraining Loss: 3959.165266 \tValidation Loss: 950.772389\n","Epoch: 99 \tTraining Loss: 3986.261632 \tValidation Loss: 925.595149\n","Epoch: 100 \tTraining Loss: 3696.849486 \tValidation Loss: 938.048900\n","Epoch: 101 \tTraining Loss: 3831.556138 \tValidation Loss: 955.273490\n","Epoch: 102 \tTraining Loss: 3899.697022 \tValidation Loss: 970.195425\n","Epoch: 103 \tTraining Loss: 3584.741736 \tValidation Loss: 960.626984\n","Epoch: 104 \tTraining Loss: 3852.817641 \tValidation Loss: 954.021551\n","Validation loss decreased (821.236630 --> 686.134851).  Saving model ...\n","Epoch: 105 \tTraining Loss: 3898.904234 \tValidation Loss: 686.134851\n","Epoch: 106 \tTraining Loss: 3927.646464 \tValidation Loss: 817.860457\n","Epoch: 107 \tTraining Loss: 3883.992979 \tValidation Loss: 866.125686\n","Epoch: 108 \tTraining Loss: 3982.654794 \tValidation Loss: 959.192565\n","Epoch: 109 \tTraining Loss: 3933.059689 \tValidation Loss: 960.835640\n","Epoch: 110 \tTraining Loss: 3839.016282 \tValidation Loss: 951.876734\n","Epoch: 111 \tTraining Loss: 3927.581441 \tValidation Loss: 951.600701\n","Epoch: 112 \tTraining Loss: 3953.218749 \tValidation Loss: 951.169358\n","Epoch: 113 \tTraining Loss: 3875.593121 \tValidation Loss: 946.823861\n","Epoch: 114 \tTraining Loss: 3909.860712 \tValidation Loss: 960.533286\n","Epoch: 115 \tTraining Loss: 3935.103749 \tValidation Loss: 973.840727\n","Epoch: 116 \tTraining Loss: 3824.950579 \tValidation Loss: 935.004408\n","Epoch: 117 \tTraining Loss: 4003.172553 \tValidation Loss: 903.461645\n","Epoch: 118 \tTraining Loss: 3943.298866 \tValidation Loss: 951.794443\n","Epoch: 119 \tTraining Loss: 3947.015909 \tValidation Loss: 947.644554\n","Epoch: 120 \tTraining Loss: 3900.040645 \tValidation Loss: 900.445399\n","Epoch: 121 \tTraining Loss: 3919.212613 \tValidation Loss: 916.726483\n","Epoch: 122 \tTraining Loss: 3890.693921 \tValidation Loss: 922.839198\n","Epoch: 123 \tTraining Loss: 3903.416673 \tValidation Loss: 922.226310\n","Epoch: 124 \tTraining Loss: 3575.283223 \tValidation Loss: 941.171741\n","Epoch: 125 \tTraining Loss: 3436.880896 \tValidation Loss: 937.806189\n","Epoch: 126 \tTraining Loss: 3895.681430 \tValidation Loss: 956.774494\n","Epoch: 127 \tTraining Loss: 3888.372303 \tValidation Loss: 750.035700\n","Epoch: 128 \tTraining Loss: 3919.007015 \tValidation Loss: 951.480005\n","Epoch: 129 \tTraining Loss: 3949.808791 \tValidation Loss: 952.989084\n","Epoch: 130 \tTraining Loss: 3888.520019 \tValidation Loss: 944.213088\n","Epoch: 131 \tTraining Loss: 3965.543775 \tValidation Loss: 949.972197\n","Epoch: 132 \tTraining Loss: 3905.122032 \tValidation Loss: 933.998634\n","Epoch: 133 \tTraining Loss: 3935.252929 \tValidation Loss: 934.713846\n","Epoch: 134 \tTraining Loss: 3512.434405 \tValidation Loss: 953.519575\n","Epoch: 135 \tTraining Loss: 3955.216780 \tValidation Loss: 950.607637\n","Epoch: 136 \tTraining Loss: 3970.025690 \tValidation Loss: 957.236112\n","Epoch: 137 \tTraining Loss: 3928.170874 \tValidation Loss: 831.009051\n","Epoch: 138 \tTraining Loss: 3900.873248 \tValidation Loss: 954.246059\n","Epoch: 139 \tTraining Loss: 3874.931531 \tValidation Loss: 950.273338\n","Epoch: 140 \tTraining Loss: 3797.565418 \tValidation Loss: 896.843971\n","Epoch: 141 \tTraining Loss: 3923.733034 \tValidation Loss: 736.944530\n","Epoch: 142 \tTraining Loss: 3879.206722 \tValidation Loss: 912.266101\n","Epoch: 143 \tTraining Loss: 3833.029541 \tValidation Loss: 932.609358\n","Epoch: 144 \tTraining Loss: 3703.453633 \tValidation Loss: 805.974954\n","Epoch: 145 \tTraining Loss: 3902.962707 \tValidation Loss: 897.659189\n","Epoch: 146 \tTraining Loss: 3827.417131 \tValidation Loss: 927.317976\n","Epoch: 147 \tTraining Loss: 3942.422723 \tValidation Loss: 956.230874\n","Epoch: 148 \tTraining Loss: 3872.581927 \tValidation Loss: 944.296371\n","Epoch: 149 \tTraining Loss: 3853.202416 \tValidation Loss: 867.030378\n","Epoch: 150 \tTraining Loss: 3852.535661 \tValidation Loss: 922.425064\n","Epoch: 151 \tTraining Loss: 3859.447919 \tValidation Loss: 943.816486\n","Epoch: 152 \tTraining Loss: 3852.204243 \tValidation Loss: 938.464315\n","Epoch: 153 \tTraining Loss: 3916.001362 \tValidation Loss: 790.163682\n","Epoch: 154 \tTraining Loss: 3861.550000 \tValidation Loss: 950.334446\n","Epoch: 155 \tTraining Loss: 3895.096539 \tValidation Loss: 919.476355\n","Epoch: 156 \tTraining Loss: 3853.199730 \tValidation Loss: 954.340687\n","Epoch: 157 \tTraining Loss: 3825.232060 \tValidation Loss: 917.653944\n","Epoch: 158 \tTraining Loss: 3877.212665 \tValidation Loss: 901.807944\n","Epoch: 159 \tTraining Loss: 3868.545157 \tValidation Loss: 917.903498\n","Epoch: 160 \tTraining Loss: 3957.919905 \tValidation Loss: 930.019592\n","Epoch: 161 \tTraining Loss: 3881.980869 \tValidation Loss: 898.005400\n","Epoch: 162 \tTraining Loss: 3768.268926 \tValidation Loss: 949.861536\n","Epoch: 163 \tTraining Loss: 3833.436163 \tValidation Loss: 928.268745\n","Epoch: 164 \tTraining Loss: 3830.257455 \tValidation Loss: 915.828100\n","Epoch: 165 \tTraining Loss: 3780.652626 \tValidation Loss: 919.847624\n","Epoch: 166 \tTraining Loss: 3924.305331 \tValidation Loss: 924.877051\n","Epoch: 167 \tTraining Loss: 3931.622856 \tValidation Loss: 929.500151\n","Epoch: 168 \tTraining Loss: 3875.610258 \tValidation Loss: 873.928296\n","Epoch: 169 \tTraining Loss: 3684.575731 \tValidation Loss: 906.166183\n","Epoch: 170 \tTraining Loss: 3800.564803 \tValidation Loss: 930.821030\n","Epoch: 171 \tTraining Loss: 3915.773582 \tValidation Loss: 912.480801\n","Epoch: 172 \tTraining Loss: 3823.694937 \tValidation Loss: 937.771662\n","Epoch: 173 \tTraining Loss: 3772.325674 \tValidation Loss: 920.159569\n","Epoch: 174 \tTraining Loss: 3845.612172 \tValidation Loss: 913.369449\n","Epoch: 175 \tTraining Loss: 3849.356631 \tValidation Loss: 947.002919\n","Epoch: 176 \tTraining Loss: 3819.344641 \tValidation Loss: 782.258159\n","Epoch: 177 \tTraining Loss: 3808.115069 \tValidation Loss: 922.582336\n","Epoch: 178 \tTraining Loss: 3819.560194 \tValidation Loss: 965.064566\n","Epoch: 179 \tTraining Loss: 3837.922943 \tValidation Loss: 920.221827\n","Epoch: 180 \tTraining Loss: 3362.176390 \tValidation Loss: 912.777815\n","Epoch: 181 \tTraining Loss: 3766.848251 \tValidation Loss: 924.658643\n","Epoch: 182 \tTraining Loss: 3967.251417 \tValidation Loss: 915.362785\n","Epoch: 183 \tTraining Loss: 3880.084442 \tValidation Loss: 940.009726\n","Epoch: 184 \tTraining Loss: 3847.809949 \tValidation Loss: 927.084925\n","Epoch: 185 \tTraining Loss: 3906.048935 \tValidation Loss: 915.692598\n","Epoch: 186 \tTraining Loss: 3843.228275 \tValidation Loss: 928.114793\n","Epoch: 187 \tTraining Loss: 3830.575471 \tValidation Loss: 922.377654\n","Epoch: 188 \tTraining Loss: 3735.196999 \tValidation Loss: 912.658345\n","Epoch: 189 \tTraining Loss: 3831.968903 \tValidation Loss: 924.206181\n","Epoch: 190 \tTraining Loss: 3835.144896 \tValidation Loss: 902.491770\n","Epoch: 191 \tTraining Loss: 3762.894109 \tValidation Loss: 943.978877\n","Epoch: 192 \tTraining Loss: 3832.422348 \tValidation Loss: 911.461522\n","Epoch: 193 \tTraining Loss: 3747.773779 \tValidation Loss: 911.405008\n","Epoch: 194 \tTraining Loss: 3738.890977 \tValidation Loss: 902.295315\n","Epoch: 195 \tTraining Loss: 3844.341465 \tValidation Loss: 946.767855\n","Epoch: 196 \tTraining Loss: 3798.921984 \tValidation Loss: 920.480511\n","Epoch: 197 \tTraining Loss: 3849.063571 \tValidation Loss: 897.145026\n","Epoch: 198 \tTraining Loss: 3800.395512 \tValidation Loss: 912.792409\n","Epoch: 199 \tTraining Loss: 3753.565685 \tValidation Loss: 901.423561\n","Epoch: 200 \tTraining Loss: 3748.681098 \tValidation Loss: 914.606780\n","Epoch: 201 \tTraining Loss: 3844.419911 \tValidation Loss: 817.747479\n","Epoch: 202 \tTraining Loss: 3723.378840 \tValidation Loss: 944.524930\n","Epoch: 203 \tTraining Loss: 3826.671275 \tValidation Loss: 945.589676\n","Epoch: 204 \tTraining Loss: 3803.181716 \tValidation Loss: 967.205486\n","Epoch: 205 \tTraining Loss: 3769.301497 \tValidation Loss: 928.394589\n","Epoch: 206 \tTraining Loss: 3743.715853 \tValidation Loss: 952.945636\n","Epoch: 207 \tTraining Loss: 3797.273146 \tValidation Loss: 909.741581\n","Epoch: 208 \tTraining Loss: 3836.973163 \tValidation Loss: 950.227410\n","Epoch: 209 \tTraining Loss: 3868.776414 \tValidation Loss: 923.699465\n","Epoch: 210 \tTraining Loss: 3824.230854 \tValidation Loss: 924.784710\n","Epoch: 211 \tTraining Loss: 3886.651748 \tValidation Loss: 801.195239\n","Epoch: 212 \tTraining Loss: 3799.177593 \tValidation Loss: 946.494225\n","Epoch: 213 \tTraining Loss: 3755.015146 \tValidation Loss: 919.091041\n","Epoch: 214 \tTraining Loss: 3661.819589 \tValidation Loss: 900.549385\n","Epoch: 215 \tTraining Loss: 3730.251688 \tValidation Loss: 924.899511\n","Epoch: 216 \tTraining Loss: 3704.551580 \tValidation Loss: 924.785896\n","Epoch: 217 \tTraining Loss: 3872.717923 \tValidation Loss: 899.341553\n","Epoch: 218 \tTraining Loss: 3788.624575 \tValidation Loss: 919.218371\n","Epoch: 219 \tTraining Loss: 3653.707480 \tValidation Loss: 936.479872\n","Epoch: 220 \tTraining Loss: 3758.059357 \tValidation Loss: 935.698545\n","Epoch: 221 \tTraining Loss: 3696.128934 \tValidation Loss: 872.213008\n","Epoch: 222 \tTraining Loss: 3762.379577 \tValidation Loss: 910.536020\n","Epoch: 223 \tTraining Loss: 3750.986974 \tValidation Loss: 946.954234\n","Epoch: 224 \tTraining Loss: 3876.110835 \tValidation Loss: 905.507466\n","Epoch: 225 \tTraining Loss: 3781.948660 \tValidation Loss: 909.104217\n","Epoch: 226 \tTraining Loss: 3764.025657 \tValidation Loss: 933.698592\n","Epoch: 227 \tTraining Loss: 3770.007466 \tValidation Loss: 923.582065\n","Epoch: 228 \tTraining Loss: 3778.922956 \tValidation Loss: 930.563756\n","Epoch: 229 \tTraining Loss: 3879.790756 \tValidation Loss: 925.103676\n","Epoch: 230 \tTraining Loss: 3797.248303 \tValidation Loss: 915.051266\n","Epoch: 231 \tTraining Loss: 3815.678995 \tValidation Loss: 955.173301\n","Epoch: 232 \tTraining Loss: 3750.440319 \tValidation Loss: 920.617474\n","Epoch: 233 \tTraining Loss: 3684.074432 \tValidation Loss: 921.784806\n","Epoch: 234 \tTraining Loss: 3788.805537 \tValidation Loss: 902.408601\n","Epoch: 235 \tTraining Loss: 3735.200918 \tValidation Loss: 918.567759\n","Epoch: 236 \tTraining Loss: 3773.427219 \tValidation Loss: 923.013997\n","Epoch: 237 \tTraining Loss: 3889.533553 \tValidation Loss: 948.607115\n","Epoch: 238 \tTraining Loss: 3757.647529 \tValidation Loss: 913.951501\n","Epoch: 239 \tTraining Loss: 3789.239673 \tValidation Loss: 934.209030\n","Epoch: 240 \tTraining Loss: 3706.989941 \tValidation Loss: 931.148379\n","Epoch: 241 \tTraining Loss: 3815.842160 \tValidation Loss: 928.580767\n","Epoch: 242 \tTraining Loss: 3816.048969 \tValidation Loss: 915.863639\n","Epoch: 243 \tTraining Loss: 3820.138243 \tValidation Loss: 905.954802\n","Epoch: 244 \tTraining Loss: 3734.986200 \tValidation Loss: 908.491021\n","Epoch: 245 \tTraining Loss: 3818.606003 \tValidation Loss: 924.686195\n","Epoch: 246 \tTraining Loss: 3680.113107 \tValidation Loss: 922.002814\n","Epoch: 247 \tTraining Loss: 3462.258657 \tValidation Loss: 918.306382\n","Epoch: 248 \tTraining Loss: 3728.460006 \tValidation Loss: 916.841065\n","Epoch: 249 \tTraining Loss: 3679.036001 \tValidation Loss: 892.376744\n","Epoch: 250 \tTraining Loss: 3762.547817 \tValidation Loss: 918.589541\n","Epoch: 251 \tTraining Loss: 3637.022793 \tValidation Loss: 902.661486\n","Epoch: 252 \tTraining Loss: 3861.122695 \tValidation Loss: 729.228480\n","Epoch: 253 \tTraining Loss: 3699.554810 \tValidation Loss: 942.083565\n","Epoch: 254 \tTraining Loss: 3695.628547 \tValidation Loss: 937.522185\n","Epoch: 255 \tTraining Loss: 3790.194696 \tValidation Loss: 904.472672\n","Epoch: 256 \tTraining Loss: 3732.437416 \tValidation Loss: 922.775010\n","Epoch: 257 \tTraining Loss: 3695.565250 \tValidation Loss: 812.663682\n","Epoch: 258 \tTraining Loss: 3761.283512 \tValidation Loss: 921.812306\n","Epoch: 259 \tTraining Loss: 3699.254148 \tValidation Loss: 933.780725\n","Epoch: 260 \tTraining Loss: 3700.274415 \tValidation Loss: 914.012227\n","Epoch: 261 \tTraining Loss: 3680.133822 \tValidation Loss: 925.591527\n","Epoch: 262 \tTraining Loss: 3731.064180 \tValidation Loss: 907.980712\n","Epoch: 263 \tTraining Loss: 3692.006306 \tValidation Loss: 909.872060\n","Epoch: 264 \tTraining Loss: 3639.899494 \tValidation Loss: 940.339260\n","Epoch: 265 \tTraining Loss: 3705.156692 \tValidation Loss: 926.021073\n","Epoch: 266 \tTraining Loss: 3743.393503 \tValidation Loss: 919.245300\n","Epoch: 267 \tTraining Loss: 3625.899208 \tValidation Loss: 922.753665\n","Epoch: 268 \tTraining Loss: 3803.634394 \tValidation Loss: 939.460525\n","Epoch: 269 \tTraining Loss: 3823.560459 \tValidation Loss: 944.228897\n","Epoch: 270 \tTraining Loss: 3830.936848 \tValidation Loss: 827.993083\n","Epoch: 271 \tTraining Loss: 3661.014280 \tValidation Loss: 923.334974\n","Epoch: 272 \tTraining Loss: 3709.638537 \tValidation Loss: 928.008484\n","Epoch: 273 \tTraining Loss: 3696.330117 \tValidation Loss: 915.795919\n","Epoch: 274 \tTraining Loss: 3651.518793 \tValidation Loss: 936.751548\n","Epoch: 275 \tTraining Loss: 3665.296900 \tValidation Loss: 919.178017\n","Epoch: 276 \tTraining Loss: 3721.737938 \tValidation Loss: 918.969718\n","Epoch: 277 \tTraining Loss: 3763.114536 \tValidation Loss: 936.609512\n","Epoch: 278 \tTraining Loss: 3749.182575 \tValidation Loss: 929.988302\n","Epoch: 279 \tTraining Loss: 3705.182624 \tValidation Loss: 882.299340\n","Epoch: 280 \tTraining Loss: 3700.958997 \tValidation Loss: 815.379020\n","Epoch: 281 \tTraining Loss: 3686.878452 \tValidation Loss: 922.983289\n","Epoch: 282 \tTraining Loss: 3676.419016 \tValidation Loss: 909.661958\n","Epoch: 283 \tTraining Loss: 3717.519057 \tValidation Loss: 900.386841\n","Epoch: 284 \tTraining Loss: 3743.263624 \tValidation Loss: 896.064700\n","Epoch: 285 \tTraining Loss: 3724.817435 \tValidation Loss: 896.412044\n","Epoch: 286 \tTraining Loss: 3742.891617 \tValidation Loss: 878.915560\n","Epoch: 287 \tTraining Loss: 3690.676383 \tValidation Loss: 918.817867\n","Epoch: 288 \tTraining Loss: 3726.984467 \tValidation Loss: 904.573050\n","Epoch: 289 \tTraining Loss: 3764.097214 \tValidation Loss: 917.216106\n","Epoch: 290 \tTraining Loss: 3620.335370 \tValidation Loss: 885.505562\n","Epoch: 291 \tTraining Loss: 3319.061270 \tValidation Loss: 924.236584\n","Epoch: 292 \tTraining Loss: 3704.670572 \tValidation Loss: 852.623878\n","Epoch: 293 \tTraining Loss: 3629.589723 \tValidation Loss: 917.505723\n","Epoch: 294 \tTraining Loss: 3683.929185 \tValidation Loss: 914.921870\n","Epoch: 295 \tTraining Loss: 3681.928711 \tValidation Loss: 911.757383\n","Epoch: 296 \tTraining Loss: 3711.304542 \tValidation Loss: 845.687477\n","Epoch: 297 \tTraining Loss: 3715.595707 \tValidation Loss: 720.178986\n","Epoch: 298 \tTraining Loss: 3696.431609 \tValidation Loss: 913.321040\n","Epoch: 299 \tTraining Loss: 3719.108494 \tValidation Loss: 911.335388\n","Epoch: 300 \tTraining Loss: 3722.749558 \tValidation Loss: 920.178476\n","Epoch: 301 \tTraining Loss: 3700.509603 \tValidation Loss: 907.460433\n","Epoch: 302 \tTraining Loss: 3635.004743 \tValidation Loss: 914.119037\n","Epoch: 303 \tTraining Loss: 3781.916682 \tValidation Loss: 887.992085\n","Epoch: 304 \tTraining Loss: 3723.388119 \tValidation Loss: 902.640530\n","Epoch: 305 \tTraining Loss: 3571.313864 \tValidation Loss: 904.305974\n","Epoch: 306 \tTraining Loss: 3720.946837 \tValidation Loss: 897.840889\n","Epoch: 307 \tTraining Loss: 3620.203721 \tValidation Loss: 909.825163\n","Epoch: 308 \tTraining Loss: 3695.958697 \tValidation Loss: 889.926603\n","Epoch: 309 \tTraining Loss: 3643.468317 \tValidation Loss: 923.606199\n","Epoch: 310 \tTraining Loss: 3624.363649 \tValidation Loss: 834.705686\n","Epoch: 311 \tTraining Loss: 3644.678652 \tValidation Loss: 908.341729\n","Epoch: 312 \tTraining Loss: 3695.859920 \tValidation Loss: 910.413485\n","Epoch: 313 \tTraining Loss: 3686.892223 \tValidation Loss: 908.533307\n","Epoch: 314 \tTraining Loss: 3738.948034 \tValidation Loss: 914.821337\n","Epoch: 315 \tTraining Loss: 3689.882222 \tValidation Loss: 826.285792\n","Epoch: 316 \tTraining Loss: 3603.098903 \tValidation Loss: 865.772555\n","Epoch: 317 \tTraining Loss: 3598.400232 \tValidation Loss: 875.939756\n","Epoch: 318 \tTraining Loss: 3645.571340 \tValidation Loss: 895.992620\n","Epoch: 319 \tTraining Loss: 3538.561468 \tValidation Loss: 878.592392\n","Epoch: 320 \tTraining Loss: 3653.619101 \tValidation Loss: 815.363574\n","Epoch: 321 \tTraining Loss: 3715.472194 \tValidation Loss: 904.582350\n","Epoch: 322 \tTraining Loss: 3662.085245 \tValidation Loss: 907.526916\n","Epoch: 323 \tTraining Loss: 3603.759044 \tValidation Loss: 896.470000\n","Epoch: 324 \tTraining Loss: 3634.384235 \tValidation Loss: 918.491019\n","Epoch: 325 \tTraining Loss: 3623.975691 \tValidation Loss: 912.525388\n","Epoch: 326 \tTraining Loss: 3638.333999 \tValidation Loss: 887.723099\n","Epoch: 327 \tTraining Loss: 3655.164717 \tValidation Loss: 899.188293\n","Epoch: 328 \tTraining Loss: 3633.419444 \tValidation Loss: 907.135501\n","Epoch: 329 \tTraining Loss: 3613.469670 \tValidation Loss: 914.032909\n","Epoch: 330 \tTraining Loss: 3671.764994 \tValidation Loss: 886.130451\n","Epoch: 331 \tTraining Loss: 3596.235153 \tValidation Loss: 924.061351\n","Epoch: 332 \tTraining Loss: 3231.448865 \tValidation Loss: 907.778500\n","Epoch: 333 \tTraining Loss: 3618.138761 \tValidation Loss: 906.401760\n","Epoch: 334 \tTraining Loss: 3637.511331 \tValidation Loss: 920.759065\n","Epoch: 335 \tTraining Loss: 3708.677004 \tValidation Loss: 897.351336\n","Epoch: 336 \tTraining Loss: 3526.368447 \tValidation Loss: 888.777114\n","Epoch: 337 \tTraining Loss: 3695.375858 \tValidation Loss: 913.669659\n","Epoch: 338 \tTraining Loss: 3619.215191 \tValidation Loss: 808.767154\n","Epoch: 339 \tTraining Loss: 3620.029985 \tValidation Loss: 893.013295\n","Epoch: 340 \tTraining Loss: 3658.735051 \tValidation Loss: 807.569277\n","Epoch: 341 \tTraining Loss: 3617.560179 \tValidation Loss: 877.467420\n","Epoch: 342 \tTraining Loss: 3665.527699 \tValidation Loss: 870.433130\n","Epoch: 343 \tTraining Loss: 3715.464195 \tValidation Loss: 809.103321\n","Epoch: 344 \tTraining Loss: 3602.539033 \tValidation Loss: 768.286710\n","Epoch: 345 \tTraining Loss: 3603.557882 \tValidation Loss: 914.527720\n","Epoch: 346 \tTraining Loss: 3697.382652 \tValidation Loss: 888.470800\n","Epoch: 347 \tTraining Loss: 3499.850502 \tValidation Loss: 851.982441\n","Epoch: 348 \tTraining Loss: 3415.281069 \tValidation Loss: 878.602393\n","Epoch: 349 \tTraining Loss: 3705.555717 \tValidation Loss: 831.990592\n","Epoch: 350 \tTraining Loss: 3739.105594 \tValidation Loss: 895.131010\n","Epoch: 351 \tTraining Loss: 3743.740381 \tValidation Loss: 902.849679\n","Epoch: 352 \tTraining Loss: 3667.187548 \tValidation Loss: 906.121432\n","Epoch: 353 \tTraining Loss: 3664.419269 \tValidation Loss: 869.856925\n","Epoch: 354 \tTraining Loss: 3649.063638 \tValidation Loss: 893.097467\n","Epoch: 355 \tTraining Loss: 3676.212546 \tValidation Loss: 902.600960\n","Epoch: 356 \tTraining Loss: 3564.971145 \tValidation Loss: 908.073553\n","Epoch: 357 \tTraining Loss: 3663.679838 \tValidation Loss: 894.257983\n","Epoch: 358 \tTraining Loss: 3597.727959 \tValidation Loss: 909.350221\n","Epoch: 359 \tTraining Loss: 3572.416625 \tValidation Loss: 725.021012\n","Epoch: 360 \tTraining Loss: 3637.247662 \tValidation Loss: 898.810764\n","Epoch: 361 \tTraining Loss: 3638.638407 \tValidation Loss: 908.563472\n","Epoch: 362 \tTraining Loss: 3727.776871 \tValidation Loss: 915.457281\n","Epoch: 363 \tTraining Loss: 3630.815946 \tValidation Loss: 913.432116\n","Epoch: 364 \tTraining Loss: 3701.397388 \tValidation Loss: 853.950177\n","Epoch: 365 \tTraining Loss: 3637.707670 \tValidation Loss: 873.731833\n","Epoch: 366 \tTraining Loss: 3685.653671 \tValidation Loss: 919.199127\n","Epoch: 367 \tTraining Loss: 3618.260614 \tValidation Loss: 916.626279\n","Epoch: 368 \tTraining Loss: 3614.663822 \tValidation Loss: 919.913056\n","Epoch: 369 \tTraining Loss: 3593.233540 \tValidation Loss: 781.144918\n","Epoch: 370 \tTraining Loss: 3559.338765 \tValidation Loss: 917.752149\n","Epoch: 371 \tTraining Loss: 3587.870673 \tValidation Loss: 892.425512\n","Epoch: 372 \tTraining Loss: 3570.636379 \tValidation Loss: 897.147509\n","Epoch: 373 \tTraining Loss: 3632.019557 \tValidation Loss: 912.851209\n","Epoch: 374 \tTraining Loss: 3617.449277 \tValidation Loss: 907.092779\n","Epoch: 375 \tTraining Loss: 3635.448285 \tValidation Loss: 910.513249\n","Epoch: 376 \tTraining Loss: 3615.824289 \tValidation Loss: 905.232423\n","Epoch: 377 \tTraining Loss: 3663.072034 \tValidation Loss: 911.484791\n","Epoch: 378 \tTraining Loss: 3473.488199 \tValidation Loss: 907.817486\n","Epoch: 379 \tTraining Loss: 3553.029615 \tValidation Loss: 912.962773\n","Epoch: 380 \tTraining Loss: 3508.819844 \tValidation Loss: 703.749281\n","Epoch: 381 \tTraining Loss: 3533.888509 \tValidation Loss: 924.317793\n","Epoch: 382 \tTraining Loss: 3629.560503 \tValidation Loss: 924.718387\n","Epoch: 383 \tTraining Loss: 3695.077498 \tValidation Loss: 925.001631\n","Epoch: 384 \tTraining Loss: 3575.649250 \tValidation Loss: 714.510530\n","Epoch: 385 \tTraining Loss: 3566.378588 \tValidation Loss: 916.527340\n","Epoch: 386 \tTraining Loss: 3533.722657 \tValidation Loss: 933.183346\n","Validation loss decreased (686.134851 --> 642.829318).  Saving model ...\n","Epoch: 387 \tTraining Loss: 3573.038820 \tValidation Loss: 642.829318\n","Epoch: 388 \tTraining Loss: 3692.292549 \tValidation Loss: 933.492989\n","Epoch: 389 \tTraining Loss: 3697.063977 \tValidation Loss: 923.881933\n","Epoch: 390 \tTraining Loss: 3640.119638 \tValidation Loss: 922.359502\n","Epoch: 391 \tTraining Loss: 3634.812868 \tValidation Loss: 854.638008\n","Epoch: 392 \tTraining Loss: 3721.460966 \tValidation Loss: 927.238286\n","Epoch: 393 \tTraining Loss: 3588.703511 \tValidation Loss: 928.550975\n","Epoch: 394 \tTraining Loss: 3633.251668 \tValidation Loss: 923.112673\n","Epoch: 395 \tTraining Loss: 3578.653287 \tValidation Loss: 892.294006\n","Epoch: 396 \tTraining Loss: 3476.178192 \tValidation Loss: 907.180144\n","Epoch: 397 \tTraining Loss: 3612.604861 \tValidation Loss: 907.409999\n","Epoch: 398 \tTraining Loss: 3567.475261 \tValidation Loss: 906.384060\n","Epoch: 399 \tTraining Loss: 3607.108436 \tValidation Loss: 894.684867\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OqtVdqkZP21K","colab_type":"code","outputId":"f79edf13-0848-4e19-9654-6b472476c90e","executionInfo":{"status":"ok","timestamp":1586929265664,"user_tz":300,"elapsed":809,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["net.load_state_dict(torch.load('mixed_model.pt'))"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":440}]},{"cell_type":"code","metadata":{"id":"iRCN4REPP93Y","colab_type":"code","outputId":"f5aa18ba-328b-40d6-d252-4df6ee482872","executionInfo":{"status":"ok","timestamp":1586929260594,"user_tz":300,"elapsed":1369,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":199}},"source":["from sklearn.metrics import mean_absolute_error\n","\n","# track test loss\n","test_loss = 0.0\n","mean_abs = 0.0\n","\n","net.eval()\n","# iterate over test data\n","#bin_op.binarization()\n","test_h = net.init_hidden(batch_size)\n","\n","for batch_idx, (X_temp, X_num, X_cat, labels) in enumerate(test_loader):\n","    if(train_on_gpu):\n","        X_temp, X_num, X_cat, labels = X_temp.cuda(), X_num.cuda(), X_cat.type(torch.LongTensor).cuda(), labels.cuda()\n","    \n","    test_h = tuple([each.data for each in test_h])\n","\n","    # get the output from the model\n","    output, h = net(X_temp, h, X_num, X_cat)\n","    # / (weight_lstm + weight_num + weight_cat)\n","\n","    # calculate the loss and perform backprop\n","    loss = criterion(output.squeeze(), labels.double())\n","\n","    out_np = output.detach().squeeze().cpu().numpy().astype(int).T\n","    target_np = labels.detach().cpu().numpy().astype(int).T\n","\n","    print([(out, tar) for out, tar in zip(out_np, target_np)])\n","\n","    mean_abs += mean_absolute_error(out_np, target_np) * batch_size\n","        \n","    test_loss += loss.item() * batch_size\n","\n","# calculate average losses\n","test_loss = np.sqrt(test_loss / len(test_loader.dataset))\n","mean_abs /= len(test_loader.dataset)\n","print('Test Loss: {:.6f}\\n'.format(test_loss))\n","print('Mean Abs Loss: {:.6f}\\n'.format(mean_abs))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[(4, 3), (4, 3), (4, 2), (7, 2), (4, 2), (4, 3), (8, 3), (3, 3), (3, 2), (3, 6), (3, 4), (3, 4), (3, 5), (3, 3), (3, 2), (3, 16)]\n","[(3, 4), (3, 10), (3, 5), (4, 3), (4, 8), (8, 2), (6, 9), (4, 66), (6, 3), (7, 9), (7, 11), (7, 8), (5, 25), (5, 4), (6, 3), (5, 2)]\n","[(10, 3), (5, 3), (6, 6), (5, 3), (6, 8), (6, 8), (5, 6), (5, 9), (6, 12), (6, 18), (5, 24), (5, 19), (8, 3), (5, 15), (7, 12), (7, 12)]\n","[(5, 17), (5, 9), (7, 3), (5, 12), (6, 33), (6, 11), (5, 6), (6, 2), (5, 7), (5, 5), (5, 2), (8, 11), (6, 23), (6, 4), (5, 7), (7, 5)]\n","[(5, 6), (7, 10), (7, 12), (6, 5), (7, 10), (4, 2), (8, 3), (7, 32), (7, 13), (4, 27), (8, 9), (8, 12), (5, 16), (8, 6), (4, 3), (7, 7)]\n","[(4, 2), (4, 6), (8, 10), (7, 6), (7, 2), (4, 3), (5, 5), (8, 2), (8, 5), (5, 17), (7, 18), (6, 3), (5, 8), (7, 2), (6, 15), (5, 5)]\n","Test Loss: 8.977377\n","\n","Mean Abs Loss: 4.866667\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5GSpw9ExoD6B","colab_type":"text"},"source":["## Deluxe Mixed Model"]},{"cell_type":"code","metadata":{"id":"98-Fe0zNWv-v","colab_type":"code","colab":{}},"source":["def train(model, criterion, optimizer, epochs):\n","    # training params\n","\n","    #epochs = 00 # 3-4 is approx where I noticed the validation loss stop decreasing\n","\n","    counter = 0\n","    print_every = 10\n","    clip = 8 # gradient clipping\n","\n","    # move model to GPU, if available\n","    if(train_on_gpu):\n","        model.cuda()\n","\n","    model.train()\n","    # train for some number of epochs\n","    valid_loss_min = np.Inf\n","    train_loss_min = np.Inf\n","\n","    best_train_epoch = 0\n","    best_valid_epoch = 0\n","\n","    for epoch in range(1, epochs + 1):\n","        # initialize hidden state\n","        h = model.init_hidden(batch_size)\n","\n","        train_loss = 0.0\n","        valid_loss = 0.0\n","\n","        model.train()\n","\n","        # batch loop\n","        for X_temp, X_num, X_cat, labels in train_loader:\n","            counter += 1\n","\n","            if(train_on_gpu):\n","                X_temp, X_num, X_cat, labels = X_temp.cuda(), X_num.cuda(), X_cat.type(torch.LongTensor).cuda(), labels.cuda()\n","\n","            # Creating new variables for the hidden state, otherwise\n","            # we'd backprop through the entire training history\n","            h = tuple([each.data for each in h])\n","            # zero accumulated gradients\n","            model.zero_grad()\n","\n","            # get the output from the model\n","            output, h = model(X_temp, h, X_num, X_cat)\n","\n","            # calculate the loss and perform backprop\n","            loss = criterion(output.squeeze(), labels.double())\n","            loss.backward()\n","            train_loss += loss.item()\n","            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","            nn.utils.clip_grad_norm_(model.parameters(), clip)\n","            optimizer.step()\n","\n","        if train_loss <= train_loss_min:\n","            torch.save(model.state_dict(), 'train_mixed_model2.pt')\n","            train_loss_min = train_loss\n","            best_train_epoch = epoch\n","\n","        model.eval()\n","\n","        val_h = model.init_hidden(batch_size)\n","\n","        for X_temp, X_num, X_cat, labels in valid_loader:\n","            \n","            \n","            if(train_on_gpu):\n","                X_temp, X_num, X_cat, labels = X_temp.cuda(), X_num.cuda(), X_cat.type(torch.LongTensor).cuda(), labels.cuda()\n","\n","            # Creating new variables for the hidden state, otherwise\n","            # we'd backprop through the entire training history\n","            val_h = tuple([each.data for each in val_h])\n","\n","            # get the output from the model\n","            output, h = model(X_temp, h, X_num, X_cat)\n","\n","            # calculate the loss and perform backprop\n","            loss = criterion(output.squeeze(), labels.double())\n","            valid_loss += loss.item()\n","\n","        # save model if validation loss has decreased\n","        if valid_loss <= valid_loss_min:\n","            torch.save(model.state_dict(), 'valid_mixed_model2.pt')\n","            valid_loss_min = valid_loss\n","            best_valid_epoch = epoch\n","            \n","        if epoch % 50 == 0:\n","            print('  Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n","                epoch, train_loss, valid_loss))\n","            \n","    print(\"Best Validation Loss: {:.6f} in epoch {}\".format(\n","        valid_loss_min, str(best_valid_epoch)\n","    ))\n","\n","    print(\"Best Train Loss: {:.6f} in epoch {}\".format(\n","        train_loss_min, str(best_train_epoch)\n","    ))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CNlwIrKlXixG","colab_type":"code","colab":{}},"source":["def test(model, criterion):\n","    # track test loss\n","    test_loss = 0.0\n","    mean_abs = 0.0\n","\n","    model.eval()\n","    # iterate over test data\n","    #bin_op.binarization()\n","    test_h = model.init_hidden(batch_size)\n","\n","    for batch_idx, (X_temp, X_num, X_cat, labels) in enumerate(test_loader):\n","        if(train_on_gpu):\n","            X_temp, X_num, X_cat, labels = X_temp.cuda(), X_num.cuda(), X_cat.type(torch.LongTensor).cuda(), labels.cuda()\n","        \n","        test_h = tuple([each.data for each in test_h])\n","\n","        # get the output from the model\n","        output, test_h = model(X_temp, test_h, X_num, X_cat)\n","        # / (weight_lstm + weight_num + weight_cat)\n","\n","        # calculate the loss and perform backprop\n","        loss = criterion(output.squeeze(), labels.double())\n","\n","        out_np = output.detach().squeeze().cpu().numpy().astype(int).T\n","        target_np = labels.detach().cpu().numpy().astype(int).T\n","\n","        #print([(out, tar) for out, tar in zip(out_np, target_np)])\n","\n","        mean_abs += mean_absolute_error(out_np, target_np) * batch_size\n","            \n","        test_loss += loss.item() * batch_size\n","\n","    # calculate average losses\n","    test_loss = np.sqrt(test_loss / len(test_loader.dataset))\n","    mean_abs /= len(test_loader.dataset)\n","    print('  Test Loss: {:.6f}\\n'.format(test_loss))\n","    print('  Mean Abs Loss: {:.6f}\\n'.format(mean_abs))\n","\n","    return mean_abs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"48KzkMyMXrt7","colab_type":"code","outputId":"f0a72de0-a5ca-4a70-b4f5-5c054b8a0c20","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1587004200072,"user_tz":300,"elapsed":21617093,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}}},"source":["learning_rates = [0.001, 0.0001]\n","a_hs = [2, 3, 4]\n","num_hidden_dims = [10, 15, 20]\n","cat_hidden_dims = [10, 15, 20]\n","lstm_outputs = [5, 10, 15]\n","combined_dims = [30]\n","\n","criterion = nn.MSELoss()\n","lstm_n_layers = 2\n","\n","#overall_test_square_error = np.Inf\n","overall_test_mean_error = np.Inf\n","\n","for combined_dim in combined_dims:\n","    for lstm_output in lstm_outputs:\n","        for cat_hidden_dim in cat_hidden_dims:\n","            for num_hidden_dim in num_hidden_dims:\n","                for a_h in a_hs:\n","                    for lr in learning_rates:\n","                        lstm_hidden_dim = np.int(len(train_idx) / (a_h * len(numerical_columns) + 1))\n","                        \n","                        net = MosquitoMixed(\n","                            len(numerical_columns), lstm_hidden_dim, lstm_n_layers, lstm_output,\n","                            len(non_num_columns), num_hidden_dim,\n","                            embedding_sizes, cat_hidden_dim,\n","                            combined_dim\n","                        ).double()\n","\n","                        \n","                        epochs = 300\n","\n","                        optimizer = torch.optim.RMSprop(net.parameters(), lr=lr)\n","\n","                        print('\\nCombined Dimension: ' + str(combined_dim))\n","                        print('LSTM Output Dimension: ' + str(lstm_output))\n","                        print('Categorical Dimension: ' + str(cat_hidden_dim))\n","                        print('Numerical Dimension: ' + str(num_hidden_dim))\n","                        print('LSTM Dimension: ' + str(lstm_hidden_dim))\n","                        print('Learning Rate: ' + str(lr))\n","                        \n","                        train(net, criterion, optimizer, epochs)\n","\n","                        net.load_state_dict(torch.load('train_mixed_model2.pt'))\n","\n","                        otme = test(net, criterion)\n","                        \n","\n","                        if otme <= overall_test_mean_error:\n","                            print(' Test loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","                            overall_test_mean_error,\n","                            otme))\n","                            torch.save(net.state_dict(), 'best_mixed_model2.pt')\n","                            overall_test_mean_error = otme\n","\n","                        net.load_state_dict(torch.load('valid_mixed_model2.pt'))\n","\n","                        otme = test(net, criterion)\n","\n","                        if otme <= overall_test_mean_error:\n","                            print(' Test loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","                            overall_test_mean_error,\n","                            otme))\n","                            torch.save(net.state_dict(), 'best_mixed_model2.pt')\n","                            overall_test_mean_error = otme\n","\n","                        \n","\n","\n"],"execution_count":18,"outputs":[{"output_type":"stream","text":["\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3686.190747 \tValidation Loss: 1128.261913\n","  Epoch: 100 \tTraining Loss: 3493.350038 \tValidation Loss: 1119.519289\n","  Epoch: 150 \tTraining Loss: 3559.671949 \tValidation Loss: 1096.093441\n","  Epoch: 200 \tTraining Loss: 3560.374504 \tValidation Loss: 1089.188774\n","  Epoch: 250 \tTraining Loss: 3352.008380 \tValidation Loss: 1089.470795\n","  Epoch: 300 \tTraining Loss: 3384.616347 \tValidation Loss: 1080.336851\n","Best Validation Loss: 765.051689 in epoch 129\n","Best Train Loss: 3116.485250 in epoch 297\n","  Test Loss: 8.468317\n","\n","  Mean Abs Loss: 4.571429\n","\n"," Test loss decreased (inf --> 4.571429).  Saving model ...\n","  Test Loss: 8.806661\n","\n","  Mean Abs Loss: 4.790476\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4615.307572 \tValidation Loss: 1256.561720\n","  Epoch: 100 \tTraining Loss: 4090.448227 \tValidation Loss: 804.739639\n","  Epoch: 150 \tTraining Loss: 3939.762257 \tValidation Loss: 1168.738967\n","  Epoch: 200 \tTraining Loss: 3919.447870 \tValidation Loss: 845.583039\n","  Epoch: 250 \tTraining Loss: 3756.135979 \tValidation Loss: 1155.186827\n","  Epoch: 300 \tTraining Loss: 3650.189436 \tValidation Loss: 1142.664311\n","Best Validation Loss: 517.742651 in epoch 145\n","Best Train Loss: 3058.850006 in epoch 174\n","  Test Loss: 9.036418\n","\n","  Mean Abs Loss: 5.028571\n","\n","  Test Loss: 9.035872\n","\n","  Mean Abs Loss: 5.038095\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3851.284908 \tValidation Loss: 997.006988\n","  Epoch: 100 \tTraining Loss: 3584.213023 \tValidation Loss: 1117.313893\n","  Epoch: 150 \tTraining Loss: 3524.386964 \tValidation Loss: 1104.344363\n","  Epoch: 200 \tTraining Loss: 3601.581236 \tValidation Loss: 1117.464103\n","  Epoch: 250 \tTraining Loss: 3415.142686 \tValidation Loss: 1090.427821\n","  Epoch: 300 \tTraining Loss: 3334.401924 \tValidation Loss: 815.652876\n","Best Validation Loss: 765.195374 in epoch 209\n","Best Train Loss: 2877.567120 in epoch 114\n","  Test Loss: 8.741856\n","\n","  Mean Abs Loss: 4.676190\n","\n","  Test Loss: 8.526074\n","\n","  Mean Abs Loss: 4.561905\n","\n"," Test loss decreased (4.571429 --> 4.561905).  Saving model ...\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4791.666724 \tValidation Loss: 1326.464919\n","  Epoch: 100 \tTraining Loss: 4286.861646 \tValidation Loss: 1171.132130\n","  Epoch: 150 \tTraining Loss: 3971.432188 \tValidation Loss: 1152.395867\n","  Epoch: 200 \tTraining Loss: 3864.654874 \tValidation Loss: 1151.724600\n","  Epoch: 250 \tTraining Loss: 3757.349929 \tValidation Loss: 1146.285123\n","  Epoch: 300 \tTraining Loss: 3853.560249 \tValidation Loss: 1137.515844\n","Best Validation Loss: 498.819632 in epoch 232\n","Best Train Loss: 2959.772803 in epoch 258\n","  Test Loss: 8.774620\n","\n","  Mean Abs Loss: 4.800000\n","\n","  Test Loss: 8.678957\n","\n","  Mean Abs Loss: 4.790476\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3694.766524 \tValidation Loss: 1132.831277\n","  Epoch: 100 \tTraining Loss: 3695.337619 \tValidation Loss: 1132.860481\n","  Epoch: 150 \tTraining Loss: 3902.293401 \tValidation Loss: 1126.353559\n","  Epoch: 200 \tTraining Loss: 3453.267166 \tValidation Loss: 1109.305062\n","  Epoch: 250 \tTraining Loss: 3415.477791 \tValidation Loss: 1115.766495\n","  Epoch: 300 \tTraining Loss: 3428.176806 \tValidation Loss: 1109.945207\n","Best Validation Loss: 640.177232 in epoch 291\n","Best Train Loss: 2648.070494 in epoch 213\n","  Test Loss: 8.647969\n","\n","  Mean Abs Loss: 4.609524\n","\n","  Test Loss: 8.641750\n","\n","  Mean Abs Loss: 4.638095\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4786.862228 \tValidation Loss: 1617.058009\n","  Epoch: 100 \tTraining Loss: 4397.385562 \tValidation Loss: 1359.831746\n","  Epoch: 150 \tTraining Loss: 3754.779905 \tValidation Loss: 1320.871960\n","  Epoch: 200 \tTraining Loss: 3933.520448 \tValidation Loss: 1226.221670\n","  Epoch: 250 \tTraining Loss: 3943.761505 \tValidation Loss: 1229.601742\n","  Epoch: 300 \tTraining Loss: 3875.764482 \tValidation Loss: 1231.621125\n","Best Validation Loss: 876.435327 in epoch 260\n","Best Train Loss: 3225.251569 in epoch 119\n","  Test Loss: 9.531657\n","\n","  Mean Abs Loss: 5.314286\n","\n","  Test Loss: 8.906489\n","\n","  Mean Abs Loss: 4.914286\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3717.992283 \tValidation Loss: 959.514464\n","  Epoch: 100 \tTraining Loss: 3719.356968 \tValidation Loss: 853.915261\n","  Epoch: 150 \tTraining Loss: 3560.086681 \tValidation Loss: 1102.460216\n","  Epoch: 200 \tTraining Loss: 3431.350643 \tValidation Loss: 1080.641670\n","  Epoch: 250 \tTraining Loss: 3360.931220 \tValidation Loss: 1072.195561\n","  Epoch: 300 \tTraining Loss: 3248.523648 \tValidation Loss: 1099.763760\n","Best Validation Loss: 748.446887 in epoch 244\n","Best Train Loss: 3155.738871 in epoch 234\n","  Test Loss: 8.653070\n","\n","  Mean Abs Loss: 4.638095\n","\n","  Test Loss: 8.837915\n","\n","  Mean Abs Loss: 4.752381\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4703.256808 \tValidation Loss: 996.138368\n","  Epoch: 100 \tTraining Loss: 4142.757308 \tValidation Loss: 1174.316323\n","  Epoch: 150 \tTraining Loss: 3820.199753 \tValidation Loss: 1137.512898\n","  Epoch: 200 \tTraining Loss: 3824.527287 \tValidation Loss: 1121.021210\n","  Epoch: 250 \tTraining Loss: 3698.499720 \tValidation Loss: 1146.034798\n","  Epoch: 300 \tTraining Loss: 3668.241880 \tValidation Loss: 1147.530445\n","Best Validation Loss: 517.130317 in epoch 232\n","Best Train Loss: 2971.973204 in epoch 274\n","  Test Loss: 8.839441\n","\n","  Mean Abs Loss: 4.876190\n","\n","  Test Loss: 8.716485\n","\n","  Mean Abs Loss: 4.838095\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3637.988576 \tValidation Loss: 1124.427678\n","  Epoch: 100 \tTraining Loss: 3613.298164 \tValidation Loss: 1120.155579\n","  Epoch: 150 \tTraining Loss: 3265.983755 \tValidation Loss: 1113.013411\n","  Epoch: 200 \tTraining Loss: 3360.356701 \tValidation Loss: 1119.464404\n","  Epoch: 250 \tTraining Loss: 3379.678879 \tValidation Loss: 1091.792105\n","  Epoch: 300 \tTraining Loss: 3272.764718 \tValidation Loss: 1108.019580\n","Best Validation Loss: 636.572729 in epoch 170\n","Best Train Loss: 2793.767272 in epoch 56\n","  Test Loss: 8.784376\n","\n","  Mean Abs Loss: 4.847619\n","\n","  Test Loss: 8.656952\n","\n","  Mean Abs Loss: 4.609524\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4847.411913 \tValidation Loss: 1695.998195\n","  Epoch: 100 \tTraining Loss: 4310.081639 \tValidation Loss: 1414.366891\n","  Epoch: 150 \tTraining Loss: 4085.396913 \tValidation Loss: 1208.927265\n","  Epoch: 200 \tTraining Loss: 3916.616455 \tValidation Loss: 1213.421728\n","  Epoch: 250 \tTraining Loss: 3859.561232 \tValidation Loss: 842.918119\n","  Epoch: 300 \tTraining Loss: 3836.063403 \tValidation Loss: 1163.677292\n","Best Validation Loss: 842.918119 in epoch 250\n","Best Train Loss: 3101.826569 in epoch 174\n","  Test Loss: 8.957490\n","\n","  Mean Abs Loss: 5.038095\n","\n","  Test Loss: 8.882779\n","\n","  Mean Abs Loss: 4.933333\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3671.603334 \tValidation Loss: 1110.015297\n","  Epoch: 100 \tTraining Loss: 3656.824957 \tValidation Loss: 1086.250835\n","  Epoch: 150 \tTraining Loss: 3489.811141 \tValidation Loss: 1071.353414\n","  Epoch: 200 \tTraining Loss: 3625.038452 \tValidation Loss: 1092.381952\n","  Epoch: 250 \tTraining Loss: 3407.940795 \tValidation Loss: 1073.240996\n","  Epoch: 300 \tTraining Loss: 3315.581454 \tValidation Loss: 993.367088\n","Best Validation Loss: 607.721282 in epoch 141\n","Best Train Loss: 2581.436849 in epoch 299\n","  Test Loss: 8.699361\n","\n","  Mean Abs Loss: 4.685714\n","\n","  Test Loss: 8.766697\n","\n","  Mean Abs Loss: 4.704762\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4510.118071 \tValidation Loss: 1326.050463\n","  Epoch: 100 \tTraining Loss: 4090.439401 \tValidation Loss: 1187.764640\n","  Epoch: 150 \tTraining Loss: 3930.882409 \tValidation Loss: 1132.015893\n","  Epoch: 200 \tTraining Loss: 3803.078209 \tValidation Loss: 1136.577639\n","  Epoch: 250 \tTraining Loss: 3730.547789 \tValidation Loss: 1137.234754\n","  Epoch: 300 \tTraining Loss: 3744.445711 \tValidation Loss: 1137.523542\n","Best Validation Loss: 544.851424 in epoch 76\n","Best Train Loss: 2930.954101 in epoch 233\n","  Test Loss: 9.610115\n","\n","  Mean Abs Loss: 5.447619\n","\n","  Test Loss: 9.726279\n","\n","  Mean Abs Loss: 5.514286\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3667.709483 \tValidation Loss: 1119.010342\n","  Epoch: 100 \tTraining Loss: 3614.331924 \tValidation Loss: 1109.140245\n","  Epoch: 150 \tTraining Loss: 3438.461098 \tValidation Loss: 1088.780421\n","  Epoch: 200 \tTraining Loss: 3407.926342 \tValidation Loss: 813.390523\n","  Epoch: 250 \tTraining Loss: 3396.693633 \tValidation Loss: 1104.249906\n","  Epoch: 300 \tTraining Loss: 3323.865687 \tValidation Loss: 755.819554\n","Best Validation Loss: 669.042423 in epoch 105\n","Best Train Loss: 2755.259883 in epoch 86\n","  Test Loss: 8.750744\n","\n","  Mean Abs Loss: 4.723810\n","\n","  Test Loss: 8.650605\n","\n","  Mean Abs Loss: 4.733333\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4729.275980 \tValidation Loss: 1372.694527\n","  Epoch: 100 \tTraining Loss: 4186.955705 \tValidation Loss: 1147.295965\n","  Epoch: 150 \tTraining Loss: 3895.821880 \tValidation Loss: 1141.352855\n","  Epoch: 200 \tTraining Loss: 3787.203673 \tValidation Loss: 1100.352858\n","  Epoch: 250 \tTraining Loss: 3797.685112 \tValidation Loss: 951.728288\n","  Epoch: 300 \tTraining Loss: 3856.988771 \tValidation Loss: 1120.281030\n","Best Validation Loss: 455.654310 in epoch 271\n","Best Train Loss: 2944.349562 in epoch 201\n","  Test Loss: 8.660946\n","\n","  Mean Abs Loss: 4.752381\n","\n","  Test Loss: 8.587609\n","\n","  Mean Abs Loss: 4.800000\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3683.841841 \tValidation Loss: 802.675260\n","  Epoch: 100 \tTraining Loss: 3598.528661 \tValidation Loss: 1096.887594\n","  Epoch: 150 \tTraining Loss: 3456.823403 \tValidation Loss: 1097.219791\n","  Epoch: 200 \tTraining Loss: 3429.867725 \tValidation Loss: 1110.890369\n","  Epoch: 250 \tTraining Loss: 3427.926052 \tValidation Loss: 1103.802949\n","  Epoch: 300 \tTraining Loss: 3340.016423 \tValidation Loss: 1102.945034\n","Best Validation Loss: 772.551055 in epoch 266\n","Best Train Loss: 2513.503978 in epoch 264\n","  Test Loss: 8.707401\n","\n","  Mean Abs Loss: 4.657143\n","\n","  Test Loss: 8.764734\n","\n","  Mean Abs Loss: 4.723810\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4486.448850 \tValidation Loss: 1295.379212\n","  Epoch: 100 \tTraining Loss: 4027.365596 \tValidation Loss: 1159.861523\n","  Epoch: 150 \tTraining Loss: 3930.671221 \tValidation Loss: 1142.470807\n","  Epoch: 200 \tTraining Loss: 3790.380969 \tValidation Loss: 1149.071074\n","  Epoch: 250 \tTraining Loss: 3776.979554 \tValidation Loss: 834.915082\n","  Epoch: 300 \tTraining Loss: 3763.088711 \tValidation Loss: 1143.391447\n","Best Validation Loss: 502.771581 in epoch 267\n","Best Train Loss: 2891.886168 in epoch 273\n","  Test Loss: 8.740926\n","\n","  Mean Abs Loss: 4.723810\n","\n","  Test Loss: 8.738094\n","\n","  Mean Abs Loss: 4.790476\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3703.929137 \tValidation Loss: 1098.245589\n","  Epoch: 100 \tTraining Loss: 3619.748907 \tValidation Loss: 1097.222211\n","  Epoch: 150 \tTraining Loss: 3510.856611 \tValidation Loss: 1134.938328\n","  Epoch: 200 \tTraining Loss: 3431.539132 \tValidation Loss: 1119.150699\n","  Epoch: 250 \tTraining Loss: 3314.585072 \tValidation Loss: 1110.285453\n","  Epoch: 300 \tTraining Loss: 3433.360822 \tValidation Loss: 1109.560255\n","Best Validation Loss: 621.296205 in epoch 128\n","Best Train Loss: 2634.225138 in epoch 231\n","  Test Loss: 8.761217\n","\n","  Mean Abs Loss: 4.609524\n","\n","  Test Loss: 8.737491\n","\n","  Mean Abs Loss: 4.657143\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4336.944987 \tValidation Loss: 1257.427099\n","  Epoch: 100 \tTraining Loss: 3976.998535 \tValidation Loss: 1159.971767\n","  Epoch: 150 \tTraining Loss: 3951.202501 \tValidation Loss: 1121.568723\n","  Epoch: 200 \tTraining Loss: 3802.014568 \tValidation Loss: 1130.915681\n","  Epoch: 250 \tTraining Loss: 3826.287141 \tValidation Loss: 1109.614002\n","  Epoch: 300 \tTraining Loss: 3641.627876 \tValidation Loss: 792.925309\n","Best Validation Loss: 643.705531 in epoch 233\n","Best Train Loss: 3492.928970 in epoch 166\n","  Test Loss: 8.650419\n","\n","  Mean Abs Loss: 4.780952\n","\n","  Test Loss: 8.636576\n","\n","  Mean Abs Loss: 4.723810\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3772.778249 \tValidation Loss: 1128.347275\n","  Epoch: 100 \tTraining Loss: 3698.597084 \tValidation Loss: 1126.658399\n","  Epoch: 150 \tTraining Loss: 3603.660215 \tValidation Loss: 1097.213390\n","  Epoch: 200 \tTraining Loss: 3406.017016 \tValidation Loss: 1086.620040\n","  Epoch: 250 \tTraining Loss: 3415.420897 \tValidation Loss: 1091.729681\n","  Epoch: 300 \tTraining Loss: 3342.284079 \tValidation Loss: 762.455397\n","Best Validation Loss: 746.181465 in epoch 289\n","Best Train Loss: 3127.194968 in epoch 268\n","  Test Loss: 8.551170\n","\n","  Mean Abs Loss: 4.590476\n","\n","  Test Loss: 8.626570\n","\n","  Mean Abs Loss: 4.600000\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4680.806767 \tValidation Loss: 1399.577089\n","  Epoch: 100 \tTraining Loss: 4220.471241 \tValidation Loss: 1199.928568\n","  Epoch: 150 \tTraining Loss: 3876.150150 \tValidation Loss: 1038.248023\n","  Epoch: 200 \tTraining Loss: 3875.385996 \tValidation Loss: 1115.358285\n","  Epoch: 250 \tTraining Loss: 3776.567340 \tValidation Loss: 1154.517856\n","  Epoch: 300 \tTraining Loss: 3805.330863 \tValidation Loss: 1143.662985\n","Best Validation Loss: 779.117956 in epoch 166\n","Best Train Loss: 2971.059871 in epoch 262\n","  Test Loss: 8.888554\n","\n","  Mean Abs Loss: 5.152381\n","\n","  Test Loss: 8.706135\n","\n","  Mean Abs Loss: 4.933333\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3972.964853 \tValidation Loss: 1036.215215\n","  Epoch: 100 \tTraining Loss: 3594.031419 \tValidation Loss: 1112.894313\n","  Epoch: 150 \tTraining Loss: 3591.357961 \tValidation Loss: 984.047142\n","  Epoch: 200 \tTraining Loss: 3481.762499 \tValidation Loss: 1119.576698\n","  Epoch: 250 \tTraining Loss: 3369.554097 \tValidation Loss: 1127.335058\n","  Epoch: 300 \tTraining Loss: 3289.084056 \tValidation Loss: 1115.405522\n","Best Validation Loss: 760.613484 in epoch 117\n","Best Train Loss: 2605.755204 in epoch 210\n","  Test Loss: 8.504426\n","\n","  Mean Abs Loss: 4.609524\n","\n","  Test Loss: 8.688523\n","\n","  Mean Abs Loss: 4.685714\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4570.349201 \tValidation Loss: 1454.005478\n","  Epoch: 100 \tTraining Loss: 4211.478785 \tValidation Loss: 750.995360\n","  Epoch: 150 \tTraining Loss: 3891.367736 \tValidation Loss: 1159.695693\n","  Epoch: 200 \tTraining Loss: 3917.868486 \tValidation Loss: 1138.081569\n","  Epoch: 250 \tTraining Loss: 3799.271411 \tValidation Loss: 1123.326244\n","  Epoch: 300 \tTraining Loss: 3768.106838 \tValidation Loss: 1125.965880\n","Best Validation Loss: 750.995360 in epoch 100\n","Best Train Loss: 3557.484097 in epoch 274\n","  Test Loss: 9.192871\n","\n","  Mean Abs Loss: 5.238095\n","\n","  Test Loss: 10.081766\n","\n","  Mean Abs Loss: 5.838095\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3683.347168 \tValidation Loss: 1142.780781\n","  Epoch: 100 \tTraining Loss: 3570.045234 \tValidation Loss: 1124.609510\n","  Epoch: 150 \tTraining Loss: 3513.238852 \tValidation Loss: 1121.599621\n","  Epoch: 200 \tTraining Loss: 3463.963168 \tValidation Loss: 1103.460990\n","  Epoch: 250 \tTraining Loss: 2520.021328 \tValidation Loss: 1090.795569\n","  Epoch: 300 \tTraining Loss: 3385.071169 \tValidation Loss: 1100.275383\n","Best Validation Loss: 488.924954 in epoch 142\n","Best Train Loss: 2520.021328 in epoch 250\n","  Test Loss: 8.800906\n","\n","  Mean Abs Loss: 4.733333\n","\n","  Test Loss: 8.848259\n","\n","  Mean Abs Loss: 4.828571\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4852.555554 \tValidation Loss: 1409.068810\n","  Epoch: 100 \tTraining Loss: 4329.921849 \tValidation Loss: 1324.272516\n","  Epoch: 150 \tTraining Loss: 4037.803820 \tValidation Loss: 786.768843\n","  Epoch: 200 \tTraining Loss: 3951.787160 \tValidation Loss: 1204.129790\n","  Epoch: 250 \tTraining Loss: 3828.835571 \tValidation Loss: 1186.751843\n","  Epoch: 300 \tTraining Loss: 3797.617682 \tValidation Loss: 1000.273107\n","Best Validation Loss: 786.768843 in epoch 150\n","Best Train Loss: 3463.454055 in epoch 197\n","  Test Loss: 9.498644\n","\n","  Mean Abs Loss: 5.457143\n","\n","  Test Loss: 10.063627\n","\n","  Mean Abs Loss: 6.047619\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3731.862627 \tValidation Loss: 1142.418056\n","  Epoch: 100 \tTraining Loss: 3646.928607 \tValidation Loss: 1009.172572\n","  Epoch: 150 \tTraining Loss: 3463.548364 \tValidation Loss: 1114.366489\n","  Epoch: 200 \tTraining Loss: 3441.977096 \tValidation Loss: 1120.426941\n","  Epoch: 250 \tTraining Loss: 3334.512934 \tValidation Loss: 1122.122358\n","  Epoch: 300 \tTraining Loss: 3264.379890 \tValidation Loss: 1136.431219\n","Best Validation Loss: 675.323563 in epoch 236\n","Best Train Loss: 2736.773710 in epoch 137\n","  Test Loss: 8.585941\n","\n","  Mean Abs Loss: 4.647619\n","\n","  Test Loss: 8.785210\n","\n","  Mean Abs Loss: 4.828571\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4381.614665 \tValidation Loss: 1280.671170\n","  Epoch: 100 \tTraining Loss: 4151.441685 \tValidation Loss: 817.234263\n","  Epoch: 150 \tTraining Loss: 3844.335959 \tValidation Loss: 1135.054921\n","  Epoch: 200 \tTraining Loss: 3835.291102 \tValidation Loss: 1130.822340\n","  Epoch: 250 \tTraining Loss: 3767.013177 \tValidation Loss: 1135.602939\n","  Epoch: 300 \tTraining Loss: 3639.374113 \tValidation Loss: 1134.325090\n","Best Validation Loss: 684.941689 in epoch 220\n","Best Train Loss: 2885.874865 in epoch 203\n","  Test Loss: 8.775519\n","\n","  Mean Abs Loss: 4.761905\n","\n","  Test Loss: 8.799169\n","\n","  Mean Abs Loss: 4.771429\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3682.404664 \tValidation Loss: 1098.198904\n","  Epoch: 100 \tTraining Loss: 3590.482334 \tValidation Loss: 1121.140562\n","  Epoch: 150 \tTraining Loss: 3585.665856 \tValidation Loss: 1107.103828\n","  Epoch: 200 \tTraining Loss: 3538.510714 \tValidation Loss: 1112.431061\n","  Epoch: 250 \tTraining Loss: 3454.275842 \tValidation Loss: 1063.665545\n","  Epoch: 300 \tTraining Loss: 3350.780880 \tValidation Loss: 1084.291763\n","Best Validation Loss: 636.562189 in epoch 283\n","Best Train Loss: 2674.058895 in epoch 196\n","  Test Loss: 8.739651\n","\n","  Mean Abs Loss: 4.561905\n","\n"," Test loss decreased (4.561905 --> 4.561905).  Saving model ...\n","  Test Loss: 8.638139\n","\n","  Mean Abs Loss: 4.714286\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4634.410221 \tValidation Loss: 1235.235420\n","  Epoch: 100 \tTraining Loss: 4102.252760 \tValidation Loss: 1213.172995\n","  Epoch: 150 \tTraining Loss: 3965.451191 \tValidation Loss: 1138.956114\n","  Epoch: 200 \tTraining Loss: 3818.339748 \tValidation Loss: 1131.187657\n","  Epoch: 250 \tTraining Loss: 3739.538773 \tValidation Loss: 1135.819311\n","  Epoch: 300 \tTraining Loss: 3801.314622 \tValidation Loss: 1114.197844\n","Best Validation Loss: 653.545680 in epoch 129\n","Best Train Loss: 2972.740028 in epoch 225\n","  Test Loss: 9.606609\n","\n","  Mean Abs Loss: 5.400000\n","\n","  Test Loss: 9.604921\n","\n","  Mean Abs Loss: 5.380952\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3685.340597 \tValidation Loss: 1111.462627\n","  Epoch: 100 \tTraining Loss: 3643.034827 \tValidation Loss: 1108.500767\n","  Epoch: 150 \tTraining Loss: 3591.519885 \tValidation Loss: 1078.637834\n","  Epoch: 200 \tTraining Loss: 3617.223134 \tValidation Loss: 914.110696\n","  Epoch: 250 \tTraining Loss: 3552.399088 \tValidation Loss: 1088.085459\n","  Epoch: 300 \tTraining Loss: 3367.118431 \tValidation Loss: 805.629248\n","Best Validation Loss: 759.584087 in epoch 163\n","Best Train Loss: 2653.848106 in epoch 285\n","  Test Loss: 8.728685\n","\n","  Mean Abs Loss: 4.704762\n","\n","  Test Loss: 8.652507\n","\n","  Mean Abs Loss: 4.552381\n","\n"," Test loss decreased (4.561905 --> 4.552381).  Saving model ...\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4410.700859 \tValidation Loss: 1304.732071\n","  Epoch: 100 \tTraining Loss: 4166.155220 \tValidation Loss: 1196.164500\n","  Epoch: 150 \tTraining Loss: 3873.491545 \tValidation Loss: 1157.013812\n","  Epoch: 200 \tTraining Loss: 3935.928828 \tValidation Loss: 1160.682117\n","  Epoch: 250 \tTraining Loss: 3874.088730 \tValidation Loss: 1160.416229\n","  Epoch: 300 \tTraining Loss: 3723.426469 \tValidation Loss: 1160.938586\n","Best Validation Loss: 691.538993 in epoch 271\n","Best Train Loss: 2989.930326 in epoch 259\n","  Test Loss: 8.775344\n","\n","  Mean Abs Loss: 4.800000\n","\n","  Test Loss: 8.759671\n","\n","  Mean Abs Loss: 4.809524\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3670.763913 \tValidation Loss: 1208.589147\n","  Epoch: 100 \tTraining Loss: 3605.719624 \tValidation Loss: 1128.395731\n","  Epoch: 150 \tTraining Loss: 3568.507558 \tValidation Loss: 1113.890917\n","  Epoch: 200 \tTraining Loss: 3363.075924 \tValidation Loss: 1105.023876\n","  Epoch: 250 \tTraining Loss: 3424.521901 \tValidation Loss: 1009.043810\n","  Epoch: 300 \tTraining Loss: 3311.536534 \tValidation Loss: 1088.013516\n","Best Validation Loss: 669.636291 in epoch 84\n","Best Train Loss: 2831.841293 in epoch 75\n","  Test Loss: 8.867550\n","\n","  Mean Abs Loss: 4.733333\n","\n","  Test Loss: 8.784423\n","\n","  Mean Abs Loss: 4.695238\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4252.795032 \tValidation Loss: 1368.156193\n","  Epoch: 100 \tTraining Loss: 4077.625110 \tValidation Loss: 1045.275561\n","  Epoch: 150 \tTraining Loss: 4011.931586 \tValidation Loss: 1129.727823\n","  Epoch: 200 \tTraining Loss: 3873.790385 \tValidation Loss: 1134.163807\n","  Epoch: 250 \tTraining Loss: 3726.079260 \tValidation Loss: 1102.087935\n","  Epoch: 300 \tTraining Loss: 3824.990941 \tValidation Loss: 1123.329393\n","Best Validation Loss: 636.517870 in epoch 125\n","Best Train Loss: 2910.377992 in epoch 232\n","  Test Loss: 8.575141\n","\n","  Mean Abs Loss: 4.780952\n","\n","  Test Loss: 8.725544\n","\n","  Mean Abs Loss: 4.685714\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3662.769768 \tValidation Loss: 1109.068871\n","  Epoch: 100 \tTraining Loss: 3606.026643 \tValidation Loss: 1080.118155\n","  Epoch: 150 \tTraining Loss: 3603.127864 \tValidation Loss: 1127.190007\n","  Epoch: 200 \tTraining Loss: 3457.819600 \tValidation Loss: 990.825753\n","  Epoch: 250 \tTraining Loss: 3333.465729 \tValidation Loss: 1117.643616\n","  Epoch: 300 \tTraining Loss: 2529.316309 \tValidation Loss: 1089.389851\n","Best Validation Loss: 719.983691 in epoch 52\n","Best Train Loss: 2529.316309 in epoch 300\n","  Test Loss: 8.833468\n","\n","  Mean Abs Loss: 4.723810\n","\n","  Test Loss: 8.963714\n","\n","  Mean Abs Loss: 5.104762\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4779.779480 \tValidation Loss: 1347.978863\n","  Epoch: 100 \tTraining Loss: 4320.238766 \tValidation Loss: 1289.508288\n","  Epoch: 150 \tTraining Loss: 4210.253091 \tValidation Loss: 1242.407631\n","  Epoch: 200 \tTraining Loss: 4017.647957 \tValidation Loss: 1228.800277\n","  Epoch: 250 \tTraining Loss: 3775.645649 \tValidation Loss: 1202.497411\n","  Epoch: 300 \tTraining Loss: 3740.197796 \tValidation Loss: 1167.543591\n","Best Validation Loss: 715.518201 in epoch 193\n","Best Train Loss: 2940.338810 in epoch 184\n","  Test Loss: 9.384381\n","\n","  Mean Abs Loss: 5.380952\n","\n","  Test Loss: 9.470199\n","\n","  Mean Abs Loss: 5.419048\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3831.439947 \tValidation Loss: 1124.249166\n","  Epoch: 100 \tTraining Loss: 3613.995993 \tValidation Loss: 1107.902315\n","  Epoch: 150 \tTraining Loss: 3546.520649 \tValidation Loss: 1113.188798\n","  Epoch: 200 \tTraining Loss: 3369.109176 \tValidation Loss: 1080.276688\n","  Epoch: 250 \tTraining Loss: 3353.522808 \tValidation Loss: 1067.727931\n","  Epoch: 300 \tTraining Loss: 3379.776437 \tValidation Loss: 942.248663\n","Best Validation Loss: 620.672648 in epoch 119\n","Best Train Loss: 2671.489832 in epoch 218\n","  Test Loss: 8.535719\n","\n","  Mean Abs Loss: 4.676190\n","\n","  Test Loss: 8.670869\n","\n","  Mean Abs Loss: 4.609524\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4552.002569 \tValidation Loss: 1396.137291\n","  Epoch: 100 \tTraining Loss: 4063.942878 \tValidation Loss: 1204.641975\n","  Epoch: 150 \tTraining Loss: 3844.205662 \tValidation Loss: 1167.114793\n","  Epoch: 200 \tTraining Loss: 3859.916622 \tValidation Loss: 1045.420871\n","  Epoch: 250 \tTraining Loss: 3805.227773 \tValidation Loss: 813.504715\n","  Epoch: 300 \tTraining Loss: 3639.727346 \tValidation Loss: 1145.770481\n","Best Validation Loss: 813.504715 in epoch 250\n","Best Train Loss: 3145.184589 in epoch 116\n","  Test Loss: 9.695826\n","\n","  Mean Abs Loss: 5.438095\n","\n","  Test Loss: 8.872910\n","\n","  Mean Abs Loss: 4.809524\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3780.593570 \tValidation Loss: 1152.579630\n","  Epoch: 100 \tTraining Loss: 3660.379670 \tValidation Loss: 1089.213180\n","  Epoch: 150 \tTraining Loss: 3538.260254 \tValidation Loss: 1072.293608\n","  Epoch: 200 \tTraining Loss: 3385.047098 \tValidation Loss: 1090.874006\n","  Epoch: 250 \tTraining Loss: 3361.855426 \tValidation Loss: 1090.051219\n","  Epoch: 300 \tTraining Loss: 3255.417778 \tValidation Loss: 1079.565807\n","Best Validation Loss: 678.806070 in epoch 30\n","Best Train Loss: 2432.621311 in epoch 289\n","  Test Loss: 8.426962\n","\n","  Mean Abs Loss: 4.571429\n","\n","  Test Loss: 8.804813\n","\n","  Mean Abs Loss: 4.914286\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4332.678845 \tValidation Loss: 1333.885813\n","  Epoch: 100 \tTraining Loss: 4075.509931 \tValidation Loss: 1030.649315\n","  Epoch: 150 \tTraining Loss: 3890.666350 \tValidation Loss: 1165.772784\n","  Epoch: 200 \tTraining Loss: 3897.294718 \tValidation Loss: 966.245438\n","  Epoch: 250 \tTraining Loss: 3814.377406 \tValidation Loss: 1129.541853\n","  Epoch: 300 \tTraining Loss: 3772.354760 \tValidation Loss: 1017.700738\n","Best Validation Loss: 657.901685 in epoch 274\n","Best Train Loss: 2909.634404 in epoch 279\n","  Test Loss: 8.987655\n","\n","  Mean Abs Loss: 5.057143\n","\n","  Test Loss: 8.856178\n","\n","  Mean Abs Loss: 4.971429\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3728.492542 \tValidation Loss: 1136.780774\n","  Epoch: 100 \tTraining Loss: 3594.421669 \tValidation Loss: 1091.323992\n","  Epoch: 150 \tTraining Loss: 3615.104849 \tValidation Loss: 1099.726513\n","  Epoch: 200 \tTraining Loss: 3418.815571 \tValidation Loss: 1076.688261\n","  Epoch: 250 \tTraining Loss: 3412.137298 \tValidation Loss: 1107.213063\n","  Epoch: 300 \tTraining Loss: 3224.970179 \tValidation Loss: 1105.753192\n","Best Validation Loss: 689.204527 in epoch 196\n","Best Train Loss: 2594.251730 in epoch 203\n","  Test Loss: 8.765441\n","\n","  Mean Abs Loss: 4.676190\n","\n","  Test Loss: 8.706263\n","\n","  Mean Abs Loss: 4.695238\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4439.644499 \tValidation Loss: 1330.299031\n","  Epoch: 100 \tTraining Loss: 3951.248825 \tValidation Loss: 1148.975735\n","  Epoch: 150 \tTraining Loss: 4012.480302 \tValidation Loss: 1147.114485\n","  Epoch: 200 \tTraining Loss: 3767.621285 \tValidation Loss: 1157.244296\n","  Epoch: 250 \tTraining Loss: 3904.271371 \tValidation Loss: 1136.215090\n","  Epoch: 300 \tTraining Loss: 3767.847669 \tValidation Loss: 1129.813165\n","Best Validation Loss: 799.013486 in epoch 284\n","Best Train Loss: 3131.157579 in epoch 129\n","  Test Loss: 8.919032\n","\n","  Mean Abs Loss: 4.742857\n","\n","  Test Loss: 8.780454\n","\n","  Mean Abs Loss: 4.733333\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3745.240448 \tValidation Loss: 1124.772108\n","  Epoch: 100 \tTraining Loss: 3665.967136 \tValidation Loss: 1114.400307\n","  Epoch: 150 \tTraining Loss: 3483.837332 \tValidation Loss: 1091.502066\n","  Epoch: 200 \tTraining Loss: 3475.722740 \tValidation Loss: 1102.851703\n","  Epoch: 250 \tTraining Loss: 3408.764230 \tValidation Loss: 1078.988390\n","  Epoch: 300 \tTraining Loss: 3329.250395 \tValidation Loss: 1085.281981\n","Best Validation Loss: 626.820708 in epoch 67\n","Best Train Loss: 3040.123153 in epoch 234\n","  Test Loss: 8.615700\n","\n","  Mean Abs Loss: 4.638095\n","\n","  Test Loss: 8.656701\n","\n","  Mean Abs Loss: 4.695238\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4434.512675 \tValidation Loss: 1264.513979\n","  Epoch: 100 \tTraining Loss: 4187.501816 \tValidation Loss: 1149.089276\n","  Epoch: 150 \tTraining Loss: 4056.257321 \tValidation Loss: 1134.626854\n","  Epoch: 200 \tTraining Loss: 3840.067203 \tValidation Loss: 1123.761766\n","  Epoch: 250 \tTraining Loss: 3819.897794 \tValidation Loss: 1132.780659\n","  Epoch: 300 \tTraining Loss: 3750.387659 \tValidation Loss: 817.430629\n","Best Validation Loss: 484.771218 in epoch 202\n","Best Train Loss: 2952.628772 in epoch 246\n","  Test Loss: 8.563610\n","\n","  Mean Abs Loss: 4.695238\n","\n","  Test Loss: 8.615204\n","\n","  Mean Abs Loss: 4.695238\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3822.743877 \tValidation Loss: 1151.395478\n","  Epoch: 100 \tTraining Loss: 3689.318990 \tValidation Loss: 1112.810190\n","  Epoch: 150 \tTraining Loss: 3523.400982 \tValidation Loss: 1091.237982\n","  Epoch: 200 \tTraining Loss: 3370.939657 \tValidation Loss: 1103.663849\n","  Epoch: 250 \tTraining Loss: 3289.701082 \tValidation Loss: 1058.381389\n","  Epoch: 300 \tTraining Loss: 3293.157588 \tValidation Loss: 1086.046576\n","Best Validation Loss: 645.741603 in epoch 122\n","Best Train Loss: 3056.811737 in epoch 277\n","  Test Loss: 8.473950\n","\n","  Mean Abs Loss: 4.580952\n","\n","  Test Loss: 8.504718\n","\n","  Mean Abs Loss: 4.619048\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4381.635770 \tValidation Loss: 1425.986541\n","  Epoch: 100 \tTraining Loss: 4088.669748 \tValidation Loss: 1154.810862\n","  Epoch: 150 \tTraining Loss: 3951.331146 \tValidation Loss: 1183.299301\n","  Epoch: 200 \tTraining Loss: 3899.241678 \tValidation Loss: 1144.995293\n","  Epoch: 250 \tTraining Loss: 3711.108252 \tValidation Loss: 1139.034683\n","  Epoch: 300 \tTraining Loss: 3688.731891 \tValidation Loss: 1146.083196\n","Best Validation Loss: 594.303913 in epoch 110\n","Best Train Loss: 2994.701757 in epoch 246\n","  Test Loss: 9.187832\n","\n","  Mean Abs Loss: 5.209524\n","\n","  Test Loss: 9.833091\n","\n","  Mean Abs Loss: 5.752381\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3772.369807 \tValidation Loss: 1180.949287\n","  Epoch: 100 \tTraining Loss: 3678.584811 \tValidation Loss: 1115.300755\n","  Epoch: 150 \tTraining Loss: 3604.079306 \tValidation Loss: 992.173060\n","  Epoch: 200 \tTraining Loss: 3346.241939 \tValidation Loss: 1119.569352\n","  Epoch: 250 \tTraining Loss: 3433.282683 \tValidation Loss: 1118.811456\n","  Epoch: 300 \tTraining Loss: 3385.161673 \tValidation Loss: 1135.001429\n","Best Validation Loss: 661.894266 in epoch 6\n","Best Train Loss: 2584.175327 in epoch 242\n","  Test Loss: 8.817252\n","\n","  Mean Abs Loss: 4.695238\n","\n","  Test Loss: 10.197688\n","\n","  Mean Abs Loss: 5.980952\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4418.566255 \tValidation Loss: 1063.459029\n","  Epoch: 100 \tTraining Loss: 4104.570982 \tValidation Loss: 1195.931357\n","  Epoch: 150 \tTraining Loss: 3981.079103 \tValidation Loss: 1151.308660\n","  Epoch: 200 \tTraining Loss: 3768.962938 \tValidation Loss: 1162.152533\n","  Epoch: 250 \tTraining Loss: 3711.402836 \tValidation Loss: 1151.508224\n","  Epoch: 300 \tTraining Loss: 3706.184292 \tValidation Loss: 1138.520918\n","Best Validation Loss: 703.527611 in epoch 239\n","Best Train Loss: 3098.355357 in epoch 161\n","  Test Loss: 9.214251\n","\n","  Mean Abs Loss: 5.180952\n","\n","  Test Loss: 8.820465\n","\n","  Mean Abs Loss: 4.800000\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3808.226464 \tValidation Loss: 1106.928441\n","  Epoch: 100 \tTraining Loss: 3571.455042 \tValidation Loss: 1093.566550\n","  Epoch: 150 \tTraining Loss: 3586.634824 \tValidation Loss: 1113.481948\n","  Epoch: 200 \tTraining Loss: 3487.778446 \tValidation Loss: 1120.334917\n","  Epoch: 250 \tTraining Loss: 3402.572161 \tValidation Loss: 1009.165684\n","  Epoch: 300 \tTraining Loss: 3343.953783 \tValidation Loss: 1102.219675\n","Best Validation Loss: 474.153363 in epoch 292\n","Best Train Loss: 2935.568054 in epoch 18\n","  Test Loss: 9.603415\n","\n","  Mean Abs Loss: 5.457143\n","\n","  Test Loss: 8.856108\n","\n","  Mean Abs Loss: 4.723810\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 3500.937376 \tValidation Loss: 1383.272040\n","  Epoch: 100 \tTraining Loss: 4007.750103 \tValidation Loss: 1226.278622\n","  Epoch: 150 \tTraining Loss: 3805.366717 \tValidation Loss: 1152.494480\n","  Epoch: 200 \tTraining Loss: 3976.531255 \tValidation Loss: 1128.677425\n","  Epoch: 250 \tTraining Loss: 3737.700166 \tValidation Loss: 801.045663\n","  Epoch: 300 \tTraining Loss: 3663.872330 \tValidation Loss: 1138.777291\n","Best Validation Loss: 801.045663 in epoch 250\n","Best Train Loss: 3500.937376 in epoch 50\n","  Test Loss: 10.459638\n","\n","  Mean Abs Loss: 6.247619\n","\n","  Test Loss: 9.218865\n","\n","  Mean Abs Loss: 5.190476\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3774.319407 \tValidation Loss: 1116.962580\n","  Epoch: 100 \tTraining Loss: 3545.039132 \tValidation Loss: 1090.965352\n","  Epoch: 150 \tTraining Loss: 3470.056642 \tValidation Loss: 1085.756949\n","  Epoch: 200 \tTraining Loss: 2673.875782 \tValidation Loss: 1003.776594\n","  Epoch: 250 \tTraining Loss: 3335.023657 \tValidation Loss: 1118.638904\n","  Epoch: 300 \tTraining Loss: 3324.162537 \tValidation Loss: 1138.164997\n","Best Validation Loss: 627.942687 in epoch 109\n","Best Train Loss: 2595.172546 in epoch 296\n","  Test Loss: 9.386424\n","\n","  Mean Abs Loss: 5.238095\n","\n","  Test Loss: 8.745749\n","\n","  Mean Abs Loss: 4.676190\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4451.495459 \tValidation Loss: 1327.908386\n","  Epoch: 100 \tTraining Loss: 4345.732629 \tValidation Loss: 1287.952748\n","  Epoch: 150 \tTraining Loss: 4114.771459 \tValidation Loss: 1248.168290\n","  Epoch: 200 \tTraining Loss: 3892.730544 \tValidation Loss: 1196.124176\n","  Epoch: 250 \tTraining Loss: 3988.310363 \tValidation Loss: 1201.464296\n","  Epoch: 300 \tTraining Loss: 3884.998442 \tValidation Loss: 1188.427546\n","Best Validation Loss: 795.052688 in epoch 280\n","Best Train Loss: 3026.759802 in epoch 230\n","  Test Loss: 9.107014\n","\n","  Mean Abs Loss: 5.085714\n","\n","  Test Loss: 8.860689\n","\n","  Mean Abs Loss: 4.895238\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3783.889822 \tValidation Loss: 1185.777708\n","  Epoch: 100 \tTraining Loss: 3666.599839 \tValidation Loss: 1103.002974\n","  Epoch: 150 \tTraining Loss: 3640.140990 \tValidation Loss: 790.212209\n","  Epoch: 200 \tTraining Loss: 3333.751853 \tValidation Loss: 988.561559\n","  Epoch: 250 \tTraining Loss: 3447.109650 \tValidation Loss: 1076.645790\n","  Epoch: 300 \tTraining Loss: 3378.108306 \tValidation Loss: 1081.751116\n","Best Validation Loss: 735.553485 in epoch 135\n","Best Train Loss: 2466.896205 in epoch 239\n","  Test Loss: 8.641956\n","\n","  Mean Abs Loss: 4.695238\n","\n","  Test Loss: 8.517512\n","\n","  Mean Abs Loss: 4.771429\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4466.802554 \tValidation Loss: 1331.028946\n","  Epoch: 100 \tTraining Loss: 4123.634760 \tValidation Loss: 1194.040450\n","  Epoch: 150 \tTraining Loss: 3874.426487 \tValidation Loss: 1168.104919\n","  Epoch: 200 \tTraining Loss: 3855.470429 \tValidation Loss: 1114.749196\n","  Epoch: 250 \tTraining Loss: 3777.312959 \tValidation Loss: 1116.018050\n","  Epoch: 300 \tTraining Loss: 3757.503327 \tValidation Loss: 1134.805546\n","Best Validation Loss: 581.826316 in epoch 63\n","Best Train Loss: 3034.640431 in epoch 169\n","  Test Loss: 8.837446\n","\n","  Mean Abs Loss: 4.638095\n","\n","  Test Loss: 9.793986\n","\n","  Mean Abs Loss: 5.580952\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3594.862147 \tValidation Loss: 1136.529790\n","  Epoch: 100 \tTraining Loss: 3547.540645 \tValidation Loss: 941.479813\n","  Epoch: 150 \tTraining Loss: 3515.616345 \tValidation Loss: 1078.247784\n","  Epoch: 200 \tTraining Loss: 3433.283004 \tValidation Loss: 1081.845123\n","  Epoch: 250 \tTraining Loss: 3432.008443 \tValidation Loss: 1095.916035\n","  Epoch: 300 \tTraining Loss: 3354.403862 \tValidation Loss: 944.313773\n","Best Validation Loss: 665.218343 in epoch 294\n","Best Train Loss: 2535.162313 in epoch 222\n","  Test Loss: 8.746362\n","\n","  Mean Abs Loss: 4.628571\n","\n","  Test Loss: 8.737724\n","\n","  Mean Abs Loss: 4.742857\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 5\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4463.611875 \tValidation Loss: 907.702549\n","  Epoch: 100 \tTraining Loss: 4036.370455 \tValidation Loss: 1121.737810\n","  Epoch: 150 \tTraining Loss: 3789.438520 \tValidation Loss: 1130.114675\n","  Epoch: 200 \tTraining Loss: 3760.508612 \tValidation Loss: 1014.381911\n","  Epoch: 250 \tTraining Loss: 3828.228129 \tValidation Loss: 1117.286707\n","  Epoch: 300 \tTraining Loss: 3729.610326 \tValidation Loss: 1097.191391\n","Best Validation Loss: 677.523479 in epoch 179\n","Best Train Loss: 2857.581692 in epoch 244\n","  Test Loss: 8.603456\n","\n","  Mean Abs Loss: 4.714286\n","\n","  Test Loss: 8.558550\n","\n","  Mean Abs Loss: 4.752381\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3716.944951 \tValidation Loss: 1149.835962\n","  Epoch: 100 \tTraining Loss: 3639.317523 \tValidation Loss: 944.989612\n","  Epoch: 150 \tTraining Loss: 3396.310841 \tValidation Loss: 1094.719827\n","  Epoch: 200 \tTraining Loss: 3500.814914 \tValidation Loss: 1100.866388\n","  Epoch: 250 \tTraining Loss: 3304.690993 \tValidation Loss: 1105.596546\n","  Epoch: 300 \tTraining Loss: 3269.379343 \tValidation Loss: 749.050574\n","Best Validation Loss: 729.627607 in epoch 259\n","Best Train Loss: 2448.084345 in epoch 287\n","  Test Loss: 8.901407\n","\n","  Mean Abs Loss: 4.990476\n","\n","  Test Loss: 8.700785\n","\n","  Mean Abs Loss: 4.647619\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4597.498998 \tValidation Loss: 1375.739489\n","  Epoch: 100 \tTraining Loss: 4046.330940 \tValidation Loss: 1217.799461\n","  Epoch: 150 \tTraining Loss: 3924.893181 \tValidation Loss: 1149.838686\n","  Epoch: 200 \tTraining Loss: 3832.740881 \tValidation Loss: 1156.908992\n","  Epoch: 250 \tTraining Loss: 3571.313049 \tValidation Loss: 1165.048143\n","  Epoch: 300 \tTraining Loss: 3660.008934 \tValidation Loss: 1156.297389\n","Best Validation Loss: 772.783822 in epoch 95\n","Best Train Loss: 2884.320279 in epoch 238\n","  Test Loss: 8.969662\n","\n","  Mean Abs Loss: 5.047619\n","\n","  Test Loss: 9.878919\n","\n","  Mean Abs Loss: 5.609524\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3806.454109 \tValidation Loss: 1142.766288\n","  Epoch: 100 \tTraining Loss: 3741.796801 \tValidation Loss: 1116.959625\n","  Epoch: 150 \tTraining Loss: 3545.028401 \tValidation Loss: 1107.255098\n","  Epoch: 200 \tTraining Loss: 3475.050015 \tValidation Loss: 1105.342689\n","  Epoch: 250 \tTraining Loss: 3403.790122 \tValidation Loss: 1115.213865\n","  Epoch: 300 \tTraining Loss: 3319.307598 \tValidation Loss: 1133.039571\n","Best Validation Loss: 633.513350 in epoch 65\n","Best Train Loss: 2535.740549 in epoch 227\n","  Test Loss: 8.647970\n","\n","  Mean Abs Loss: 4.647619\n","\n","  Test Loss: 8.867577\n","\n","  Mean Abs Loss: 4.828571\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4661.767381 \tValidation Loss: 1506.223007\n","  Epoch: 100 \tTraining Loss: 4180.294621 \tValidation Loss: 1289.610202\n","  Epoch: 150 \tTraining Loss: 3973.798449 \tValidation Loss: 1267.464611\n","  Epoch: 200 \tTraining Loss: 3980.107924 \tValidation Loss: 1226.670010\n","  Epoch: 250 \tTraining Loss: 3917.099233 \tValidation Loss: 1163.542415\n","  Epoch: 300 \tTraining Loss: 3647.127282 \tValidation Loss: 1149.118384\n","Best Validation Loss: 718.712623 in epoch 196\n","Best Train Loss: 3030.361539 in epoch 238\n","  Test Loss: 9.192241\n","\n","  Mean Abs Loss: 5.295238\n","\n","  Test Loss: 9.213695\n","\n","  Mean Abs Loss: 5.380952\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3813.683662 \tValidation Loss: 1147.990208\n","  Epoch: 100 \tTraining Loss: 3862.142685 \tValidation Loss: 1132.130430\n","  Epoch: 150 \tTraining Loss: 3475.430455 \tValidation Loss: 1099.711951\n","  Epoch: 200 \tTraining Loss: 3415.714347 \tValidation Loss: 1131.120799\n","  Epoch: 250 \tTraining Loss: 3518.135725 \tValidation Loss: 1125.223701\n","  Epoch: 300 \tTraining Loss: 3346.730785 \tValidation Loss: 1124.823812\n","Best Validation Loss: 479.487117 in epoch 255\n","Best Train Loss: 2554.979146 in epoch 252\n","  Test Loss: 8.794881\n","\n","  Mean Abs Loss: 4.790476\n","\n","  Test Loss: 8.735575\n","\n","  Mean Abs Loss: 4.628571\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4402.818394 \tValidation Loss: 1340.986145\n","  Epoch: 100 \tTraining Loss: 3950.562017 \tValidation Loss: 1173.727680\n","  Epoch: 150 \tTraining Loss: 3931.567532 \tValidation Loss: 1130.468532\n","  Epoch: 200 \tTraining Loss: 3817.066359 \tValidation Loss: 1137.518684\n","  Epoch: 250 \tTraining Loss: 3812.406150 \tValidation Loss: 1149.721769\n","  Epoch: 300 \tTraining Loss: 2937.720628 \tValidation Loss: 1141.684090\n","Best Validation Loss: 725.133446 in epoch 66\n","Best Train Loss: 2797.364375 in epoch 158\n","  Test Loss: 8.708248\n","\n","  Mean Abs Loss: 4.790476\n","\n","  Test Loss: 9.427705\n","\n","  Mean Abs Loss: 5.171429\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3745.800391 \tValidation Loss: 1160.835835\n","  Epoch: 100 \tTraining Loss: 3614.590992 \tValidation Loss: 1019.723303\n","  Epoch: 150 \tTraining Loss: 3455.891548 \tValidation Loss: 1097.597221\n","  Epoch: 200 \tTraining Loss: 3418.854502 \tValidation Loss: 1079.348866\n","  Epoch: 250 \tTraining Loss: 3501.107896 \tValidation Loss: 1086.726754\n","  Epoch: 300 \tTraining Loss: 3293.024188 \tValidation Loss: 1086.472489\n","Best Validation Loss: 473.221978 in epoch 183\n","Best Train Loss: 2585.313552 in epoch 178\n","  Test Loss: 8.939817\n","\n","  Mean Abs Loss: 4.904762\n","\n","  Test Loss: 8.968791\n","\n","  Mean Abs Loss: 4.923810\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4776.225882 \tValidation Loss: 1500.936521\n","  Epoch: 100 \tTraining Loss: 3870.094406 \tValidation Loss: 1222.177457\n","  Epoch: 150 \tTraining Loss: 4003.699149 \tValidation Loss: 1176.056572\n","  Epoch: 200 \tTraining Loss: 3844.462010 \tValidation Loss: 1159.959530\n","  Epoch: 250 \tTraining Loss: 3827.214806 \tValidation Loss: 1120.861290\n","  Epoch: 300 \tTraining Loss: 3753.409141 \tValidation Loss: 1110.310586\n","Best Validation Loss: 787.867872 in epoch 235\n","Best Train Loss: 2903.641892 in epoch 282\n","  Test Loss: 8.864839\n","\n","  Mean Abs Loss: 4.952381\n","\n","  Test Loss: 8.827378\n","\n","  Mean Abs Loss: 4.904762\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3770.760915 \tValidation Loss: 1018.037118\n","  Epoch: 100 \tTraining Loss: 3605.659246 \tValidation Loss: 992.282538\n","  Epoch: 150 \tTraining Loss: 3519.443062 \tValidation Loss: 1120.624100\n","  Epoch: 200 \tTraining Loss: 3426.361854 \tValidation Loss: 786.081452\n","  Epoch: 250 \tTraining Loss: 3393.973549 \tValidation Loss: 1087.820472\n","  Epoch: 300 \tTraining Loss: 3160.030179 \tValidation Loss: 1107.821019\n","Best Validation Loss: 740.686367 in epoch 261\n","Best Train Loss: 2578.254108 in epoch 299\n","  Test Loss: 8.258550\n","\n","  Mean Abs Loss: 4.361905\n","\n"," Test loss decreased (4.552381 --> 4.361905).  Saving model ...\n","  Test Loss: 8.360688\n","\n","  Mean Abs Loss: 4.390476\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4489.711527 \tValidation Loss: 1297.043563\n","  Epoch: 100 \tTraining Loss: 3971.990824 \tValidation Loss: 1183.974887\n","  Epoch: 150 \tTraining Loss: 3864.756196 \tValidation Loss: 1175.106036\n","  Epoch: 200 \tTraining Loss: 3100.875739 \tValidation Loss: 826.434547\n","  Epoch: 250 \tTraining Loss: 3798.223629 \tValidation Loss: 1157.048574\n","  Epoch: 300 \tTraining Loss: 3651.034275 \tValidation Loss: 1143.848441\n","Best Validation Loss: 684.817771 in epoch 270\n","Best Train Loss: 2910.564720 in epoch 216\n","  Test Loss: 8.905340\n","\n","  Mean Abs Loss: 5.047619\n","\n","  Test Loss: 8.902389\n","\n","  Mean Abs Loss: 5.085714\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3707.874552 \tValidation Loss: 971.242372\n","  Epoch: 100 \tTraining Loss: 3620.387968 \tValidation Loss: 959.346407\n","  Epoch: 150 \tTraining Loss: 3554.939663 \tValidation Loss: 1121.344855\n","  Epoch: 200 \tTraining Loss: 3267.940797 \tValidation Loss: 1092.656267\n","  Epoch: 250 \tTraining Loss: 3356.221143 \tValidation Loss: 1100.892662\n","  Epoch: 300 \tTraining Loss: 3390.557899 \tValidation Loss: 1110.055069\n","Best Validation Loss: 474.674845 in epoch 216\n","Best Train Loss: 3109.832148 in epoch 298\n","  Test Loss: 8.966349\n","\n","  Mean Abs Loss: 4.847619\n","\n","  Test Loss: 8.828888\n","\n","  Mean Abs Loss: 4.771429\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4625.076863 \tValidation Loss: 1483.917929\n","  Epoch: 100 \tTraining Loss: 4231.045996 \tValidation Loss: 1279.887255\n","  Epoch: 150 \tTraining Loss: 3995.378331 \tValidation Loss: 1230.727545\n","  Epoch: 200 \tTraining Loss: 3988.734870 \tValidation Loss: 1228.624336\n","  Epoch: 250 \tTraining Loss: 3921.675534 \tValidation Loss: 1228.709442\n","  Epoch: 300 \tTraining Loss: 3893.442935 \tValidation Loss: 1215.986759\n","Best Validation Loss: 538.529412 in epoch 107\n","Best Train Loss: 2953.571938 in epoch 255\n","  Test Loss: 9.208265\n","\n","  Mean Abs Loss: 5.133333\n","\n","  Test Loss: 9.946590\n","\n","  Mean Abs Loss: 5.780952\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3603.991400 \tValidation Loss: 1094.358745\n","  Epoch: 100 \tTraining Loss: 3525.825510 \tValidation Loss: 748.725155\n","  Epoch: 150 \tTraining Loss: 3590.489701 \tValidation Loss: 1072.945397\n","  Epoch: 200 \tTraining Loss: 3255.659249 \tValidation Loss: 1090.198255\n","  Epoch: 250 \tTraining Loss: 3271.819164 \tValidation Loss: 1089.963551\n","  Epoch: 300 \tTraining Loss: 3249.319989 \tValidation Loss: 1085.467372\n","Best Validation Loss: 446.203010 in epoch 266\n","Best Train Loss: 2942.884351 in epoch 210\n","  Test Loss: 8.683522\n","\n","  Mean Abs Loss: 4.580952\n","\n","  Test Loss: 8.754412\n","\n","  Mean Abs Loss: 4.628571\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4461.283439 \tValidation Loss: 1292.255818\n","  Epoch: 100 \tTraining Loss: 4120.798041 \tValidation Loss: 850.707078\n","  Epoch: 150 \tTraining Loss: 3994.251243 \tValidation Loss: 1238.736529\n","  Epoch: 200 \tTraining Loss: 3827.361390 \tValidation Loss: 1174.548340\n","  Epoch: 250 \tTraining Loss: 3828.917055 \tValidation Loss: 1164.833059\n","  Epoch: 300 \tTraining Loss: 3788.934005 \tValidation Loss: 1147.305004\n","Best Validation Loss: 813.657784 in epoch 161\n","Best Train Loss: 3179.706687 in epoch 136\n","  Test Loss: 9.279496\n","\n","  Mean Abs Loss: 5.295238\n","\n","  Test Loss: 9.171998\n","\n","  Mean Abs Loss: 5.247619\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3613.121010 \tValidation Loss: 1010.681451\n","  Epoch: 100 \tTraining Loss: 3693.220392 \tValidation Loss: 817.934841\n","  Epoch: 150 \tTraining Loss: 3362.018055 \tValidation Loss: 1082.756349\n","  Epoch: 200 \tTraining Loss: 3495.777539 \tValidation Loss: 1084.684372\n","  Epoch: 250 \tTraining Loss: 3287.289903 \tValidation Loss: 1065.704618\n","  Epoch: 300 \tTraining Loss: 3442.867681 \tValidation Loss: 1109.963850\n","Best Validation Loss: 705.303842 in epoch 174\n","Best Train Loss: 2565.903559 in epoch 277\n","  Test Loss: 8.763942\n","\n","  Mean Abs Loss: 4.695238\n","\n","  Test Loss: 8.735308\n","\n","  Mean Abs Loss: 4.723810\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4270.594006 \tValidation Loss: 1359.899695\n","  Epoch: 100 \tTraining Loss: 4068.298288 \tValidation Loss: 1175.106604\n","  Epoch: 150 \tTraining Loss: 3938.066788 \tValidation Loss: 1137.457581\n","  Epoch: 200 \tTraining Loss: 3942.446758 \tValidation Loss: 1137.784260\n","  Epoch: 250 \tTraining Loss: 3715.917365 \tValidation Loss: 483.139769\n","  Epoch: 300 \tTraining Loss: 3741.528896 \tValidation Loss: 981.361957\n","Best Validation Loss: 483.139769 in epoch 250\n","Best Train Loss: 2908.179406 in epoch 285\n","  Test Loss: 8.891550\n","\n","  Mean Abs Loss: 5.161905\n","\n","  Test Loss: 8.871514\n","\n","  Mean Abs Loss: 5.142857\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3811.187721 \tValidation Loss: 1109.511972\n","  Epoch: 100 \tTraining Loss: 3778.590339 \tValidation Loss: 1119.071541\n","  Epoch: 150 \tTraining Loss: 3474.707484 \tValidation Loss: 795.825938\n","  Epoch: 200 \tTraining Loss: 3417.043949 \tValidation Loss: 1084.189171\n","  Epoch: 250 \tTraining Loss: 3428.960509 \tValidation Loss: 1092.969044\n","  Epoch: 300 \tTraining Loss: 3414.485496 \tValidation Loss: 1089.435711\n","Best Validation Loss: 744.291070 in epoch 188\n","Best Train Loss: 2611.314374 in epoch 263\n","  Test Loss: 8.893113\n","\n","  Mean Abs Loss: 4.857143\n","\n","  Test Loss: 8.814941\n","\n","  Mean Abs Loss: 4.800000\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4550.687147 \tValidation Loss: 1325.433809\n","  Epoch: 100 \tTraining Loss: 4134.149146 \tValidation Loss: 1172.001131\n","  Epoch: 150 \tTraining Loss: 3871.350743 \tValidation Loss: 1124.519675\n","  Epoch: 200 \tTraining Loss: 3765.117286 \tValidation Loss: 1128.661489\n","  Epoch: 250 \tTraining Loss: 3729.016980 \tValidation Loss: 1133.077390\n","  Epoch: 300 \tTraining Loss: 3714.863339 \tValidation Loss: 1123.280617\n","Best Validation Loss: 786.654779 in epoch 186\n","Best Train Loss: 3429.841935 in epoch 213\n","  Test Loss: 9.535176\n","\n","  Mean Abs Loss: 5.447619\n","\n","  Test Loss: 9.555417\n","\n","  Mean Abs Loss: 5.485714\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3587.387365 \tValidation Loss: 1112.496886\n","  Epoch: 100 \tTraining Loss: 3575.517150 \tValidation Loss: 1092.104919\n","  Epoch: 150 \tTraining Loss: 3449.375580 \tValidation Loss: 1093.940129\n","  Epoch: 200 \tTraining Loss: 3326.744001 \tValidation Loss: 1101.028135\n","  Epoch: 250 \tTraining Loss: 3422.568477 \tValidation Loss: 1104.045044\n","  Epoch: 300 \tTraining Loss: 3343.607963 \tValidation Loss: 1120.823139\n","Best Validation Loss: 464.842817 in epoch 292\n","Best Train Loss: 2990.921141 in epoch 292\n","  Test Loss: 8.742713\n","\n","  Mean Abs Loss: 4.761905\n","\n","  Test Loss: 8.742713\n","\n","  Mean Abs Loss: 4.761905\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4474.378286 \tValidation Loss: 1322.675819\n","  Epoch: 100 \tTraining Loss: 4238.040385 \tValidation Loss: 1179.352311\n","  Epoch: 150 \tTraining Loss: 4008.990226 \tValidation Loss: 1138.444680\n","  Epoch: 200 \tTraining Loss: 3756.730644 \tValidation Loss: 1127.971970\n","  Epoch: 250 \tTraining Loss: 3612.424600 \tValidation Loss: 1121.401992\n","  Epoch: 300 \tTraining Loss: 3682.855028 \tValidation Loss: 822.473910\n","Best Validation Loss: 501.194315 in epoch 111\n","Best Train Loss: 3208.334074 in epoch 105\n","  Test Loss: 9.514844\n","\n","  Mean Abs Loss: 5.371429\n","\n","  Test Loss: 9.347510\n","\n","  Mean Abs Loss: 5.171429\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3761.682084 \tValidation Loss: 1132.539945\n","  Epoch: 100 \tTraining Loss: 3652.899535 \tValidation Loss: 1103.519879\n","  Epoch: 150 \tTraining Loss: 3542.782101 \tValidation Loss: 1092.912287\n","  Epoch: 200 \tTraining Loss: 3499.616882 \tValidation Loss: 1091.055635\n","  Epoch: 250 \tTraining Loss: 3421.624310 \tValidation Loss: 1096.797208\n","  Epoch: 300 \tTraining Loss: 3410.312365 \tValidation Loss: 1095.195725\n","Best Validation Loss: 483.882043 in epoch 52\n","Best Train Loss: 2437.220345 in epoch 286\n","  Test Loss: 8.897456\n","\n","  Mean Abs Loss: 4.809524\n","\n","  Test Loss: 8.711645\n","\n","  Mean Abs Loss: 4.800000\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4590.725112 \tValidation Loss: 1412.239752\n","  Epoch: 100 \tTraining Loss: 4104.594559 \tValidation Loss: 1196.684844\n","  Epoch: 150 \tTraining Loss: 3980.038502 \tValidation Loss: 1153.888794\n","  Epoch: 200 \tTraining Loss: 3861.489293 \tValidation Loss: 1134.839797\n","  Epoch: 250 \tTraining Loss: 3784.806710 \tValidation Loss: 1122.305156\n","  Epoch: 300 \tTraining Loss: 3691.195787 \tValidation Loss: 1126.115783\n","Best Validation Loss: 786.329874 in epoch 226\n","Best Train Loss: 2905.646220 in epoch 283\n","  Test Loss: 8.545221\n","\n","  Mean Abs Loss: 4.771429\n","\n","  Test Loss: 8.558481\n","\n","  Mean Abs Loss: 4.866667\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3707.106022 \tValidation Loss: 1150.851309\n","  Epoch: 100 \tTraining Loss: 3595.179785 \tValidation Loss: 1148.737092\n","  Epoch: 150 \tTraining Loss: 3328.761412 \tValidation Loss: 1080.801840\n","  Epoch: 200 \tTraining Loss: 3418.310705 \tValidation Loss: 808.257166\n","  Epoch: 250 \tTraining Loss: 3365.315212 \tValidation Loss: 1112.674109\n","  Epoch: 300 \tTraining Loss: 3493.080722 \tValidation Loss: 779.657376\n","Best Validation Loss: 763.910697 in epoch 177\n","Best Train Loss: 2758.981248 in epoch 41\n","  Test Loss: 8.871138\n","\n","  Mean Abs Loss: 5.076190\n","\n","  Test Loss: 8.689862\n","\n","  Mean Abs Loss: 4.666667\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4623.004094 \tValidation Loss: 1338.651286\n","  Epoch: 100 \tTraining Loss: 4060.594674 \tValidation Loss: 1333.590998\n","  Epoch: 150 \tTraining Loss: 4055.415917 \tValidation Loss: 1228.576020\n","  Epoch: 200 \tTraining Loss: 3669.458028 \tValidation Loss: 1203.339506\n","  Epoch: 250 \tTraining Loss: 3905.809834 \tValidation Loss: 1167.179874\n","  Epoch: 300 \tTraining Loss: 3855.126451 \tValidation Loss: 801.548977\n","Best Validation Loss: 579.580608 in epoch 141\n","Best Train Loss: 3019.849798 in epoch 231\n","  Test Loss: 8.987986\n","\n","  Mean Abs Loss: 4.885714\n","\n","  Test Loss: 9.525908\n","\n","  Mean Abs Loss: 5.371429\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3629.427956 \tValidation Loss: 1095.846052\n","  Epoch: 100 \tTraining Loss: 3516.348974 \tValidation Loss: 1101.845696\n","  Epoch: 150 \tTraining Loss: 3430.825879 \tValidation Loss: 1098.852319\n","  Epoch: 200 \tTraining Loss: 3372.518259 \tValidation Loss: 1111.985742\n","  Epoch: 250 \tTraining Loss: 3311.965662 \tValidation Loss: 1098.393663\n","  Epoch: 300 \tTraining Loss: 3345.626785 \tValidation Loss: 1076.144950\n","Best Validation Loss: 745.092159 in epoch 234\n","Best Train Loss: 2442.238489 in epoch 270\n","  Test Loss: 8.707075\n","\n","  Mean Abs Loss: 4.628571\n","\n","  Test Loss: 8.710847\n","\n","  Mean Abs Loss: 4.552381\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4497.225513 \tValidation Loss: 1123.157137\n","  Epoch: 100 \tTraining Loss: 4138.812641 \tValidation Loss: 818.685650\n","  Epoch: 150 \tTraining Loss: 3940.971464 \tValidation Loss: 1133.015897\n","  Epoch: 200 \tTraining Loss: 3755.327155 \tValidation Loss: 1109.012818\n","  Epoch: 250 \tTraining Loss: 3728.202880 \tValidation Loss: 1124.005255\n","  Epoch: 300 \tTraining Loss: 3653.322972 \tValidation Loss: 1120.513778\n","Best Validation Loss: 664.795512 in epoch 257\n","Best Train Loss: 2937.541983 in epoch 232\n","  Test Loss: 8.955580\n","\n","  Mean Abs Loss: 4.876190\n","\n","  Test Loss: 8.739063\n","\n","  Mean Abs Loss: 4.790476\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3768.527132 \tValidation Loss: 1136.888295\n","  Epoch: 100 \tTraining Loss: 3648.079269 \tValidation Loss: 1132.144875\n","  Epoch: 150 \tTraining Loss: 3575.863620 \tValidation Loss: 1092.777474\n","  Epoch: 200 \tTraining Loss: 3451.357266 \tValidation Loss: 780.030936\n","  Epoch: 250 \tTraining Loss: 3383.992492 \tValidation Loss: 1097.777157\n","  Epoch: 300 \tTraining Loss: 3392.312809 \tValidation Loss: 1136.112380\n","Best Validation Loss: 706.114205 in epoch 127\n","Best Train Loss: 2669.584299 in epoch 172\n","  Test Loss: 8.703520\n","\n","  Mean Abs Loss: 4.580952\n","\n","  Test Loss: 8.718941\n","\n","  Mean Abs Loss: 4.819048\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4669.259109 \tValidation Loss: 1132.989636\n","  Epoch: 100 \tTraining Loss: 4143.309599 \tValidation Loss: 1152.422396\n","  Epoch: 150 \tTraining Loss: 3871.089731 \tValidation Loss: 830.050646\n","  Epoch: 200 \tTraining Loss: 3790.259304 \tValidation Loss: 1159.228759\n","  Epoch: 250 \tTraining Loss: 3744.985295 \tValidation Loss: 1147.937796\n","  Epoch: 300 \tTraining Loss: 3730.436793 \tValidation Loss: 1144.454302\n","Best Validation Loss: 792.406816 in epoch 261\n","Best Train Loss: 2825.278142 in epoch 284\n","  Test Loss: 8.666538\n","\n","  Mean Abs Loss: 4.857143\n","\n","  Test Loss: 8.591677\n","\n","  Mean Abs Loss: 4.819048\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3768.555936 \tValidation Loss: 1131.010015\n","  Epoch: 100 \tTraining Loss: 3558.017232 \tValidation Loss: 1096.801741\n","  Epoch: 150 \tTraining Loss: 3562.150634 \tValidation Loss: 1089.036111\n","  Epoch: 200 \tTraining Loss: 3687.577707 \tValidation Loss: 1067.330159\n","  Epoch: 250 \tTraining Loss: 3363.265949 \tValidation Loss: 1079.130846\n","  Epoch: 300 \tTraining Loss: 3342.961690 \tValidation Loss: 1073.504679\n","Best Validation Loss: 479.064157 in epoch 277\n","Best Train Loss: 2830.696413 in epoch 129\n","  Test Loss: 8.491984\n","\n","  Mean Abs Loss: 4.580952\n","\n","  Test Loss: 8.805854\n","\n","  Mean Abs Loss: 4.733333\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4598.362640 \tValidation Loss: 1427.404973\n","  Epoch: 100 \tTraining Loss: 4024.428137 \tValidation Loss: 1257.818178\n","  Epoch: 150 \tTraining Loss: 3989.568209 \tValidation Loss: 1208.462797\n","  Epoch: 200 \tTraining Loss: 3782.632596 \tValidation Loss: 1171.146894\n","  Epoch: 250 \tTraining Loss: 3731.125710 \tValidation Loss: 1140.181639\n","  Epoch: 300 \tTraining Loss: 3740.074579 \tValidation Loss: 1103.523333\n","Best Validation Loss: 633.461289 in epoch 274\n","Best Train Loss: 3007.239597 in epoch 230\n","  Test Loss: 8.733226\n","\n","  Mean Abs Loss: 4.809524\n","\n","  Test Loss: 8.816018\n","\n","  Mean Abs Loss: 4.771429\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3731.350448 \tValidation Loss: 1018.577413\n","  Epoch: 100 \tTraining Loss: 3549.397575 \tValidation Loss: 1105.410671\n","  Epoch: 150 \tTraining Loss: 3395.556180 \tValidation Loss: 1078.556343\n","  Epoch: 200 \tTraining Loss: 3167.268387 \tValidation Loss: 1084.682221\n","  Epoch: 250 \tTraining Loss: 3271.834148 \tValidation Loss: 1097.349140\n","  Epoch: 300 \tTraining Loss: 3210.324351 \tValidation Loss: 1105.553193\n","Best Validation Loss: 742.547444 in epoch 247\n","Best Train Loss: 2900.074321 in epoch 27\n","  Test Loss: 9.661062\n","\n","  Mean Abs Loss: 5.390476\n","\n","  Test Loss: 8.694899\n","\n","  Mean Abs Loss: 4.628571\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4568.817437 \tValidation Loss: 1319.096540\n","  Epoch: 100 \tTraining Loss: 4020.003257 \tValidation Loss: 1174.809487\n","  Epoch: 150 \tTraining Loss: 3932.103566 \tValidation Loss: 1129.803926\n","  Epoch: 200 \tTraining Loss: 3706.969655 \tValidation Loss: 800.337864\n","  Epoch: 250 \tTraining Loss: 3725.491292 \tValidation Loss: 1117.563901\n","  Epoch: 300 \tTraining Loss: 3696.781254 \tValidation Loss: 1133.689705\n","Best Validation Loss: 676.023589 in epoch 262\n","Best Train Loss: 3400.816658 in epoch 161\n","  Test Loss: 9.013724\n","\n","  Mean Abs Loss: 5.152381\n","\n","  Test Loss: 8.838849\n","\n","  Mean Abs Loss: 5.066667\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3662.730239 \tValidation Loss: 1116.500852\n","  Epoch: 100 \tTraining Loss: 3628.780945 \tValidation Loss: 1119.818611\n","  Epoch: 150 \tTraining Loss: 3573.391901 \tValidation Loss: 1102.445288\n","  Epoch: 200 \tTraining Loss: 3364.840334 \tValidation Loss: 1072.708151\n","  Epoch: 250 \tTraining Loss: 3260.024672 \tValidation Loss: 944.403000\n","  Epoch: 300 \tTraining Loss: 3244.640401 \tValidation Loss: 809.682344\n","Best Validation Loss: 720.702068 in epoch 247\n","Best Train Loss: 3007.768951 in epoch 270\n","  Test Loss: 8.552033\n","\n","  Mean Abs Loss: 4.600000\n","\n","  Test Loss: 8.511708\n","\n","  Mean Abs Loss: 4.580952\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4581.220141 \tValidation Loss: 1472.301140\n","  Epoch: 100 \tTraining Loss: 4086.483340 \tValidation Loss: 1247.214617\n","  Epoch: 150 \tTraining Loss: 3973.333680 \tValidation Loss: 1247.551207\n","  Epoch: 200 \tTraining Loss: 3788.398411 \tValidation Loss: 1212.999090\n","  Epoch: 250 \tTraining Loss: 3778.562760 \tValidation Loss: 1158.813808\n","  Epoch: 300 \tTraining Loss: 3722.229664 \tValidation Loss: 1140.892336\n","Best Validation Loss: 803.051279 in epoch 265\n","Best Train Loss: 3455.039728 in epoch 199\n","  Test Loss: 9.378475\n","\n","  Mean Abs Loss: 5.523810\n","\n","  Test Loss: 8.881848\n","\n","  Mean Abs Loss: 5.133333\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3754.063859 \tValidation Loss: 1149.416613\n","  Epoch: 100 \tTraining Loss: 3558.010241 \tValidation Loss: 1110.794080\n","  Epoch: 150 \tTraining Loss: 3109.606973 \tValidation Loss: 1077.809518\n","  Epoch: 200 \tTraining Loss: 3345.412157 \tValidation Loss: 1098.310350\n","  Epoch: 250 \tTraining Loss: 3312.759011 \tValidation Loss: 1092.768885\n","  Epoch: 300 \tTraining Loss: 3288.887205 \tValidation Loss: 1085.098731\n","Best Validation Loss: 711.199626 in epoch 151\n","Best Train Loss: 3052.557929 in epoch 244\n","  Test Loss: 8.806897\n","\n","  Mean Abs Loss: 4.657143\n","\n","  Test Loss: 8.704928\n","\n","  Mean Abs Loss: 4.685714\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4633.620987 \tValidation Loss: 1361.569462\n","  Epoch: 100 \tTraining Loss: 4177.528272 \tValidation Loss: 1185.454657\n","  Epoch: 150 \tTraining Loss: 3815.113519 \tValidation Loss: 1114.859324\n","  Epoch: 200 \tTraining Loss: 3839.022090 \tValidation Loss: 1114.325380\n","  Epoch: 250 \tTraining Loss: 3736.479930 \tValidation Loss: 1110.561502\n","  Epoch: 300 \tTraining Loss: 3797.938075 \tValidation Loss: 1136.739058\n","Best Validation Loss: 713.394460 in epoch 239\n","Best Train Loss: 3533.589776 in epoch 276\n","  Test Loss: 8.876383\n","\n","  Mean Abs Loss: 5.085714\n","\n","  Test Loss: 8.907189\n","\n","  Mean Abs Loss: 5.095238\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3792.055238 \tValidation Loss: 1140.786430\n","  Epoch: 100 \tTraining Loss: 3548.046170 \tValidation Loss: 1128.749874\n","  Epoch: 150 \tTraining Loss: 3523.520190 \tValidation Loss: 1078.692669\n","  Epoch: 200 \tTraining Loss: 3422.965085 \tValidation Loss: 1090.218682\n","  Epoch: 250 \tTraining Loss: 3348.570558 \tValidation Loss: 1059.698670\n","  Epoch: 300 \tTraining Loss: 3408.861222 \tValidation Loss: 1098.867443\n","Best Validation Loss: 739.617170 in epoch 217\n","Best Train Loss: 2676.230744 in epoch 119\n","  Test Loss: 8.585467\n","\n","  Mean Abs Loss: 4.628571\n","\n","  Test Loss: 8.733609\n","\n","  Mean Abs Loss: 4.771429\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4758.123810 \tValidation Loss: 1472.717089\n","  Epoch: 100 \tTraining Loss: 4009.920198 \tValidation Loss: 1234.499573\n","  Epoch: 150 \tTraining Loss: 3896.918579 \tValidation Loss: 1156.956542\n","  Epoch: 200 \tTraining Loss: 3854.479114 \tValidation Loss: 1147.248162\n","  Epoch: 250 \tTraining Loss: 3839.340069 \tValidation Loss: 804.958810\n","  Epoch: 300 \tTraining Loss: 3638.206410 \tValidation Loss: 779.642245\n","Best Validation Loss: 491.636358 in epoch 277\n","Best Train Loss: 2753.166689 in epoch 108\n","  Test Loss: 8.985424\n","\n","  Mean Abs Loss: 4.904762\n","\n","  Test Loss: 8.834855\n","\n","  Mean Abs Loss: 4.857143\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3710.023079 \tValidation Loss: 1110.103151\n","  Epoch: 100 \tTraining Loss: 3589.738034 \tValidation Loss: 1091.282992\n","  Epoch: 150 \tTraining Loss: 3599.115758 \tValidation Loss: 1114.322271\n","  Epoch: 200 \tTraining Loss: 3263.787591 \tValidation Loss: 1139.849008\n","  Epoch: 250 \tTraining Loss: 3454.525462 \tValidation Loss: 1099.443342\n","  Epoch: 300 \tTraining Loss: 3453.276088 \tValidation Loss: 1113.434527\n","Best Validation Loss: 680.874033 in epoch 281\n","Best Train Loss: 3225.727073 in epoch 101\n","  Test Loss: 8.680367\n","\n","  Mean Abs Loss: 4.771429\n","\n","  Test Loss: 8.945929\n","\n","  Mean Abs Loss: 4.904762\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4277.403087 \tValidation Loss: 1343.823602\n","  Epoch: 100 \tTraining Loss: 4118.589377 \tValidation Loss: 1299.157320\n","  Epoch: 150 \tTraining Loss: 3954.412512 \tValidation Loss: 1163.722602\n","  Epoch: 200 \tTraining Loss: 3878.498279 \tValidation Loss: 1152.436228\n","  Epoch: 250 \tTraining Loss: 3934.331337 \tValidation Loss: 1138.376648\n","  Epoch: 300 \tTraining Loss: 3840.030076 \tValidation Loss: 1149.807339\n","Best Validation Loss: 751.753151 in epoch 96\n","Best Train Loss: 2937.456268 in epoch 242\n","  Test Loss: 9.499214\n","\n","  Mean Abs Loss: 5.257143\n","\n","  Test Loss: 9.705845\n","\n","  Mean Abs Loss: 5.466667\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3923.122440 \tValidation Loss: 1239.363190\n","  Epoch: 100 \tTraining Loss: 3949.549125 \tValidation Loss: 1143.583879\n","  Epoch: 150 \tTraining Loss: 3545.870597 \tValidation Loss: 798.641853\n","  Epoch: 200 \tTraining Loss: 3512.454876 \tValidation Loss: 789.037698\n","  Epoch: 250 \tTraining Loss: 3476.014056 \tValidation Loss: 1094.915905\n","  Epoch: 300 \tTraining Loss: 3379.008541 \tValidation Loss: 1115.133324\n","Best Validation Loss: 668.603698 in epoch 54\n","Best Train Loss: 3128.177270 in epoch 289\n","  Test Loss: 8.875360\n","\n","  Mean Abs Loss: 4.790476\n","\n","  Test Loss: 9.142072\n","\n","  Mean Abs Loss: 5.180952\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4738.289731 \tValidation Loss: 1498.223215\n","  Epoch: 100 \tTraining Loss: 4430.719962 \tValidation Loss: 1298.815970\n","  Epoch: 150 \tTraining Loss: 4203.710678 \tValidation Loss: 1289.503963\n","  Epoch: 200 \tTraining Loss: 3942.393041 \tValidation Loss: 1247.461481\n","  Epoch: 250 \tTraining Loss: 3509.437946 \tValidation Loss: 1247.663111\n","  Epoch: 300 \tTraining Loss: 3741.853141 \tValidation Loss: 1078.093511\n","Best Validation Loss: 844.728108 in epoch 278\n","Best Train Loss: 3090.301797 in epoch 221\n","  Test Loss: 9.798674\n","\n","  Mean Abs Loss: 5.885714\n","\n","  Test Loss: 9.056137\n","\n","  Mean Abs Loss: 5.361905\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3722.539328 \tValidation Loss: 1151.163050\n","  Epoch: 100 \tTraining Loss: 3622.045138 \tValidation Loss: 1131.641827\n","  Epoch: 150 \tTraining Loss: 3427.352675 \tValidation Loss: 1133.152739\n","  Epoch: 200 \tTraining Loss: 3468.055768 \tValidation Loss: 1140.270446\n","  Epoch: 250 \tTraining Loss: 3180.318139 \tValidation Loss: 1126.686446\n","  Epoch: 300 \tTraining Loss: 3251.600960 \tValidation Loss: 1128.885559\n","Best Validation Loss: 764.766194 in epoch 194\n","Best Train Loss: 2523.407446 in epoch 176\n","  Test Loss: 8.667912\n","\n","  Mean Abs Loss: 4.628571\n","\n","  Test Loss: 8.602452\n","\n","  Mean Abs Loss: 4.666667\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4675.264796 \tValidation Loss: 1470.455031\n","  Epoch: 100 \tTraining Loss: 4197.720651 \tValidation Loss: 1243.436677\n","  Epoch: 150 \tTraining Loss: 3909.000757 \tValidation Loss: 1220.912462\n","  Epoch: 200 \tTraining Loss: 3822.670526 \tValidation Loss: 1220.071879\n","  Epoch: 250 \tTraining Loss: 3776.055970 \tValidation Loss: 1205.358917\n","  Epoch: 300 \tTraining Loss: 3704.691994 \tValidation Loss: 1152.866124\n","Best Validation Loss: 697.597367 in epoch 201\n","Best Train Loss: 3447.781792 in epoch 236\n","  Test Loss: 9.198668\n","\n","  Mean Abs Loss: 5.247619\n","\n","  Test Loss: 9.285419\n","\n","  Mean Abs Loss: 5.285714\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3737.609512 \tValidation Loss: 1140.597259\n","  Epoch: 100 \tTraining Loss: 3546.221764 \tValidation Loss: 1099.319955\n","  Epoch: 150 \tTraining Loss: 3408.654258 \tValidation Loss: 1081.689482\n","  Epoch: 200 \tTraining Loss: 3437.392789 \tValidation Loss: 1079.178799\n","  Epoch: 250 \tTraining Loss: 3419.297729 \tValidation Loss: 1095.037806\n","  Epoch: 300 \tTraining Loss: 3327.949125 \tValidation Loss: 1080.743139\n","Best Validation Loss: 755.388936 in epoch 148\n","Best Train Loss: 2448.968716 in epoch 296\n","  Test Loss: 8.829344\n","\n","  Mean Abs Loss: 4.714286\n","\n","  Test Loss: 8.839348\n","\n","  Mean Abs Loss: 4.790476\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4684.070273 \tValidation Loss: 1383.331678\n","  Epoch: 100 \tTraining Loss: 4154.337270 \tValidation Loss: 1229.904836\n","  Epoch: 150 \tTraining Loss: 3924.489888 \tValidation Loss: 1142.632480\n","  Epoch: 200 \tTraining Loss: 3812.160482 \tValidation Loss: 1026.692570\n","  Epoch: 250 \tTraining Loss: 3795.237255 \tValidation Loss: 1123.537164\n","  Epoch: 300 \tTraining Loss: 3709.746814 \tValidation Loss: 1132.505336\n","Best Validation Loss: 633.499889 in epoch 276\n","Best Train Loss: 3395.798084 in epoch 193\n","  Test Loss: 9.676661\n","\n","  Mean Abs Loss: 5.495238\n","\n","  Test Loss: 9.628230\n","\n","  Mean Abs Loss: 5.428571\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3683.705961 \tValidation Loss: 1145.034757\n","  Epoch: 100 \tTraining Loss: 3662.009043 \tValidation Loss: 1118.202145\n","  Epoch: 150 \tTraining Loss: 3556.691865 \tValidation Loss: 1083.980780\n","  Epoch: 200 \tTraining Loss: 3412.746917 \tValidation Loss: 1097.377019\n","  Epoch: 250 \tTraining Loss: 3400.016817 \tValidation Loss: 673.067160\n","  Epoch: 300 \tTraining Loss: 3363.738789 \tValidation Loss: 1117.315724\n","Best Validation Loss: 472.995231 in epoch 214\n","Best Train Loss: 3079.271897 in epoch 284\n","  Test Loss: 8.928382\n","\n","  Mean Abs Loss: 4.838095\n","\n","  Test Loss: 8.639300\n","\n","  Mean Abs Loss: 4.704762\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4654.692869 \tValidation Loss: 1423.515274\n","  Epoch: 100 \tTraining Loss: 4292.857738 \tValidation Loss: 1219.461661\n","  Epoch: 150 \tTraining Loss: 3876.812218 \tValidation Loss: 1026.828950\n","  Epoch: 200 \tTraining Loss: 3906.810799 \tValidation Loss: 1160.061140\n","  Epoch: 250 \tTraining Loss: 3845.804051 \tValidation Loss: 1168.037612\n","  Epoch: 300 \tTraining Loss: 3687.347628 \tValidation Loss: 1160.510696\n","Best Validation Loss: 727.069225 in epoch 243\n","Best Train Loss: 3568.223699 in epoch 180\n","  Test Loss: 9.090156\n","\n","  Mean Abs Loss: 5.228571\n","\n","  Test Loss: 8.955586\n","\n","  Mean Abs Loss: 5.095238\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3667.398087 \tValidation Loss: 1127.222001\n","  Epoch: 100 \tTraining Loss: 3552.495827 \tValidation Loss: 800.917359\n","  Epoch: 150 \tTraining Loss: 3537.253846 \tValidation Loss: 1086.528079\n","  Epoch: 200 \tTraining Loss: 3373.118703 \tValidation Loss: 1077.711797\n","  Epoch: 250 \tTraining Loss: 3394.258598 \tValidation Loss: 1086.061605\n","  Epoch: 300 \tTraining Loss: 3337.610969 \tValidation Loss: 1065.337165\n","Best Validation Loss: 611.309016 in epoch 141\n","Best Train Loss: 2485.936315 in epoch 271\n","  Test Loss: 8.765653\n","\n","  Mean Abs Loss: 4.742857\n","\n","  Test Loss: 8.722311\n","\n","  Mean Abs Loss: 4.647619\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4524.194250 \tValidation Loss: 1313.151697\n","  Epoch: 100 \tTraining Loss: 4201.602219 \tValidation Loss: 1252.826748\n","  Epoch: 150 \tTraining Loss: 3974.777322 \tValidation Loss: 1232.019549\n","  Epoch: 200 \tTraining Loss: 3928.626474 \tValidation Loss: 1219.441596\n","  Epoch: 250 \tTraining Loss: 3692.662251 \tValidation Loss: 1130.821845\n","  Epoch: 300 \tTraining Loss: 3794.388599 \tValidation Loss: 1169.834585\n","Best Validation Loss: 670.735395 in epoch 116\n","Best Train Loss: 3111.512204 in epoch 168\n","  Test Loss: 9.496232\n","\n","  Mean Abs Loss: 5.571429\n","\n","  Test Loss: 9.571884\n","\n","  Mean Abs Loss: 5.695238\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3643.960306 \tValidation Loss: 1107.411526\n","  Epoch: 100 \tTraining Loss: 3484.476206 \tValidation Loss: 1100.559183\n","  Epoch: 150 \tTraining Loss: 3450.923108 \tValidation Loss: 953.483099\n","  Epoch: 200 \tTraining Loss: 3480.324589 \tValidation Loss: 1110.454986\n","  Epoch: 250 \tTraining Loss: 3392.646222 \tValidation Loss: 1112.640163\n","  Epoch: 300 \tTraining Loss: 3363.943277 \tValidation Loss: 817.046535\n","Best Validation Loss: 517.017890 in epoch 295\n","Best Train Loss: 2726.768982 in epoch 139\n","  Test Loss: 8.745006\n","\n","  Mean Abs Loss: 4.638095\n","\n","  Test Loss: 8.756430\n","\n","  Mean Abs Loss: 4.695238\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4504.367908 \tValidation Loss: 1306.942303\n","  Epoch: 100 \tTraining Loss: 4031.897638 \tValidation Loss: 1197.513431\n","  Epoch: 150 \tTraining Loss: 3845.818627 \tValidation Loss: 1169.565763\n","  Epoch: 200 \tTraining Loss: 3766.289829 \tValidation Loss: 1145.683714\n","  Epoch: 250 \tTraining Loss: 3698.590875 \tValidation Loss: 1116.776547\n","  Epoch: 300 \tTraining Loss: 3715.534532 \tValidation Loss: 1125.506080\n","Best Validation Loss: 821.883729 in epoch 171\n","Best Train Loss: 2937.696963 in epoch 193\n","  Test Loss: 8.680897\n","\n","  Mean Abs Loss: 4.761905\n","\n","  Test Loss: 8.653097\n","\n","  Mean Abs Loss: 4.771429\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3760.515674 \tValidation Loss: 1104.515398\n","  Epoch: 100 \tTraining Loss: 3580.654672 \tValidation Loss: 1107.914845\n","  Epoch: 150 \tTraining Loss: 3576.687233 \tValidation Loss: 1081.572391\n","  Epoch: 200 \tTraining Loss: 3536.390522 \tValidation Loss: 781.919950\n","  Epoch: 250 \tTraining Loss: 3356.906058 \tValidation Loss: 1056.872492\n","  Epoch: 300 \tTraining Loss: 3348.794676 \tValidation Loss: 924.790882\n","Best Validation Loss: 746.930564 in epoch 273\n","Best Train Loss: 2627.484820 in epoch 240\n","  Test Loss: 9.083741\n","\n","  Mean Abs Loss: 5.095238\n","\n","  Test Loss: 9.110855\n","\n","  Mean Abs Loss: 5.133333\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 10\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4175.948345 \tValidation Loss: 1308.765864\n","  Epoch: 100 \tTraining Loss: 3882.844316 \tValidation Loss: 1139.884040\n","  Epoch: 150 \tTraining Loss: 3930.443742 \tValidation Loss: 1137.128661\n","  Epoch: 200 \tTraining Loss: 3353.732749 \tValidation Loss: 1001.626124\n","  Epoch: 250 \tTraining Loss: 3716.932326 \tValidation Loss: 1128.244032\n","  Epoch: 300 \tTraining Loss: 3669.612005 \tValidation Loss: 1124.745346\n","Best Validation Loss: 792.975010 in epoch 219\n","Best Train Loss: 2971.556321 in epoch 251\n","  Test Loss: 8.552301\n","\n","  Mean Abs Loss: 4.838095\n","\n","  Test Loss: 8.611507\n","\n","  Mean Abs Loss: 4.847619\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3633.101983 \tValidation Loss: 1125.134043\n","  Epoch: 100 \tTraining Loss: 3565.627014 \tValidation Loss: 668.105312\n","  Epoch: 150 \tTraining Loss: 3479.548228 \tValidation Loss: 1111.852214\n","  Epoch: 200 \tTraining Loss: 3437.833543 \tValidation Loss: 1107.886845\n","  Epoch: 250 \tTraining Loss: 3234.051421 \tValidation Loss: 1097.201369\n","  Epoch: 300 \tTraining Loss: 3390.460406 \tValidation Loss: 1134.291735\n","Best Validation Loss: 660.261370 in epoch 273\n","Best Train Loss: 3063.827964 in epoch 296\n","  Test Loss: 8.771040\n","\n","  Mean Abs Loss: 4.647619\n","\n","  Test Loss: 8.733513\n","\n","  Mean Abs Loss: 4.657143\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4454.408786 \tValidation Loss: 1022.726485\n","  Epoch: 100 \tTraining Loss: 4185.892695 \tValidation Loss: 1132.046232\n","  Epoch: 150 \tTraining Loss: 4025.624461 \tValidation Loss: 1166.951734\n","  Epoch: 200 \tTraining Loss: 3774.482271 \tValidation Loss: 1163.101651\n","  Epoch: 250 \tTraining Loss: 3817.080887 \tValidation Loss: 1157.137533\n","  Epoch: 300 \tTraining Loss: 3759.593413 \tValidation Loss: 848.257016\n","Best Validation Loss: 812.111039 in epoch 212\n","Best Train Loss: 3171.672493 in epoch 123\n","  Test Loss: 9.564091\n","\n","  Mean Abs Loss: 5.428571\n","\n","  Test Loss: 8.868310\n","\n","  Mean Abs Loss: 5.161905\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3473.515661 \tValidation Loss: 1119.932325\n","  Epoch: 100 \tTraining Loss: 3651.409802 \tValidation Loss: 1099.571211\n","  Epoch: 150 \tTraining Loss: 3462.422072 \tValidation Loss: 1098.806683\n","  Epoch: 200 \tTraining Loss: 3455.045837 \tValidation Loss: 1111.369019\n","  Epoch: 250 \tTraining Loss: 3427.721471 \tValidation Loss: 1106.163784\n","  Epoch: 300 \tTraining Loss: 3279.379172 \tValidation Loss: 1129.646257\n","Best Validation Loss: 605.977274 in epoch 87\n","Best Train Loss: 2570.882916 in epoch 290\n","  Test Loss: 8.772819\n","\n","  Mean Abs Loss: 4.619048\n","\n","  Test Loss: 8.665573\n","\n","  Mean Abs Loss: 4.704762\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4637.504877 \tValidation Loss: 1415.824639\n","  Epoch: 100 \tTraining Loss: 4170.617978 \tValidation Loss: 1320.136097\n","  Epoch: 150 \tTraining Loss: 3951.814367 \tValidation Loss: 1202.030676\n","  Epoch: 200 \tTraining Loss: 3754.540433 \tValidation Loss: 985.220475\n","  Epoch: 250 \tTraining Loss: 3714.552447 \tValidation Loss: 1181.436750\n","  Epoch: 300 \tTraining Loss: 3745.011395 \tValidation Loss: 1167.543427\n","Best Validation Loss: 660.851267 in epoch 183\n","Best Train Loss: 3029.061400 in epoch 140\n","  Test Loss: 9.315699\n","\n","  Mean Abs Loss: 5.333333\n","\n","  Test Loss: 8.971345\n","\n","  Mean Abs Loss: 5.180952\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3733.812548 \tValidation Loss: 1167.668663\n","  Epoch: 100 \tTraining Loss: 3634.128239 \tValidation Loss: 1097.154005\n","  Epoch: 150 \tTraining Loss: 3504.433514 \tValidation Loss: 1105.469647\n","  Epoch: 200 \tTraining Loss: 3444.304052 \tValidation Loss: 1086.231573\n","  Epoch: 250 \tTraining Loss: 3412.666076 \tValidation Loss: 1090.279595\n","  Epoch: 300 \tTraining Loss: 3208.394273 \tValidation Loss: 1074.771437\n","Best Validation Loss: 677.203426 in epoch 16\n","Best Train Loss: 2585.555466 in epoch 289\n","  Test Loss: 8.881369\n","\n","  Mean Abs Loss: 4.790476\n","\n","  Test Loss: 9.581544\n","\n","  Mean Abs Loss: 5.323810\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4626.653952 \tValidation Loss: 1399.193001\n","  Epoch: 100 \tTraining Loss: 4115.696833 \tValidation Loss: 899.543721\n","  Epoch: 150 \tTraining Loss: 3849.145663 \tValidation Loss: 1181.665967\n","  Epoch: 200 \tTraining Loss: 3693.091111 \tValidation Loss: 1165.089595\n","  Epoch: 250 \tTraining Loss: 3832.017997 \tValidation Loss: 1166.786962\n","  Epoch: 300 \tTraining Loss: 3674.274276 \tValidation Loss: 1155.049167\n","Best Validation Loss: 819.828935 in epoch 178\n","Best Train Loss: 3315.655019 in epoch 284\n","  Test Loss: 8.753483\n","\n","  Mean Abs Loss: 5.123810\n","\n","  Test Loss: 8.783674\n","\n","  Mean Abs Loss: 5.104762\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3745.604821 \tValidation Loss: 1150.607410\n","  Epoch: 100 \tTraining Loss: 3626.854858 \tValidation Loss: 965.809985\n","  Epoch: 150 \tTraining Loss: 3639.183281 \tValidation Loss: 1120.873870\n","  Epoch: 200 \tTraining Loss: 3444.332720 \tValidation Loss: 1084.861643\n","  Epoch: 250 \tTraining Loss: 3428.071208 \tValidation Loss: 1082.158498\n","  Epoch: 300 \tTraining Loss: 3425.183550 \tValidation Loss: 1118.742996\n","Best Validation Loss: 697.301802 in epoch 162\n","Best Train Loss: 2743.498582 in epoch 129\n","  Test Loss: 8.718992\n","\n","  Mean Abs Loss: 4.800000\n","\n","  Test Loss: 8.614265\n","\n","  Mean Abs Loss: 4.847619\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4641.970060 \tValidation Loss: 1453.651227\n","  Epoch: 100 \tTraining Loss: 4109.840362 \tValidation Loss: 1265.136412\n","  Epoch: 150 \tTraining Loss: 4060.976450 \tValidation Loss: 1250.618702\n","  Epoch: 200 \tTraining Loss: 3840.149668 \tValidation Loss: 920.241988\n","  Epoch: 250 \tTraining Loss: 3850.907888 \tValidation Loss: 1213.314074\n","  Epoch: 300 \tTraining Loss: 3872.563224 \tValidation Loss: 888.551701\n","Best Validation Loss: 752.726461 in epoch 122\n","Best Train Loss: 3019.943932 in epoch 173\n","  Test Loss: 9.317392\n","\n","  Mean Abs Loss: 5.314286\n","\n","  Test Loss: 9.842986\n","\n","  Mean Abs Loss: 5.638095\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3716.037469 \tValidation Loss: 1120.072310\n","  Epoch: 100 \tTraining Loss: 3604.233596 \tValidation Loss: 1112.861871\n","  Epoch: 150 \tTraining Loss: 3516.812247 \tValidation Loss: 1090.751396\n","  Epoch: 200 \tTraining Loss: 3497.325048 \tValidation Loss: 1086.478002\n","  Epoch: 250 \tTraining Loss: 3403.720364 \tValidation Loss: 1094.695672\n","  Epoch: 300 \tTraining Loss: 3252.326346 \tValidation Loss: 1091.668536\n","Best Validation Loss: 468.024581 in epoch 260\n","Best Train Loss: 2509.419974 in epoch 277\n","  Test Loss: 8.530657\n","\n","  Mean Abs Loss: 4.685714\n","\n","  Test Loss: 8.567432\n","\n","  Mean Abs Loss: 4.695238\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4609.738169 \tValidation Loss: 1371.620152\n","  Epoch: 100 \tTraining Loss: 4158.213495 \tValidation Loss: 1262.693800\n","  Epoch: 150 \tTraining Loss: 4000.188559 \tValidation Loss: 1119.753858\n","  Epoch: 200 \tTraining Loss: 3804.176753 \tValidation Loss: 1204.345588\n","  Epoch: 250 \tTraining Loss: 3837.634833 \tValidation Loss: 901.055275\n","  Epoch: 300 \tTraining Loss: 3775.775373 \tValidation Loss: 1184.236443\n","Best Validation Loss: 502.605009 in epoch 198\n","Best Train Loss: 3013.855561 in epoch 226\n","  Test Loss: 8.987773\n","\n","  Mean Abs Loss: 5.133333\n","\n","  Test Loss: 9.183897\n","\n","  Mean Abs Loss: 5.247619\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3582.052684 \tValidation Loss: 1116.672686\n","  Epoch: 100 \tTraining Loss: 3515.859264 \tValidation Loss: 812.699984\n","  Epoch: 150 \tTraining Loss: 3278.163977 \tValidation Loss: 786.498714\n","  Epoch: 200 \tTraining Loss: 3315.499175 \tValidation Loss: 1122.883491\n","  Epoch: 250 \tTraining Loss: 3333.249870 \tValidation Loss: 1117.007158\n","  Epoch: 300 \tTraining Loss: 3326.733985 \tValidation Loss: 1111.088284\n","Best Validation Loss: 657.305711 in epoch 6\n","Best Train Loss: 3090.075324 in epoch 231\n","  Test Loss: 8.674104\n","\n","  Mean Abs Loss: 4.647619\n","\n","  Test Loss: 9.441107\n","\n","  Mean Abs Loss: 5.152381\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4466.452495 \tValidation Loss: 1324.733328\n","  Epoch: 100 \tTraining Loss: 3999.965163 \tValidation Loss: 1158.131745\n","  Epoch: 150 \tTraining Loss: 3884.063351 \tValidation Loss: 1139.835032\n","  Epoch: 200 \tTraining Loss: 3725.176630 \tValidation Loss: 1143.856807\n","  Epoch: 250 \tTraining Loss: 3630.689414 \tValidation Loss: 1143.577078\n","  Epoch: 300 \tTraining Loss: 3668.490131 \tValidation Loss: 1144.118181\n","Best Validation Loss: 803.763990 in epoch 294\n","Best Train Loss: 2866.781850 in epoch 293\n","  Test Loss: 8.972681\n","\n","  Mean Abs Loss: 5.133333\n","\n","  Test Loss: 8.900423\n","\n","  Mean Abs Loss: 5.057143\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3582.268156 \tValidation Loss: 1099.734346\n","  Epoch: 100 \tTraining Loss: 3561.836383 \tValidation Loss: 1137.923127\n","  Epoch: 150 \tTraining Loss: 3416.147747 \tValidation Loss: 1086.568729\n","  Epoch: 200 \tTraining Loss: 3210.208370 \tValidation Loss: 1096.574452\n","  Epoch: 250 \tTraining Loss: 3296.020952 \tValidation Loss: 1113.832742\n","  Epoch: 300 \tTraining Loss: 3302.417785 \tValidation Loss: 1026.958019\n","Best Validation Loss: 612.855083 in epoch 132\n","Best Train Loss: 2549.828690 in epoch 145\n","  Test Loss: 8.622876\n","\n","  Mean Abs Loss: 4.485714\n","\n","  Test Loss: 8.668692\n","\n","  Mean Abs Loss: 4.714286\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4579.724417 \tValidation Loss: 1085.021881\n","  Epoch: 100 \tTraining Loss: 4239.173776 \tValidation Loss: 1251.105508\n","  Epoch: 150 \tTraining Loss: 4015.417738 \tValidation Loss: 1240.236053\n","  Epoch: 200 \tTraining Loss: 3913.347409 \tValidation Loss: 813.659419\n","  Epoch: 250 \tTraining Loss: 3672.531253 \tValidation Loss: 1174.734804\n","  Epoch: 300 \tTraining Loss: 3777.493398 \tValidation Loss: 856.115631\n","Best Validation Loss: 510.006278 in epoch 238\n","Best Train Loss: 2877.642508 in epoch 299\n","  Test Loss: 9.140782\n","\n","  Mean Abs Loss: 5.200000\n","\n","  Test Loss: 9.207522\n","\n","  Mean Abs Loss: 5.200000\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3726.497983 \tValidation Loss: 1110.266454\n","  Epoch: 100 \tTraining Loss: 3636.192830 \tValidation Loss: 1095.527326\n","  Epoch: 150 \tTraining Loss: 3497.730559 \tValidation Loss: 1093.597596\n","  Epoch: 200 \tTraining Loss: 3417.418579 \tValidation Loss: 1102.412924\n","  Epoch: 250 \tTraining Loss: 3307.907714 \tValidation Loss: 937.776989\n","  Epoch: 300 \tTraining Loss: 3262.131780 \tValidation Loss: 1113.023814\n","Best Validation Loss: 640.739192 in epoch 239\n","Best Train Loss: 2499.943268 in epoch 226\n","  Test Loss: 8.613622\n","\n","  Mean Abs Loss: 4.638095\n","\n","  Test Loss: 8.744495\n","\n","  Mean Abs Loss: 4.590476\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4646.613889 \tValidation Loss: 1400.672763\n","  Epoch: 100 \tTraining Loss: 4127.992600 \tValidation Loss: 1196.877145\n","  Epoch: 150 \tTraining Loss: 3803.017543 \tValidation Loss: 1157.865436\n","  Epoch: 200 \tTraining Loss: 3779.319618 \tValidation Loss: 1160.638546\n","  Epoch: 250 \tTraining Loss: 3741.090042 \tValidation Loss: 1146.745569\n","  Epoch: 300 \tTraining Loss: 3754.451862 \tValidation Loss: 1156.215226\n","Best Validation Loss: 646.962967 in epoch 204\n","Best Train Loss: 2740.737871 in epoch 276\n","  Test Loss: 9.077896\n","\n","  Mean Abs Loss: 5.114286\n","\n","  Test Loss: 8.914222\n","\n","  Mean Abs Loss: 5.152381\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3713.494270 \tValidation Loss: 1146.893154\n","  Epoch: 100 \tTraining Loss: 3635.360024 \tValidation Loss: 1113.595626\n","  Epoch: 150 \tTraining Loss: 3556.093793 \tValidation Loss: 1099.242790\n","  Epoch: 200 \tTraining Loss: 3462.554441 \tValidation Loss: 1120.505205\n","  Epoch: 250 \tTraining Loss: 2560.550174 \tValidation Loss: 1105.065967\n","  Epoch: 300 \tTraining Loss: 3394.126262 \tValidation Loss: 1063.608939\n","Best Validation Loss: 620.916102 in epoch 70\n","Best Train Loss: 2501.780548 in epoch 282\n","  Test Loss: 8.772677\n","\n","  Mean Abs Loss: 4.733333\n","\n","  Test Loss: 9.443426\n","\n","  Mean Abs Loss: 5.085714\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 10\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4379.671217 \tValidation Loss: 1381.904959\n","  Epoch: 100 \tTraining Loss: 4026.357204 \tValidation Loss: 1190.262520\n","  Epoch: 150 \tTraining Loss: 3879.883241 \tValidation Loss: 1166.833855\n","  Epoch: 200 \tTraining Loss: 3758.861207 \tValidation Loss: 1149.029339\n","  Epoch: 250 \tTraining Loss: 3723.416047 \tValidation Loss: 1119.568485\n","  Epoch: 300 \tTraining Loss: 3745.661017 \tValidation Loss: 1122.886735\n","Best Validation Loss: 654.293282 in epoch 134\n","Best Train Loss: 2958.880181 in epoch 152\n","  Test Loss: 9.091388\n","\n","  Mean Abs Loss: 5.171429\n","\n","  Test Loss: 9.022322\n","\n","  Mean Abs Loss: 5.047619\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3715.646327 \tValidation Loss: 833.584977\n","  Epoch: 100 \tTraining Loss: 3651.648575 \tValidation Loss: 1124.934857\n","  Epoch: 150 \tTraining Loss: 3402.946125 \tValidation Loss: 1132.176640\n","  Epoch: 200 \tTraining Loss: 3467.862138 \tValidation Loss: 1105.831560\n","  Epoch: 250 \tTraining Loss: 3219.858244 \tValidation Loss: 1114.406734\n","  Epoch: 300 \tTraining Loss: 3396.352053 \tValidation Loss: 1135.723686\n","Best Validation Loss: 474.367394 in epoch 87\n","Best Train Loss: 2392.887747 in epoch 266\n","  Test Loss: 8.672893\n","\n","  Mean Abs Loss: 4.619048\n","\n","  Test Loss: 8.614162\n","\n","  Mean Abs Loss: 4.666667\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4612.387268 \tValidation Loss: 1400.456270\n","  Epoch: 100 \tTraining Loss: 4089.933670 \tValidation Loss: 1183.842358\n","  Epoch: 150 \tTraining Loss: 3678.974667 \tValidation Loss: 1182.239944\n","  Epoch: 200 \tTraining Loss: 3790.523917 \tValidation Loss: 1144.792488\n","  Epoch: 250 \tTraining Loss: 3735.576867 \tValidation Loss: 1129.219743\n","  Epoch: 300 \tTraining Loss: 3714.094852 \tValidation Loss: 1138.889404\n","Best Validation Loss: 504.111914 in epoch 286\n","Best Train Loss: 3035.565022 in epoch 187\n","  Test Loss: 9.554365\n","\n","  Mean Abs Loss: 5.257143\n","\n","  Test Loss: 8.988207\n","\n","  Mean Abs Loss: 5.104762\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3630.062921 \tValidation Loss: 1119.069343\n","  Epoch: 100 \tTraining Loss: 3608.839845 \tValidation Loss: 1090.172957\n","  Epoch: 150 \tTraining Loss: 3528.987466 \tValidation Loss: 1109.887380\n","  Epoch: 200 \tTraining Loss: 3389.336886 \tValidation Loss: 1109.593618\n","  Epoch: 250 \tTraining Loss: 3408.575056 \tValidation Loss: 1108.035419\n","  Epoch: 300 \tTraining Loss: 3392.802143 \tValidation Loss: 1104.860090\n","Best Validation Loss: 493.653540 in epoch 176\n","Best Train Loss: 2508.125320 in epoch 278\n","  Test Loss: 8.589693\n","\n","  Mean Abs Loss: 4.561905\n","\n","  Test Loss: 8.870642\n","\n","  Mean Abs Loss: 4.819048\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4647.170105 \tValidation Loss: 1448.666515\n","  Epoch: 100 \tTraining Loss: 4348.555747 \tValidation Loss: 1393.014965\n","  Epoch: 150 \tTraining Loss: 4206.741947 \tValidation Loss: 1303.538883\n","  Epoch: 200 \tTraining Loss: 4024.080638 \tValidation Loss: 1309.678218\n","  Epoch: 250 \tTraining Loss: 3976.022513 \tValidation Loss: 1039.085141\n","  Epoch: 300 \tTraining Loss: 3812.568861 \tValidation Loss: 1128.176860\n","Best Validation Loss: 674.235861 in epoch 273\n","Best Train Loss: 3405.200287 in epoch 131\n","  Test Loss: 9.781148\n","\n","  Mean Abs Loss: 5.714286\n","\n","  Test Loss: 8.845926\n","\n","  Mean Abs Loss: 4.809524\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3761.781810 \tValidation Loss: 1109.317945\n","  Epoch: 100 \tTraining Loss: 3534.528646 \tValidation Loss: 1122.268838\n","  Epoch: 150 \tTraining Loss: 3471.917508 \tValidation Loss: 1114.797403\n","  Epoch: 200 \tTraining Loss: 3424.144801 \tValidation Loss: 1113.986780\n","  Epoch: 250 \tTraining Loss: 3373.205526 \tValidation Loss: 1083.647407\n","  Epoch: 300 \tTraining Loss: 3337.617823 \tValidation Loss: 1025.605734\n","Best Validation Loss: 413.336083 in epoch 261\n","Best Train Loss: 3076.374797 in epoch 284\n","  Test Loss: 8.684108\n","\n","  Mean Abs Loss: 4.695238\n","\n","  Test Loss: 8.680730\n","\n","  Mean Abs Loss: 4.714286\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4722.716182 \tValidation Loss: 1390.708773\n","  Epoch: 100 \tTraining Loss: 4152.708549 \tValidation Loss: 1170.418626\n","  Epoch: 150 \tTraining Loss: 3936.301732 \tValidation Loss: 1151.477815\n","  Epoch: 200 \tTraining Loss: 3787.911437 \tValidation Loss: 1136.325701\n","  Epoch: 250 \tTraining Loss: 3838.923503 \tValidation Loss: 1137.451614\n","  Epoch: 300 \tTraining Loss: 3718.024679 \tValidation Loss: 824.002827\n","Best Validation Loss: 489.090488 in epoch 189\n","Best Train Loss: 2875.439051 in epoch 235\n","  Test Loss: 8.691359\n","\n","  Mean Abs Loss: 4.942857\n","\n","  Test Loss: 8.649557\n","\n","  Mean Abs Loss: 4.895238\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3661.023414 \tValidation Loss: 1120.266006\n","  Epoch: 100 \tTraining Loss: 3491.372556 \tValidation Loss: 1102.166811\n","  Epoch: 150 \tTraining Loss: 3436.097780 \tValidation Loss: 1122.308426\n","  Epoch: 200 \tTraining Loss: 3325.133537 \tValidation Loss: 1109.939413\n","  Epoch: 250 \tTraining Loss: 3104.143347 \tValidation Loss: 1125.078146\n","  Epoch: 300 \tTraining Loss: 3266.377612 \tValidation Loss: 1113.531901\n","Best Validation Loss: 496.990167 in epoch 77\n","Best Train Loss: 2914.669810 in epoch 246\n","  Test Loss: 8.347280\n","\n","  Mean Abs Loss: 4.409524\n","\n","  Test Loss: 8.812050\n","\n","  Mean Abs Loss: 5.104762\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4802.567029 \tValidation Loss: 1099.869413\n","  Epoch: 100 \tTraining Loss: 4161.584715 \tValidation Loss: 1263.860561\n","  Epoch: 150 \tTraining Loss: 3989.927628 \tValidation Loss: 1242.219596\n","  Epoch: 200 \tTraining Loss: 3976.999057 \tValidation Loss: 1208.931941\n","  Epoch: 250 \tTraining Loss: 3806.886441 \tValidation Loss: 1144.849450\n","  Epoch: 300 \tTraining Loss: 3700.957098 \tValidation Loss: 1160.160589\n","Best Validation Loss: 831.423231 in epoch 286\n","Best Train Loss: 3079.676351 in epoch 166\n","  Test Loss: 9.385943\n","\n","  Mean Abs Loss: 5.361905\n","\n","  Test Loss: 9.198215\n","\n","  Mean Abs Loss: 5.171429\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3652.175481 \tValidation Loss: 1144.420092\n","  Epoch: 100 \tTraining Loss: 3472.340389 \tValidation Loss: 945.809381\n","  Epoch: 150 \tTraining Loss: 3430.616021 \tValidation Loss: 970.305243\n","  Epoch: 200 \tTraining Loss: 3480.089291 \tValidation Loss: 985.642886\n","  Epoch: 250 \tTraining Loss: 3426.315539 \tValidation Loss: 1117.242036\n","  Epoch: 300 \tTraining Loss: 3331.318160 \tValidation Loss: 1123.624979\n","Best Validation Loss: 475.279552 in epoch 282\n","Best Train Loss: 2731.220220 in epoch 48\n","  Test Loss: 8.872188\n","\n","  Mean Abs Loss: 5.066667\n","\n","  Test Loss: 8.603693\n","\n","  Mean Abs Loss: 4.657143\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4441.235876 \tValidation Loss: 1336.237912\n","  Epoch: 100 \tTraining Loss: 3990.529091 \tValidation Loss: 863.112732\n","  Epoch: 150 \tTraining Loss: 3826.575895 \tValidation Loss: 1225.310549\n","  Epoch: 200 \tTraining Loss: 3725.576524 \tValidation Loss: 1217.965656\n","  Epoch: 250 \tTraining Loss: 3806.001877 \tValidation Loss: 1216.704703\n","  Epoch: 300 \tTraining Loss: 3752.645851 \tValidation Loss: 1206.774943\n","Best Validation Loss: 496.429604 in epoch 212\n","Best Train Loss: 3066.690470 in epoch 141\n","  Test Loss: 9.443925\n","\n","  Mean Abs Loss: 5.590476\n","\n","  Test Loss: 9.199780\n","\n","  Mean Abs Loss: 5.380952\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3709.904225 \tValidation Loss: 1139.303517\n","  Epoch: 100 \tTraining Loss: 3572.880129 \tValidation Loss: 956.676506\n","  Epoch: 150 \tTraining Loss: 3634.739881 \tValidation Loss: 1133.948763\n","  Epoch: 200 \tTraining Loss: 3420.514216 \tValidation Loss: 1110.858390\n","  Epoch: 250 \tTraining Loss: 3352.139224 \tValidation Loss: 1105.637172\n","  Epoch: 300 \tTraining Loss: 3363.403136 \tValidation Loss: 960.897278\n","Best Validation Loss: 632.203787 in epoch 270\n","Best Train Loss: 2543.164232 in epoch 275\n","  Test Loss: 8.825006\n","\n","  Mean Abs Loss: 4.714286\n","\n","  Test Loss: 8.695519\n","\n","  Mean Abs Loss: 4.714286\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4552.278439 \tValidation Loss: 1360.591347\n","  Epoch: 100 \tTraining Loss: 4136.822740 \tValidation Loss: 1255.565110\n","  Epoch: 150 \tTraining Loss: 3977.005282 \tValidation Loss: 1247.341819\n","  Epoch: 200 \tTraining Loss: 3922.065224 \tValidation Loss: 1215.894365\n","  Epoch: 250 \tTraining Loss: 3766.960037 \tValidation Loss: 1161.693954\n","  Epoch: 300 \tTraining Loss: 3823.490431 \tValidation Loss: 1150.549782\n","Best Validation Loss: 716.245834 in epoch 77\n","Best Train Loss: 3411.629488 in epoch 264\n","  Test Loss: 9.183745\n","\n","  Mean Abs Loss: 5.066667\n","\n","  Test Loss: 10.181389\n","\n","  Mean Abs Loss: 6.038095\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3865.720605 \tValidation Loss: 1150.622880\n","  Epoch: 100 \tTraining Loss: 3574.462844 \tValidation Loss: 1129.819878\n","  Epoch: 150 \tTraining Loss: 3494.441598 \tValidation Loss: 1094.598403\n","  Epoch: 200 \tTraining Loss: 3478.022682 \tValidation Loss: 1095.879932\n","  Epoch: 250 \tTraining Loss: 3406.122893 \tValidation Loss: 1109.704102\n","  Epoch: 300 \tTraining Loss: 3394.359415 \tValidation Loss: 1097.151709\n","Best Validation Loss: 659.421706 in epoch 176\n","Best Train Loss: 2575.263493 in epoch 299\n","  Test Loss: 8.590659\n","\n","  Mean Abs Loss: 4.657143\n","\n","  Test Loss: 8.672855\n","\n","  Mean Abs Loss: 4.695238\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4631.598313 \tValidation Loss: 1395.316283\n","  Epoch: 100 \tTraining Loss: 4217.335810 \tValidation Loss: 1274.417791\n","  Epoch: 150 \tTraining Loss: 3857.104494 \tValidation Loss: 1247.218490\n","  Epoch: 200 \tTraining Loss: 3819.400339 \tValidation Loss: 1202.937656\n","  Epoch: 250 \tTraining Loss: 3812.296748 \tValidation Loss: 1165.450020\n","  Epoch: 300 \tTraining Loss: 3803.603077 \tValidation Loss: 1164.231118\n","Best Validation Loss: 767.117597 in epoch 126\n","Best Train Loss: 3467.827439 in epoch 294\n","  Test Loss: 9.193828\n","\n","  Mean Abs Loss: 5.114286\n","\n","  Test Loss: 9.477807\n","\n","  Mean Abs Loss: 5.476190\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3697.569969 \tValidation Loss: 1112.717707\n","  Epoch: 100 \tTraining Loss: 3575.301688 \tValidation Loss: 1111.836682\n","  Epoch: 150 \tTraining Loss: 3454.472419 \tValidation Loss: 1099.982243\n","  Epoch: 200 \tTraining Loss: 3318.803318 \tValidation Loss: 1125.574276\n","  Epoch: 250 \tTraining Loss: 3344.267192 \tValidation Loss: 1082.506485\n","  Epoch: 300 \tTraining Loss: 3315.178285 \tValidation Loss: 1081.758577\n","Best Validation Loss: 748.212482 in epoch 265\n","Best Train Loss: 2513.974177 in epoch 274\n","  Test Loss: 8.757248\n","\n","  Mean Abs Loss: 4.695238\n","\n","  Test Loss: 8.777205\n","\n","  Mean Abs Loss: 4.580952\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4535.394873 \tValidation Loss: 1387.210235\n","  Epoch: 100 \tTraining Loss: 4144.734646 \tValidation Loss: 1310.350210\n","  Epoch: 150 \tTraining Loss: 3972.352328 \tValidation Loss: 1206.961141\n","  Epoch: 200 \tTraining Loss: 3763.746342 \tValidation Loss: 1171.479088\n","  Epoch: 250 \tTraining Loss: 3737.530979 \tValidation Loss: 1164.522208\n","  Epoch: 300 \tTraining Loss: 3663.245118 \tValidation Loss: 1156.276195\n","Best Validation Loss: 830.950497 in epoch 211\n","Best Train Loss: 3284.013734 in epoch 101\n","  Test Loss: 9.043825\n","\n","  Mean Abs Loss: 5.209524\n","\n","  Test Loss: 8.768735\n","\n","  Mean Abs Loss: 5.266667\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3584.456966 \tValidation Loss: 1121.464098\n","  Epoch: 100 \tTraining Loss: 3613.531816 \tValidation Loss: 1115.808994\n","  Epoch: 150 \tTraining Loss: 3603.198950 \tValidation Loss: 1110.872502\n","  Epoch: 200 \tTraining Loss: 3321.479889 \tValidation Loss: 1100.126974\n","  Epoch: 250 \tTraining Loss: 3367.233365 \tValidation Loss: 1081.030281\n","  Epoch: 300 \tTraining Loss: 3388.831865 \tValidation Loss: 1073.020619\n","Best Validation Loss: 740.969642 in epoch 243\n","Best Train Loss: 2701.798316 in epoch 130\n","  Test Loss: 8.440333\n","\n","  Mean Abs Loss: 4.876190\n","\n","  Test Loss: 8.443645\n","\n","  Mean Abs Loss: 4.780952\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 15\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4607.020501 \tValidation Loss: 1381.915841\n","  Epoch: 100 \tTraining Loss: 4211.755083 \tValidation Loss: 1265.878977\n","  Epoch: 150 \tTraining Loss: 4137.261089 \tValidation Loss: 1176.319859\n","  Epoch: 200 \tTraining Loss: 3852.193404 \tValidation Loss: 1155.515803\n","  Epoch: 250 \tTraining Loss: 3785.626814 \tValidation Loss: 1149.073119\n","  Epoch: 300 \tTraining Loss: 3729.934060 \tValidation Loss: 1121.131960\n","Best Validation Loss: 791.978861 in epoch 205\n","Best Train Loss: 2920.893608 in epoch 224\n","  Test Loss: 9.010896\n","\n","  Mean Abs Loss: 5.057143\n","\n","  Test Loss: 9.118394\n","\n","  Mean Abs Loss: 5.142857\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3761.710452 \tValidation Loss: 1124.159909\n","  Epoch: 100 \tTraining Loss: 3785.798607 \tValidation Loss: 1113.026492\n","  Epoch: 150 \tTraining Loss: 3766.404775 \tValidation Loss: 989.879368\n","  Epoch: 200 \tTraining Loss: 3599.420440 \tValidation Loss: 1097.966194\n","  Epoch: 250 \tTraining Loss: 3523.353108 \tValidation Loss: 1057.459064\n","  Epoch: 300 \tTraining Loss: 3339.189611 \tValidation Loss: 1071.227366\n","Best Validation Loss: 744.005714 in epoch 287\n","Best Train Loss: 2475.849965 in epoch 233\n","  Test Loss: 8.625847\n","\n","  Mean Abs Loss: 4.580952\n","\n","  Test Loss: 8.508357\n","\n","  Mean Abs Loss: 4.523810\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4550.245686 \tValidation Loss: 1362.213860\n","  Epoch: 100 \tTraining Loss: 4003.109985 \tValidation Loss: 876.611776\n","  Epoch: 150 \tTraining Loss: 3726.747262 \tValidation Loss: 1157.133160\n","  Epoch: 200 \tTraining Loss: 3732.590479 \tValidation Loss: 1138.847619\n","  Epoch: 250 \tTraining Loss: 3795.811510 \tValidation Loss: 1137.861546\n","  Epoch: 300 \tTraining Loss: 3596.497660 \tValidation Loss: 1143.857801\n","Best Validation Loss: 499.461750 in epoch 216\n","Best Train Loss: 2942.085438 in epoch 170\n","  Test Loss: 8.779945\n","\n","  Mean Abs Loss: 5.076190\n","\n","  Test Loss: 8.788156\n","\n","  Mean Abs Loss: 5.057143\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3717.935211 \tValidation Loss: 1137.894096\n","  Epoch: 100 \tTraining Loss: 3633.205958 \tValidation Loss: 1109.309592\n","  Epoch: 150 \tTraining Loss: 3538.599041 \tValidation Loss: 1071.200445\n","  Epoch: 200 \tTraining Loss: 3370.283112 \tValidation Loss: 1082.667603\n","  Epoch: 250 \tTraining Loss: 3288.667548 \tValidation Loss: 1098.065974\n","  Epoch: 300 \tTraining Loss: 3177.391080 \tValidation Loss: 803.717107\n","Best Validation Loss: 469.619735 in epoch 116\n","Best Train Loss: 2808.819667 in epoch 107\n","  Test Loss: 8.695440\n","\n","  Mean Abs Loss: 4.742857\n","\n","  Test Loss: 8.631105\n","\n","  Mean Abs Loss: 4.685714\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4637.837432 \tValidation Loss: 1350.651610\n","  Epoch: 100 \tTraining Loss: 4234.661435 \tValidation Loss: 853.250922\n","  Epoch: 150 \tTraining Loss: 3985.495378 \tValidation Loss: 1226.861782\n","  Epoch: 200 \tTraining Loss: 3909.809958 \tValidation Loss: 1219.537196\n","  Epoch: 250 \tTraining Loss: 3893.757227 \tValidation Loss: 1209.910499\n","  Epoch: 300 \tTraining Loss: 3957.112913 \tValidation Loss: 1159.607377\n","Best Validation Loss: 657.310803 in epoch 285\n","Best Train Loss: 3048.766388 in epoch 272\n","  Test Loss: 9.343631\n","\n","  Mean Abs Loss: 5.447619\n","\n","  Test Loss: 9.432030\n","\n","  Mean Abs Loss: 5.466667\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3761.176046 \tValidation Loss: 1154.334611\n","  Epoch: 100 \tTraining Loss: 3600.463471 \tValidation Loss: 1102.114253\n","  Epoch: 150 \tTraining Loss: 3484.469448 \tValidation Loss: 1111.558079\n","  Epoch: 200 \tTraining Loss: 3474.696978 \tValidation Loss: 1101.409249\n","  Epoch: 250 \tTraining Loss: 3391.305551 \tValidation Loss: 1105.379461\n","  Epoch: 300 \tTraining Loss: 3348.578325 \tValidation Loss: 1113.657146\n","Best Validation Loss: 764.722515 in epoch 126\n","Best Train Loss: 2682.824406 in epoch 199\n","  Test Loss: 8.715487\n","\n","  Mean Abs Loss: 4.619048\n","\n","  Test Loss: 8.645431\n","\n","  Mean Abs Loss: 4.647619\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 10\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4409.517146 \tValidation Loss: 1269.119775\n","  Epoch: 100 \tTraining Loss: 3742.524556 \tValidation Loss: 1156.811568\n","  Epoch: 150 \tTraining Loss: 3741.707868 \tValidation Loss: 815.094626\n","  Epoch: 200 \tTraining Loss: 3842.365353 \tValidation Loss: 1143.182103\n","  Epoch: 250 \tTraining Loss: 3702.098883 \tValidation Loss: 1137.404459\n","  Epoch: 300 \tTraining Loss: 3692.807195 \tValidation Loss: 1145.056030\n","Best Validation Loss: 502.586255 in epoch 93\n","Best Train Loss: 2982.120657 in epoch 220\n","  Test Loss: 8.897571\n","\n","  Mean Abs Loss: 5.028571\n","\n","  Test Loss: 9.260210\n","\n","  Mean Abs Loss: 5.161905\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3741.909145 \tValidation Loss: 1140.731176\n","  Epoch: 100 \tTraining Loss: 3563.603366 \tValidation Loss: 1069.885373\n","  Epoch: 150 \tTraining Loss: 3378.479799 \tValidation Loss: 1102.157253\n","  Epoch: 200 \tTraining Loss: 3282.914323 \tValidation Loss: 1094.104448\n","  Epoch: 250 \tTraining Loss: 3307.925131 \tValidation Loss: 1022.942116\n","  Epoch: 300 \tTraining Loss: 3144.613330 \tValidation Loss: 1089.556595\n","Best Validation Loss: 636.259229 in epoch 25\n","Best Train Loss: 2498.743334 in epoch 286\n","  Test Loss: 9.035194\n","\n","  Mean Abs Loss: 4.952381\n","\n","  Test Loss: 8.916925\n","\n","  Mean Abs Loss: 4.923810\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4315.693803 \tValidation Loss: 1310.910030\n","  Epoch: 100 \tTraining Loss: 4100.538470 \tValidation Loss: 1244.090300\n","  Epoch: 150 \tTraining Loss: 4033.260168 \tValidation Loss: 1220.347366\n","  Epoch: 200 \tTraining Loss: 3911.400965 \tValidation Loss: 813.676031\n","  Epoch: 250 \tTraining Loss: 3855.168374 \tValidation Loss: 1150.937556\n","  Epoch: 300 \tTraining Loss: 3709.360381 \tValidation Loss: 1124.143363\n","Best Validation Loss: 689.092449 in epoch 132\n","Best Train Loss: 2957.004534 in epoch 258\n","  Test Loss: 9.337201\n","\n","  Mean Abs Loss: 5.447619\n","\n","  Test Loss: 9.593534\n","\n","  Mean Abs Loss: 5.542857\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3645.425571 \tValidation Loss: 1120.486720\n","  Epoch: 100 \tTraining Loss: 3623.732158 \tValidation Loss: 1111.491389\n","  Epoch: 150 \tTraining Loss: 3489.817197 \tValidation Loss: 1111.848852\n","  Epoch: 200 \tTraining Loss: 3418.145270 \tValidation Loss: 745.563530\n","  Epoch: 250 \tTraining Loss: 3334.170554 \tValidation Loss: 1086.595996\n","  Epoch: 300 \tTraining Loss: 3181.305485 \tValidation Loss: 1085.231494\n","Best Validation Loss: 745.563530 in epoch 200\n","Best Train Loss: 2446.043423 in epoch 195\n","  Test Loss: 8.549851\n","\n","  Mean Abs Loss: 4.600000\n","\n","  Test Loss: 8.655452\n","\n","  Mean Abs Loss: 4.590476\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 3744.234207 \tValidation Loss: 1407.986325\n","  Epoch: 100 \tTraining Loss: 4119.402022 \tValidation Loss: 1272.737722\n","  Epoch: 150 \tTraining Loss: 3866.769327 \tValidation Loss: 821.707886\n","  Epoch: 200 \tTraining Loss: 3877.154218 \tValidation Loss: 1227.561824\n","  Epoch: 250 \tTraining Loss: 3833.665953 \tValidation Loss: 1213.250879\n","  Epoch: 300 \tTraining Loss: 3698.814854 \tValidation Loss: 1198.453631\n","Best Validation Loss: 814.839965 in epoch 130\n","Best Train Loss: 3191.930251 in epoch 119\n","  Test Loss: 9.199186\n","\n","  Mean Abs Loss: 5.466667\n","\n","  Test Loss: 8.958120\n","\n","  Mean Abs Loss: 5.228571\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3640.895394 \tValidation Loss: 1141.713101\n","  Epoch: 100 \tTraining Loss: 3628.715297 \tValidation Loss: 1128.089976\n","  Epoch: 150 \tTraining Loss: 3577.443168 \tValidation Loss: 1102.694472\n","  Epoch: 200 \tTraining Loss: 3376.030505 \tValidation Loss: 1093.492054\n","  Epoch: 250 \tTraining Loss: 3370.422614 \tValidation Loss: 1095.593094\n","  Epoch: 300 \tTraining Loss: 3261.131640 \tValidation Loss: 490.279386\n","Best Validation Loss: 490.279386 in epoch 300\n","Best Train Loss: 2641.915372 in epoch 221\n","  Test Loss: 8.737642\n","\n","  Mean Abs Loss: 4.790476\n","\n","  Test Loss: 8.770418\n","\n","  Mean Abs Loss: 4.733333\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 15\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4403.915905 \tValidation Loss: 1344.815664\n","  Epoch: 100 \tTraining Loss: 3981.518460 \tValidation Loss: 1172.742636\n","  Epoch: 150 \tTraining Loss: 4047.309174 \tValidation Loss: 1162.235291\n","  Epoch: 200 \tTraining Loss: 3788.868539 \tValidation Loss: 1145.681170\n","  Epoch: 250 \tTraining Loss: 3723.711086 \tValidation Loss: 1135.925539\n","  Epoch: 300 \tTraining Loss: 3675.584899 \tValidation Loss: 1137.775358\n","Best Validation Loss: 776.631121 in epoch 197\n","Best Train Loss: 2964.807327 in epoch 179\n","  Test Loss: 8.827304\n","\n","  Mean Abs Loss: 5.038095\n","\n","  Test Loss: 8.858292\n","\n","  Mean Abs Loss: 5.123810\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3830.833421 \tValidation Loss: 1131.729763\n","  Epoch: 100 \tTraining Loss: 3593.528503 \tValidation Loss: 1090.362899\n","  Epoch: 150 \tTraining Loss: 3400.641956 \tValidation Loss: 1140.113116\n","  Epoch: 200 \tTraining Loss: 3362.763841 \tValidation Loss: 1075.030200\n","  Epoch: 250 \tTraining Loss: 3269.173239 \tValidation Loss: 1087.856633\n","  Epoch: 300 \tTraining Loss: 3229.643845 \tValidation Loss: 1117.904882\n","Best Validation Loss: 764.889041 in epoch 258\n","Best Train Loss: 3040.428568 in epoch 227\n","  Test Loss: 8.804296\n","\n","  Mean Abs Loss: 4.780952\n","\n","  Test Loss: 8.838383\n","\n","  Mean Abs Loss: 4.704762\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 31\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4537.696069 \tValidation Loss: 1373.608215\n","  Epoch: 100 \tTraining Loss: 4007.606142 \tValidation Loss: 839.250689\n","  Epoch: 150 \tTraining Loss: 4077.892904 \tValidation Loss: 1092.429233\n","  Epoch: 200 \tTraining Loss: 3925.960168 \tValidation Loss: 1110.409624\n","  Epoch: 250 \tTraining Loss: 3823.384366 \tValidation Loss: 1107.948282\n","  Epoch: 300 \tTraining Loss: 3774.526025 \tValidation Loss: 1111.756230\n","Best Validation Loss: 642.536467 in epoch 128\n","Best Train Loss: 3069.681181 in epoch 146\n","  Test Loss: 9.586368\n","\n","  Mean Abs Loss: 5.304762\n","\n","  Test Loss: 9.559686\n","\n","  Mean Abs Loss: 5.314286\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3732.441317 \tValidation Loss: 1138.729046\n","  Epoch: 100 \tTraining Loss: 2786.748380 \tValidation Loss: 828.514151\n","  Epoch: 150 \tTraining Loss: 3424.191060 \tValidation Loss: 1091.008222\n","  Epoch: 200 \tTraining Loss: 3456.668581 \tValidation Loss: 1109.459395\n","  Epoch: 250 \tTraining Loss: 3382.961653 \tValidation Loss: 1073.094854\n","  Epoch: 300 \tTraining Loss: 3283.160546 \tValidation Loss: 1102.197413\n","Best Validation Loss: 685.520263 in epoch 29\n","Best Train Loss: 2466.223820 in epoch 276\n","  Test Loss: 8.923182\n","\n","  Mean Abs Loss: 4.838095\n","\n","  Test Loss: 8.849837\n","\n","  Mean Abs Loss: 5.047619\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 21\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4538.266844 \tValidation Loss: 1358.899527\n","  Epoch: 100 \tTraining Loss: 4008.122903 \tValidation Loss: 1160.081014\n","  Epoch: 150 \tTraining Loss: 3802.020831 \tValidation Loss: 1119.758641\n","  Epoch: 200 \tTraining Loss: 3753.675878 \tValidation Loss: 1146.414409\n","  Epoch: 250 \tTraining Loss: 3690.302928 \tValidation Loss: 1143.696435\n","  Epoch: 300 \tTraining Loss: 3714.927808 \tValidation Loss: 1121.007816\n","Best Validation Loss: 812.143982 in epoch 279\n","Best Train Loss: 3186.933349 in epoch 91\n","  Test Loss: 9.538132\n","\n","  Mean Abs Loss: 5.323810\n","\n","  Test Loss: 8.833348\n","\n","  Mean Abs Loss: 5.047619\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.001\n","  Epoch: 50 \tTraining Loss: 3643.560014 \tValidation Loss: 1113.614555\n","  Epoch: 100 \tTraining Loss: 3543.445878 \tValidation Loss: 1095.768722\n","  Epoch: 150 \tTraining Loss: 3312.135471 \tValidation Loss: 1111.695712\n","  Epoch: 200 \tTraining Loss: 3300.586985 \tValidation Loss: 1090.914600\n","  Epoch: 250 \tTraining Loss: 3221.200040 \tValidation Loss: 1116.554205\n","  Epoch: 300 \tTraining Loss: 3218.752230 \tValidation Loss: 1110.291331\n","Best Validation Loss: 760.604710 in epoch 97\n","Best Train Loss: 2650.494733 in epoch 151\n","  Test Loss: 8.843877\n","\n","  Mean Abs Loss: 4.838095\n","\n","  Test Loss: 8.596474\n","\n","  Mean Abs Loss: 4.619048\n","\n","\n","Combined Dimension: 30\n","LSTM Output Dimension: 15\n","Categorical Dimension: 20\n","Numerical Dimension: 20\n","LSTM Dimension: 16\n","Learning Rate: 0.0001\n","  Epoch: 50 \tTraining Loss: 4584.753074 \tValidation Loss: 1050.439186\n","  Epoch: 100 \tTraining Loss: 4053.221585 \tValidation Loss: 1216.526955\n","  Epoch: 150 \tTraining Loss: 3985.677422 \tValidation Loss: 1128.914566\n","  Epoch: 200 \tTraining Loss: 3836.712997 \tValidation Loss: 1140.273632\n","  Epoch: 250 \tTraining Loss: 3770.077841 \tValidation Loss: 1129.923493\n","  Epoch: 300 \tTraining Loss: 3845.598386 \tValidation Loss: 1127.885200\n","Best Validation Loss: 776.719073 in epoch 281\n","Best Train Loss: 3255.388706 in epoch 112\n","  Test Loss: 9.620407\n","\n","  Mean Abs Loss: 5.466667\n","\n","  Test Loss: 9.264033\n","\n","  Mean Abs Loss: 5.200000\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Liic3BnEcpyt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"7071bf53-f153-4128-f1d1-1190e4d248fc","executionInfo":{"status":"ok","timestamp":1587004697353,"user_tz":300,"elapsed":1873,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}}},"source":["print(overall_test_mean_error)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["4.3619047619047615\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"b0lbRMJT2dxM","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X3b4qDXF2lgK","colab_type":"text"},"source":["Combined Dimension: 30\n","\n","LSTM Output Dimension: 10\n","\n","Categorical Dimension: 10\n","\n","Numerical Dimension: 15\n","\n","LSTM Dimension: 21\n","\n","Learning Rate: 0.001"]},{"cell_type":"code","metadata":{"id":"WCswgkQY2mMU","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}
