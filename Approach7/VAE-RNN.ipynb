{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"VAE-RNN.ipynb","provenance":[],"authorship_tag":"ABX9TyPhjhUHDi2c3031XQV7qLx0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"FUEm9QBvYyXr","colab_type":"code","colab":{}},"source":["import argparse\n","import yaml\n","import time\n","import datetime\n","import cv2\n","import numpy as np\n","import pandas as pd\n","import random\n","\n","from skimage import io, transform\n","import matplotlib.pyplot as plt\n","from scipy.ndimage import zoom\n","from scipy import ndimage, misc\n","\n","import torch\n","import torch.utils.data\n","from torch import nn, optim\n","from torch.nn import functional as F\n","from torchvision import datasets, transforms, utils\n","from torchvision.utils import save_image\n","from torch.utils.data import Dataset, DataLoader, TensorDataset\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import torchvision\n","import torch\n","from torch.autograd import Variable\n","import torch.optim as optim\n","\n","from google.colab import drive\n","import os\n","\n","import warnings\n","from collections import defaultdict\n","\n","from scipy import stats"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vkVVfsA_Y9x5","colab_type":"code","outputId":"a18a2c5b-4587-4157-ab95-b0adcc863ed6","executionInfo":{"status":"ok","timestamp":1586719087615,"user_tz":300,"elapsed":42845,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":128}},"source":["drive.mount('/content/drive', force_remount = True)\n","os.chdir('/content/drive/My Drive/Mosquito-Tec/DA-RNN')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mAFzqb7tZAwM","colab_type":"code","colab":{}},"source":["# Open Data\n","\n","train_csv_file = '/content/drive/My Drive/Colab/mosquito/Final_Mosquito_train6_W.csv'\n","train_frame = pd.read_csv(train_csv_file)\n","\n","train_frame[\"TRAPSET\"] = pd.to_datetime(train_frame[\"TRAPSET\"])\n","train_frame[\"TRAPCOLLECT\"] = pd.to_datetime(train_frame[\"TRAPCOLLECT\"])\n","\n","train_frame[\"TRAPDAYS\"] = (train_frame[\"TRAPCOLLECT\"] - train_frame[\"TRAPSET\"]).dt.days\n","\n","train_frame[\"TRAPSET\"] = train_frame[\"TRAPSET\"].dt.week\n","train_frame[\"TRAPCOLLECT\"] = train_frame[\"TRAPCOLLECT\"].dt.week\n","\n","# Test data\n","\n","test_csv_file = '/content/drive/My Drive/Colab/mosquito/Final_Mosquito_test6_W.csv'\n","test_frame = pd.read_csv(test_csv_file)\n","\n","test_frame[\"TRAPSET\"] = pd.to_datetime(test_frame[\"TRAPSET\"])\n","test_frame[\"TRAPCOLLECT\"] = pd.to_datetime(test_frame[\"TRAPCOLLECT\"])\n","\n","test_frame[\"TRAPDAYS\"] = (test_frame[\"TRAPCOLLECT\"] - test_frame[\"TRAPSET\"]).dt.days\n","\n","test_frame[\"TRAPSET\"] = test_frame[\"TRAPSET\"].dt.week\n","test_frame[\"TRAPCOLLECT\"] = test_frame[\"TRAPCOLLECT\"].dt.week"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W8m5PuFCRrfe","colab_type":"code","colab":{}},"source":["non_temporal_columns = [\"OBJECTID\", \"X\", \"Y\", \"TRAPTYPE\", \"ATTRACTANTUSED\",\n","                        \"TRAPID\", \"LATITUDE\", \"LONGITUDE\", \"ADDRESS\", \"TOWN\",\n","                        \"STATE\", \"COUNTY\", \"TRAPSITE\", \"TRAPSET\", \"SETTIMEOFDAY\",\n","                        \"YEAR\", \"TRAPCOLLECT\", \"COLLECTTIMEOFDAY\", \"GENUS\",\n","                        \"SPECIES\", \"LIFESTAGE\", \"EGGSCOLLECTED\", \"LARVAECOLLECTED\",\n","                        \"PUPAECOLLECTED\", \"REPORTDATE\", \"TRAPDAYS\"]\n","\n","non_num_columns = [\"LATITUDE\", \"LONGITUDE\", \"TRAPDAYS\"]\n","\n","categorical_columnsT = [\"TRAPTYPE\", \"ATTRACTANTSUSED\", \"TRAPID\", \"ADDRESS\", \"TOWN\",\n","                        \"STATE\", \"COUNTY\", \"TRAPSITE\", \"TRAPSET\", \"SETTIMEOFDAY\",\n","                        \"TRAPCOLLECT\", \"COLLECTTIMEOFDAY\", \"GENUS\",\n","                        \"SPECIES\", \"LIFESTAGE\", \"EGGSCOLLECTED\", \"LARVAECOLLECTED\",\n","                        \"PUPAECOLLECTED\", \"REPORTDATE\"]\n","\n","categorical_columns = [\"TRAPTYPE\", \"ATTRACTANTSUSED\", \"SETTIMEOFDAY\"]\n","\n","temporal_columns = [\"sunriseTime\", \"sunsetTime\", \"moonPhase\", \"precipIntensity\",\n","                    \"precipIntensityMax\", \"precipProbability\", \"temperatureHigh\",\n","                    \"temperatureHighTime\", \"temperatureLow\", \"temperatureLowTime\",\n","                    \"apparentTemperatureHigh\", \"apparentTemperatureHighTime\",\n","                    \"apparentTemperatureLow\", \"apparentTemperatureLowTime\",\n","                    \"dewPoint\", \"humidity\", \"pressure\", \"windSpeed\", \"windGust\",\n","                    \"windGustTime\", \"windBearing\", \"cloudCover\", \"uvIndex\", \n","                    \"uvIndexTime\", \"visibility\", \"temperatureMin\", \"temperatureMinTime\",\n","                    \"temperatureMax\", \"temperatureMaxTime\", \"apparentTemperatureMin\",\n","                    \"apparentTemperatureMinTime\", \"apparentTemperatureMax\", \"apparentTemperatureMaxTime\",\n","                    \"icon\", \"time\", \"precipIntensityMaxTime\", \"precipType\", \"summary\"] \n","\n","numerical_columnT = [\"sunriseTime\", \"sunsetTime\", \"moonPhase\", \"precipIntensity\",\n","                    \"precipIntensityMax\", \"precipProbability\", \"temperatureHigh\",\n","                    \"temperatureHighTime\", \"temperatureLow\", \"temperatureLowTime\",\n","                    \"apparentTemperatureHigh\", \"apparentTemperatureHighTime\",\n","                    \"apparentTemperatureLow\", \"apparentTemperatureLowTime\",\n","                    \"dewPoint\", \"humidity\", \"pressure\", \"windSpeed\", \"windGust\",\n","                    \"windGustTime\", \"windBearing\", \"cloudCover\", \"uvIndex\", \n","                    \"uvIndexTime\", \"visibility\", \"temperatureMin\", \"temperatureMinTime\",\n","                    \"temperatureMax\", \"temperatureMaxTime\", \"apparentTemperatureMin\",\n","                    \"apparentTemperatureMinTime\", \"apparentTemperatureMax\", \"apparentTemperatureMaxTime\",\n","                    \"time\", \"precipIntensityMaxTime\"] \n","\n","numerical_columns = [\"precipIntensity\", \"temperatureHigh\", \"temperatureLow\",\n","                     \"humidity\", \"uvIndex\", \"temperatureMax\", \"pressure\"]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eKTqERJZEAvP","colab_type":"code","colab":{}},"source":["SEQ_LENGTH = 14\n","NUM_ENTRIES = train_frame.shape[0]\n","\n","column_means = {}\n","column_maxs = {}\n","column_mins = {}\n","\n","for col in numerical_columns:\n","    column_means[col] = 0\n","    column_maxs[col] = 0\n","    column_mins[col] = np.inf\n","\n","    for i in range(1, SEQ_LENGTH + 1):\n","        train_frame[col + str(i)].replace(to_replace=r'No*', value=np.nan, regex=True, inplace=True)\n","        test_frame[col + str(i)].replace(to_replace=r'No*', value=np.nan, regex=True, inplace=True)\n","\n","        if(train_frame[col + str(i)].dtype == np.dtype(object)):\n","            train_frame[col + str(i)] = train_frame[col + str(i)].astype(\"float64\", copy=\"False\")\n","\n","        if(test_frame[col + str(i)].dtype == np.dtype(object)):\n","            test_frame[col + str(i)] = test_frame[col + str(i)].astype(\"float64\", copy=\"False\")\n","\n","        cur_col = train_frame[col + str(i)]\n","\n","        col_mean = cur_col.mean()\n","\n","        train_frame[col + str(i)].replace(to_replace=np.nan, value=col_mean, inplace=True)\n","        test_frame[col + str(i)].replace(to_replace=np.nan, value=col_mean, inplace=True)\n","\n","        column_means[col] += col_mean * NUM_ENTRIES\n","        if cur_col.max() > column_maxs[col]:\n","            column_maxs[col] = cur_col.max()\n","\n","        if cur_col.min() < column_mins[col]:\n","            column_mins[col] = cur_col.min()\n","\n","    column_means[col] /= SEQ_LENGTH * NUM_ENTRIES\n","\n","    for i in range(1, SEQ_LENGTH + 1):\n","        test_frame[col + str(i)] = (test_frame[col + str(i)] - column_means[col]) / (column_maxs[col] - column_mins[col])\n","        train_frame[col + str(i)] = (train_frame[col + str(i)] - column_means[col]) / (column_maxs[col] - column_mins[col])\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aoXrda1jLe0z","colab_type":"code","colab":{}},"source":["# Standardize Numerical Columns (not Temporal)\n","\n","num_means = {}\n","num_maxs = {}\n","num_mins = {}\n","\n","for col in non_num_columns:\n","    cur_col = train_frame[col]\n","    num_means[col] = cur_col.mean()\n","    num_maxs[col] = cur_col.max()\n","    num_mins[col] = cur_col.min()\n","\n","    train_frame[col] = (cur_col - num_means[col]) / (num_maxs[col] - num_mins[col])\n","    test_frame[col] = (test_frame[col] - num_means[col]) / (num_maxs[col] - num_mins[col])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fT25ApU8vdWW","colab_type":"code","outputId":"549bd78a-c153-4939-9ab2-a61b267368ab","executionInfo":{"status":"ok","timestamp":1586725632966,"user_tz":300,"elapsed":1356,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Convert categories\n","ats_train = []\n","ats_test = []\n","\n","for category in categorical_columns:\n","    train_frame[category] = train_frame[category].astype('category')\n","    test_frame[category] = test_frame[category].astype('category')\n","    ats_train.append(train_frame[category].cat.codes.values)\n","    ats_test.append(test_frame[category].cat.codes.values)\n","\n","train_cat = np.stack(ats_train, 1)\n","test_cat = np.stack(ats_test, 1)\n","\n","categorical_column_sizes = [len(train_frame[column].cat.categories) for column in categorical_columns]\n","embedding_sizes = [(col_size, min(50, (col_size + 1) // 2)) for col_size in categorical_column_sizes]\n","\n","print(embedding_sizes)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[(3, 2), (4, 2), (2, 1)]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KOy39SYAGsgy","colab_type":"code","colab":{}},"source":["# Organize info by batches/sequences\n","# Shape [784, 14, 35] -> Train\n","\n","#train_features = np.zeros(shape=(train_frame.shape[0], SEQ_LENGTH, len(numerical_columns)+len(non_num_columns)))\n","#test_features = np.zeros(shape=(test_frame.shape[0], SEQ_LENGTH, len(numerical_columns)+len(non_num_columns)))\n","\n","train_features = np.zeros(shape=(train_frame.shape[0], SEQ_LENGTH, len(numerical_columns)))\n","test_features = np.zeros(shape=(test_frame.shape[0], SEQ_LENGTH, len(numerical_columns)))\n","\n","for i in range(1, SEQ_LENGTH + 1):\n","    for row, col in enumerate(numerical_columns):\n","        train_features[:, i-1, row] = train_frame[col + str(i)]\n","        test_features[:, i-1, row] = test_frame[col + str(i)]\n","\n","    '''    \n","    for row, col in enumerate(non_num_columns):\n","        train_features[:, i-1, row+len(numerical_columns)] = train_frame[col]\n","        test_features[:, i-1, row+len(numerical_columns)] = test_frame[col]\n","    '''"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J0aIP-SKMQ9R","colab_type":"code","colab":{}},"source":["# Organize info that is not temporal\n","train_y = train_frame[\"TOTAL\"].to_numpy()\n","test_y = test_frame[\"TOTAL\"].to_numpy()\n","\n","train_X = np.zeros(shape=(train_frame.shape[0], len(non_num_columns)))\n","test_X = np.zeros(shape=(test_frame.shape[0], len(non_num_columns)))\n","\n","for row, col in enumerate(non_num_columns):\n","    train_X[:, row] = train_frame[col]\n","    test_X[:, row] = test_frame[col]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4SLM3B2ZtcU4","colab_type":"text"},"source":["### Danger! Cuau Validation: DNR"]},{"cell_type":"code","metadata":{"id":"ICC4CaoOy4eh","colab_type":"code","outputId":"5925f5c0-ed0a-4339-e2b6-a0be87a5eafd","executionInfo":{"status":"ok","timestamp":1586725634214,"user_tz":300,"elapsed":289,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Create DataLoaders\n","# number of subprocesses to use for data loading\n","num_workers = 0\n","# how many samples per batch to load\n","batch_size = 16\n","# percentage of training set to use as validation\n","valid_size = 0.2\n","\n","\n","# obtain training indices that will be used for validation\n","num_train = len(train_X)\n","indices = list(range(num_train))\n","np.random.shuffle(indices)\n","split = int(np.floor(valid_size * num_train))\n","train_idx, valid_idx = indices[split:], indices[:split]\n","\n","print(len(train_idx), len(valid_idx))\n","\n","# define samplers for obtaining training and validation batches\n","train_sampler = SubsetRandomSampler(train_idx)\n","valid_sampler = SubsetRandomSampler(valid_idx)\n","\n","train_data = TensorDataset(torch.from_numpy(train_features), \n","                           torch.from_numpy(train_X), \n","                           torch.from_numpy(train_cat),\n","                           torch.from_numpy(train_y))\n","\n","test_data = TensorDataset(torch.from_numpy(test_features), \n","                          torch.from_numpy(test_X), \n","                          torch.from_numpy(test_cat),\n","                          torch.from_numpy(test_y))\n","\n","\n","# prepare data loaders (combine dataset and sampler)\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size,\n","    sampler = train_sampler, num_workers = num_workers, drop_last=True)\n","valid_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, \n","    sampler = valid_sampler, num_workers = num_workers, drop_last=True)\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size, \n","    num_workers = num_workers, shuffle = False, drop_last=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["596 149\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"i1VwYmtPtgGO","colab_type":"code","outputId":"3eec1e04-c109-4844-fd39-5555de106532","executionInfo":{"status":"ok","timestamp":1586719432626,"user_tz":300,"elapsed":1742,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["# Create DataLoaders\n","# number of subprocesses to use for data loading\n","num_workers = 0\n","# how many samples per batch to load\n","batch_size = 4\n","# percentage of training set to use as validation\n","valid_size = 0.15\n","\n","\n","# obtain training indices that will be used for validation\n","num_train = len(train_X)\n","indices = list(range(num_train))\n","np.random.shuffle(indices)\n","split = int(np.floor(valid_size * num_train))\n","train_idx, valid_idx = indices[split:], indices[:split]\n","\n","print(len(train_idx), len(valid_idx))\n","\n","# define samplers for obtaining training and validation batches\n","train_sampler = SubsetRandomSampler(train_idx)\n","valid_sampler = SubsetRandomSampler(valid_idx)\n","\n","train_data = TensorDataset(torch.from_numpy(train_features[train_idx]), \n","                           torch.from_numpy(train_X[train_idx]), \n","                           torch.from_numpy(train_cat[train_idx]),\n","                           torch.from_numpy(train_y[train_idx]))\n","\n","valid_data = TensorDataset(torch.from_numpy(train_features[valid_idx]), \n","                           torch.from_numpy(train_X[valid_idx]), \n","                           torch.from_numpy(train_cat[valid_idx]),\n","                           torch.from_numpy(train_y[valid_idx]))\n","\n","test_data = TensorDataset(torch.from_numpy(test_features), \n","                          torch.from_numpy(test_X), \n","                          torch.from_numpy(test_cat),\n","                          torch.from_numpy(test_y))\n","\n","print(len(train_data), len(valid_data))\n","\n","# prepare data loaders (combine dataset and sampler)\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size,\n","    num_workers = num_workers, shuffle = True, drop_last=True)\n","valid_loader = torch.utils.data.DataLoader(valid_data, batch_size = batch_size, \n","    num_workers = num_workers, shuffle = True, drop_last=True)\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size, \n","    num_workers = num_workers, shuffle = False, drop_last=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["634 111\n","634 111\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JBf5h7sS3nVc","colab_type":"code","colab":{}},"source":["dataiter = iter(train_loader)\n","sample_features, sample_X, sample_cat, sample_y = dataiter.next()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M9CE7bQE36vD","colab_type":"code","outputId":"dec3ae0f-756e-4002-88e4-7e59d51abe49","executionInfo":{"status":"ok","timestamp":1586725270130,"user_tz":300,"elapsed":548,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":90}},"source":["print(sample_features.shape)\n","print(sample_X.shape)\n","print(sample_cat.shape)\n","print(sample_y.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["torch.Size([16, 14, 7])\n","torch.Size([16, 3])\n","torch.Size([16, 2])\n","torch.Size([16])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uwzut-CseGO3","colab_type":"code","outputId":"5e0ecab0-1101-4105-e414-da60db13d703","executionInfo":{"status":"ok","timestamp":1586725639079,"user_tz":300,"elapsed":890,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# First checking if GPU is available\n","train_on_gpu = torch.cuda.is_available()\n","\n","if(train_on_gpu):\n","    print('Training on GPU.')\n","else:\n","    print('No GPU available, training on CPU.')\n","\n","#train_on_gpu = False"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Training on GPU.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fKaB5Hx1377j","colab_type":"code","colab":{}},"source":["class MosquitoLSTM(nn.Module):\n","    def __init__(self, num_numerical_columns, hidden_dim, n_layers):\n","        super(MosquitoLSTM, self).__init__()\n","\n","        self.n_layers = n_layers\n","        self.hidden_dim = hidden_dim\n","\n","        '''\n","        embed_size = 0\n","        for embed in embedding_sizes:\n","            embed_size += embed[1]\n","\n","        self.all_embeddings = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in embedding_sizes])\n","        '''\n","\n","        # For LSTM\n","        self.lstm = nn.LSTM(num_numerical_columns, hidden_dim, n_layers,\n","                            dropout=0.3, batch_first=True)\n","        \n","        # dropout layer\n","        self.dropout = nn.Dropout(0.3)\n","\n","        self.dense = nn.Sequential(\n","            \n","            nn.Linear(hidden_dim, 1)\n","        )\n","        self.ReLU = nn.ReLU()\n","\n","    def forward(self, X_temp, hidden):\n","        lstm_out, hidden = self.lstm(X_temp, hidden)\n","    \n","        # stack up lstm outputs\n","        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n","        print(lstm_out.shape)\n","        # dropout and fully-connected layer\n","        out = self.dense(lstm_out)\n","        # sigmoid function\n","        out = self.ReLU(out)\n","        \n","        print(out.shape)\n","        # reshape to be batch_size first\n","        out = out.view(batch_size, -1)\n","        print(out.shape)\n","        out = out[:, -1]\n","        print(out.shape)\n","\n","\n","\n","        return out, hidden\n","\n","    def init_hidden(self, batch_size):\n","        ''' Initializes hidden state '''\n","        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n","        # initialized to zero, for hidden state and cell state of LSTM\n","        weight = next(self.parameters()).data\n","        \n","        if (train_on_gpu):\n","            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n","                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n","        else:\n","            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n","                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n","        \n","        return hidden\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MO7LlGaj7oIe","colab_type":"code","colab":{}},"source":["class MosquitoRedux(nn.Module):\n","    def __init__(self, \n","                 lstm_columns, lstm_hidden_dim, lstm_n_layers,\n","                 num_columns, num_hidden_dim,\n","                 embedding_sizes, cat_hidden_dim):\n","        super(MosquitoRedux, self).__init__()\n","\n","        self.lstm_n_layers = lstm_n_layers\n","        self.lstm_hidden_dim = lstm_hidden_dim\n","\n","        self.num_hidden_dim = num_hidden_dim\n","        self.cat_hidden_dim = cat_hidden_dim\n","\n","        embed_size = 0\n","        for embed in embedding_sizes:\n","            embed_size += embed[1]\n","\n","        self.all_embeddings = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in embedding_sizes])\n","\n","        # For LSTM\n","        self.lstm = nn.LSTM(lstm_columns, lstm_hidden_dim, lstm_n_layers,\n","                            dropout=0.3, batch_first=True)\n","        \n","        # LSTM dense layer\n","        self.lstm_dense = nn.Sequential(\n","            nn.BatchNorm1d(lstm_hidden_dim),\n","            nn.Dropout(0.3),\n","            nn.Linear(lstm_hidden_dim, 1),\n","            nn.LeakyReLU()\n","        )\n","        \n","        self.num_dense = nn.Sequential(\n","            nn.Dropout(0.3),\n","            nn.Linear(num_columns, num_hidden_dim),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm1d(num_hidden_dim),\n","            nn.Linear(num_hidden_dim, 1),\n","            nn.LeakyReLU()\n","        )\n","\n","        self.cat_dense = nn.Sequential(\n","            nn.Dropout(0.3),\n","            nn.Linear(embed_size, cat_hidden_dim),\n","            nn.LeakyReLU(),\n","            nn.BatchNorm1d(cat_hidden_dim),\n","            nn.Linear(cat_hidden_dim, 1),\n","            nn.LeakyReLU()\n","        )\n","\n","    def forward(self, X_temp, hidden, X_num, X_cat):\n","        # Forward lstm\n","        lstm_out, hidden = self.lstm(X_temp, hidden)\n","        # stack up lstm outputs\n","        lstm_out = lstm_out.contiguous().view(-1, self.lstm_hidden_dim)\n","        # dropout and fully-connected layer\n","        lstm_out = self.lstm_dense(lstm_out)\n","      \n","        # reshape to be batch_size first\n","        lstm_out = lstm_out.view(batch_size, -1)\n","        lstm_out = lstm_out[:, -1]\n","        lstm_out = lstm_out.view(batch_size, 1)\n","\n","        # Forward num\n","        num_out = self.num_dense(X_num)\n","\n","        # Forward cat\n","        embeddings = []\n","        for i, e in enumerate(self.all_embeddings):\n","            embed = e(X_cat[:, i])\n","            embeddings.append(embed)\n","\n","        cat_out = torch.cat(embeddings, 1)\n","        cat_out = self.cat_dense(cat_out)\n","\n","        return lstm_out, hidden, num_out, cat_out\n","\n","    def init_hidden(self, batch_size):\n","        ''' Initializes hidden state '''\n","        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n","        # initialized to zero, for hidden state and cell state of LSTM\n","        weight = next(self.parameters()).data\n","        \n","        if (train_on_gpu):\n","            hidden = (weight.new(self.lstm_n_layers, batch_size, self.lstm_hidden_dim).zero_().cuda(),\n","                  weight.new(self.lstm_n_layers, batch_size, self.lstm_hidden_dim).zero_().cuda())\n","        else:\n","            hidden = (weight.new(self.lstm_n_layers, batch_size, self.lstm_hidden_dim).zero_(),\n","                      weight.new(self.lstm_n_layers, batch_size, self.lstm_hidden_dim).zero_())\n","        \n","        return hidden"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MjjvR7ARfbN5","colab_type":"code","outputId":"2820cef6-690c-4c6f-8f4b-47776d6bdab9","executionInfo":{"status":"ok","timestamp":1586726042706,"user_tz":300,"elapsed":857,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Instantiate the model w/ hyperparams\n","a_h = 5\n","\n","lstm_hidden_dim = np.int(len(train_idx) / (a_h * len(numerical_columns) + 1))\n","print(lstm_hidden_dim)\n","lstm_n_layers = 2\n","\n","num_hidden_dim = 8\n","cat_hidden_dim = 8\n","\n","#net = MosquitoLSTM(len(numerical_columns), hidden_dim, n_layers).double()\n","net = MosquitoRedux(\n","    len(numerical_columns), lstm_hidden_dim, lstm_n_layers,\n","    len(non_num_columns), num_hidden_dim,\n","    embedding_sizes, cat_hidden_dim\n",").double()\n","\n","weight_lstm = 2.0\n","weight_num = 3.0\n","weight_cat = 2.0"],"execution_count":0,"outputs":[{"output_type":"stream","text":["16\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"caOIYuZ5foOw","colab_type":"code","colab":{}},"source":["lr = 0.0001\n","\n","criterion = nn.MSELoss()\n","#optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n","optimizer = torch.optim.RMSprop(net.parameters(), lr=lr)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jzkXPhqufsic","colab_type":"code","outputId":"fcacf949-cb75-48e0-fc03-d7e4aa34361b","executionInfo":{"status":"ok","timestamp":1586726104916,"user_tz":300,"elapsed":60014,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# training params\n","\n","epochs = 200 # 3-4 is approx where I noticed the validation loss stop decreasing\n","\n","counter = 0\n","print_every = 10\n","clip = 10 # gradient clipping\n","\n","# move model to GPU, if available\n","if(train_on_gpu):\n","    net.cuda()\n","\n","net.train()\n","# train for some number of epochs\n","valid_loss_min = np.Inf\n","\n","for epoch in range(epochs):\n","    # initialize hidden state\n","    h = net.init_hidden(batch_size)\n","\n","    train_loss = 0.0\n","    valid_loss = 0.0\n","\n","    net.train()\n","\n","    # batch loop\n","    for X_temp, X_num, X_cat, labels in train_loader:\n","        counter += 1\n","\n","        if(train_on_gpu):\n","            X_temp, X_num, X_cat, labels = X_temp.cuda(), X_num.cuda(), X_cat.type(torch.LongTensor).cuda(), labels.cuda()\n","\n","        # Creating new variables for the hidden state, otherwise\n","        # we'd backprop through the entire training history\n","        h = tuple([each.data for each in h])\n","        # zero accumulated gradients\n","        net.zero_grad()\n","\n","        # get the output from the model\n","        lstm_output, h, num_output, cat_output = net(X_temp, h, X_num, X_cat)\n","\n","        output = (weight_lstm*lstm_output + weight_num*num_output + weight_cat*cat_output) \n","\n","        # calculate the loss and perform backprop\n","        loss = criterion(output.squeeze(), labels.double())\n","        loss.backward()\n","        train_loss += loss.item()\n","        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","        nn.utils.clip_grad_norm_(net.parameters(), clip)\n","        optimizer.step()\n","\n","    net.eval()\n","\n","    val_h = net.init_hidden(batch_size)\n","\n","    for X_temp, X_num, X_cat, labels in valid_loader:\n","        \n","        \n","        if(train_on_gpu):\n","            X_temp, X_num, X_cat, labels = X_temp.cuda(), X_num.cuda(), X_cat.type(torch.LongTensor).cuda(), labels.cuda()\n","\n","        # Creating new variables for the hidden state, otherwise\n","        # we'd backprop through the entire training history\n","        val_h = tuple([each.data for each in val_h])\n","\n","        # get the output from the model\n","        lstm_output, h, num_output, cat_output = net(X_temp, h, X_num, X_cat)\n","\n","        # calculate the loss and perform backprop\n","        output = (weight_lstm*lstm_output + weight_num*num_output + weight_cat*cat_output) \n","        loss = criterion(output.squeeze(), labels.double())\n","        valid_loss += loss.item()\n","\n","    # save model if validation loss has decreased\n","    if valid_loss <= valid_loss_min:\n","        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","        valid_loss_min,\n","        valid_loss))\n","        torch.save(net.state_dict(), 'lstm_test.pt')\n","        valid_loss_min = valid_loss\n","        \n","    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n","            epoch, train_loss, valid_loss))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Validation loss decreased (inf --> 2467.577166).  Saving model ...\n","Epoch: 0 \tTraining Loss: 4155.658451 \tValidation Loss: 2467.577166\n","Validation loss decreased (2467.577166 --> 2360.766804).  Saving model ...\n","Epoch: 1 \tTraining Loss: 3956.668756 \tValidation Loss: 2360.766804\n","Validation loss decreased (2360.766804 --> 1940.561754).  Saving model ...\n","Epoch: 2 \tTraining Loss: 4033.685055 \tValidation Loss: 1940.561754\n","Epoch: 3 \tTraining Loss: 3949.039796 \tValidation Loss: 2305.362908\n","Epoch: 4 \tTraining Loss: 4005.923488 \tValidation Loss: 2237.808165\n","Epoch: 5 \tTraining Loss: 4078.748406 \tValidation Loss: 2214.898759\n","Epoch: 6 \tTraining Loss: 3885.448730 \tValidation Loss: 2200.243593\n","Epoch: 7 \tTraining Loss: 3960.944540 \tValidation Loss: 2193.760746\n","Epoch: 8 \tTraining Loss: 3900.574044 \tValidation Loss: 2220.251198\n","Epoch: 9 \tTraining Loss: 3767.570995 \tValidation Loss: 2201.358506\n","Epoch: 10 \tTraining Loss: 3828.652089 \tValidation Loss: 2209.642114\n","Epoch: 11 \tTraining Loss: 3826.675027 \tValidation Loss: 2194.738392\n","Validation loss decreased (1940.561754 --> 1773.996943).  Saving model ...\n","Epoch: 12 \tTraining Loss: 3838.623918 \tValidation Loss: 1773.996943\n","Epoch: 13 \tTraining Loss: 3747.233818 \tValidation Loss: 1872.800607\n","Epoch: 14 \tTraining Loss: 3857.868998 \tValidation Loss: 1787.270632\n","Epoch: 15 \tTraining Loss: 3790.414100 \tValidation Loss: 2248.140408\n","Epoch: 16 \tTraining Loss: 3747.446933 \tValidation Loss: 2237.657974\n","Epoch: 17 \tTraining Loss: 3837.641518 \tValidation Loss: 2249.180976\n","Epoch: 18 \tTraining Loss: 3771.229632 \tValidation Loss: 2251.323303\n","Epoch: 19 \tTraining Loss: 3764.475083 \tValidation Loss: 2264.049983\n","Epoch: 20 \tTraining Loss: 3633.420575 \tValidation Loss: 2247.018530\n","Epoch: 21 \tTraining Loss: 3645.433339 \tValidation Loss: 2172.794058\n","Validation loss decreased (1773.996943 --> 1311.288798).  Saving model ...\n","Epoch: 22 \tTraining Loss: 3758.568953 \tValidation Loss: 1311.288798\n","Epoch: 23 \tTraining Loss: 3709.202080 \tValidation Loss: 2244.381818\n","Epoch: 24 \tTraining Loss: 3507.866295 \tValidation Loss: 2235.811845\n","Epoch: 25 \tTraining Loss: 3609.379756 \tValidation Loss: 2258.076472\n","Epoch: 26 \tTraining Loss: 3674.324386 \tValidation Loss: 2237.628653\n","Epoch: 27 \tTraining Loss: 3595.955805 \tValidation Loss: 2229.444103\n","Epoch: 28 \tTraining Loss: 3517.592811 \tValidation Loss: 2232.941203\n","Epoch: 29 \tTraining Loss: 3604.708758 \tValidation Loss: 1879.609079\n","Epoch: 30 \tTraining Loss: 3518.339384 \tValidation Loss: 2214.408090\n","Epoch: 31 \tTraining Loss: 3369.143232 \tValidation Loss: 2206.591450\n","Epoch: 32 \tTraining Loss: 3539.385524 \tValidation Loss: 2191.875246\n","Epoch: 33 \tTraining Loss: 3538.867974 \tValidation Loss: 2182.895936\n","Epoch: 34 \tTraining Loss: 3440.628666 \tValidation Loss: 2203.444741\n","Epoch: 35 \tTraining Loss: 3170.567607 \tValidation Loss: 2166.556651\n","Epoch: 36 \tTraining Loss: 3502.440688 \tValidation Loss: 2182.515647\n","Epoch: 37 \tTraining Loss: 3493.462777 \tValidation Loss: 2177.802229\n","Epoch: 38 \tTraining Loss: 3447.941638 \tValidation Loss: 2160.973301\n","Epoch: 39 \tTraining Loss: 3361.906252 \tValidation Loss: 2157.558604\n","Epoch: 40 \tTraining Loss: 3414.585981 \tValidation Loss: 2166.060600\n","Epoch: 41 \tTraining Loss: 3346.543185 \tValidation Loss: 2165.884117\n","Epoch: 42 \tTraining Loss: 3282.890655 \tValidation Loss: 2156.711319\n","Epoch: 43 \tTraining Loss: 3300.580232 \tValidation Loss: 1711.213173\n","Epoch: 44 \tTraining Loss: 3361.060620 \tValidation Loss: 2140.977030\n","Epoch: 45 \tTraining Loss: 3421.668291 \tValidation Loss: 2154.979018\n","Epoch: 46 \tTraining Loss: 3383.497894 \tValidation Loss: 2130.599017\n","Epoch: 47 \tTraining Loss: 3238.950929 \tValidation Loss: 2116.210787\n","Epoch: 48 \tTraining Loss: 3307.457172 \tValidation Loss: 2103.864179\n","Epoch: 49 \tTraining Loss: 3381.728354 \tValidation Loss: 2143.510911\n","Epoch: 50 \tTraining Loss: 3455.999762 \tValidation Loss: 2062.011155\n","Epoch: 51 \tTraining Loss: 3378.785510 \tValidation Loss: 2121.771243\n","Epoch: 52 \tTraining Loss: 3297.151433 \tValidation Loss: 2122.769932\n","Epoch: 53 \tTraining Loss: 3236.557699 \tValidation Loss: 2110.320157\n","Epoch: 54 \tTraining Loss: 3318.088788 \tValidation Loss: 2122.663855\n","Epoch: 55 \tTraining Loss: 3079.395697 \tValidation Loss: 2104.998093\n","Epoch: 56 \tTraining Loss: 3229.567223 \tValidation Loss: 2106.754723\n","Epoch: 57 \tTraining Loss: 3328.443755 \tValidation Loss: 2117.918972\n","Epoch: 58 \tTraining Loss: 3326.751589 \tValidation Loss: 2053.309422\n","Epoch: 59 \tTraining Loss: 3254.144563 \tValidation Loss: 2055.259388\n","Epoch: 60 \tTraining Loss: 3283.564979 \tValidation Loss: 2115.829027\n","Epoch: 61 \tTraining Loss: 3253.428498 \tValidation Loss: 2096.866341\n","Epoch: 62 \tTraining Loss: 2972.866344 \tValidation Loss: 2102.946646\n","Epoch: 63 \tTraining Loss: 3204.735440 \tValidation Loss: 2111.444021\n","Epoch: 64 \tTraining Loss: 3288.395574 \tValidation Loss: 2097.955727\n","Epoch: 65 \tTraining Loss: 3218.535326 \tValidation Loss: 2085.477077\n","Epoch: 66 \tTraining Loss: 3145.950734 \tValidation Loss: 2093.904881\n","Epoch: 67 \tTraining Loss: 3139.360329 \tValidation Loss: 2095.773984\n","Epoch: 68 \tTraining Loss: 3177.832856 \tValidation Loss: 2063.846489\n","Epoch: 69 \tTraining Loss: 3218.766446 \tValidation Loss: 1627.410380\n","Epoch: 70 \tTraining Loss: 3108.147639 \tValidation Loss: 2082.973967\n","Epoch: 71 \tTraining Loss: 3117.563969 \tValidation Loss: 2082.471906\n","Epoch: 72 \tTraining Loss: 3095.463310 \tValidation Loss: 2076.856297\n","Epoch: 73 \tTraining Loss: 3105.404369 \tValidation Loss: 2069.369320\n","Epoch: 74 \tTraining Loss: 3162.073927 \tValidation Loss: 2017.335707\n","Epoch: 75 \tTraining Loss: 3052.769769 \tValidation Loss: 2070.724432\n","Epoch: 76 \tTraining Loss: 3140.472483 \tValidation Loss: 2076.944694\n","Epoch: 77 \tTraining Loss: 3241.196668 \tValidation Loss: 2073.705005\n","Epoch: 78 \tTraining Loss: 3132.991193 \tValidation Loss: 1724.824675\n","Epoch: 79 \tTraining Loss: 3204.229307 \tValidation Loss: 2059.073966\n","Epoch: 80 \tTraining Loss: 3179.709094 \tValidation Loss: 2061.977611\n","Epoch: 81 \tTraining Loss: 3158.308035 \tValidation Loss: 2062.368973\n","Epoch: 82 \tTraining Loss: 3008.685071 \tValidation Loss: 2055.926275\n","Epoch: 83 \tTraining Loss: 3152.557738 \tValidation Loss: 2059.161811\n","Epoch: 84 \tTraining Loss: 3134.859379 \tValidation Loss: 2050.848789\n","Epoch: 85 \tTraining Loss: 3077.861604 \tValidation Loss: 2068.083502\n","Epoch: 86 \tTraining Loss: 3081.034436 \tValidation Loss: 2061.196526\n","Epoch: 87 \tTraining Loss: 3204.975935 \tValidation Loss: 2058.309567\n","Epoch: 88 \tTraining Loss: 2942.417495 \tValidation Loss: 2062.681323\n","Epoch: 89 \tTraining Loss: 3095.168504 \tValidation Loss: 2054.311821\n","Epoch: 90 \tTraining Loss: 3137.208104 \tValidation Loss: 2059.827510\n","Epoch: 91 \tTraining Loss: 3031.165651 \tValidation Loss: 2058.749151\n","Epoch: 92 \tTraining Loss: 3059.429467 \tValidation Loss: 2065.995934\n","Epoch: 93 \tTraining Loss: 3054.310345 \tValidation Loss: 1999.886927\n","Epoch: 94 \tTraining Loss: 3077.135672 \tValidation Loss: 2052.459525\n","Epoch: 95 \tTraining Loss: 3035.038208 \tValidation Loss: 2051.910903\n","Epoch: 96 \tTraining Loss: 2981.203920 \tValidation Loss: 1588.659240\n","Epoch: 97 \tTraining Loss: 3166.724464 \tValidation Loss: 1623.956327\n","Epoch: 98 \tTraining Loss: 2988.832874 \tValidation Loss: 2039.556913\n","Epoch: 99 \tTraining Loss: 3085.495457 \tValidation Loss: 2041.573548\n","Epoch: 100 \tTraining Loss: 3034.233124 \tValidation Loss: 2045.407721\n","Epoch: 101 \tTraining Loss: 3050.274054 \tValidation Loss: 2041.599559\n","Epoch: 102 \tTraining Loss: 2962.704146 \tValidation Loss: 2038.985813\n","Epoch: 103 \tTraining Loss: 3134.955502 \tValidation Loss: 2047.301270\n","Epoch: 104 \tTraining Loss: 3119.952210 \tValidation Loss: 2048.480457\n","Epoch: 105 \tTraining Loss: 3109.015274 \tValidation Loss: 2044.870940\n","Epoch: 106 \tTraining Loss: 3071.893903 \tValidation Loss: 2047.729667\n","Validation loss decreased (1311.288798 --> 1148.480186).  Saving model ...\n","Epoch: 107 \tTraining Loss: 2968.973616 \tValidation Loss: 1148.480186\n","Validation loss decreased (1148.480186 --> 1141.601498).  Saving model ...\n","Epoch: 108 \tTraining Loss: 2952.924104 \tValidation Loss: 1141.601498\n","Epoch: 109 \tTraining Loss: 3072.724857 \tValidation Loss: 2047.319001\n","Epoch: 110 \tTraining Loss: 3108.732234 \tValidation Loss: 2036.797671\n","Epoch: 111 \tTraining Loss: 3076.555877 \tValidation Loss: 2043.745366\n","Epoch: 112 \tTraining Loss: 2948.611617 \tValidation Loss: 2018.591547\n","Epoch: 113 \tTraining Loss: 3004.702021 \tValidation Loss: 2023.685515\n","Epoch: 114 \tTraining Loss: 3120.870516 \tValidation Loss: 2032.738261\n","Epoch: 115 \tTraining Loss: 2962.324485 \tValidation Loss: 2002.048610\n","Epoch: 116 \tTraining Loss: 2899.891900 \tValidation Loss: 2034.406132\n","Epoch: 117 \tTraining Loss: 3127.326608 \tValidation Loss: 2041.284829\n","Epoch: 118 \tTraining Loss: 2939.516159 \tValidation Loss: 2009.062364\n","Epoch: 119 \tTraining Loss: 3077.866175 \tValidation Loss: 2025.513249\n","Epoch: 120 \tTraining Loss: 2957.798436 \tValidation Loss: 2003.172587\n","Epoch: 121 \tTraining Loss: 3134.829306 \tValidation Loss: 2028.730082\n","Epoch: 122 \tTraining Loss: 3058.963413 \tValidation Loss: 2035.082432\n","Epoch: 123 \tTraining Loss: 3017.505348 \tValidation Loss: 2029.450894\n","Epoch: 124 \tTraining Loss: 3089.447139 \tValidation Loss: 2023.589337\n","Epoch: 125 \tTraining Loss: 2991.132460 \tValidation Loss: 2018.840953\n","Epoch: 126 \tTraining Loss: 2849.750375 \tValidation Loss: 1608.632822\n","Epoch: 127 \tTraining Loss: 3034.204967 \tValidation Loss: 2033.784804\n","Epoch: 128 \tTraining Loss: 2957.477232 \tValidation Loss: 2036.561979\n","Epoch: 129 \tTraining Loss: 3024.165768 \tValidation Loss: 2034.654561\n","Epoch: 130 \tTraining Loss: 3058.252344 \tValidation Loss: 2037.243239\n","Epoch: 131 \tTraining Loss: 3052.241147 \tValidation Loss: 2028.547510\n","Epoch: 132 \tTraining Loss: 3024.598643 \tValidation Loss: 2033.776041\n","Epoch: 133 \tTraining Loss: 3012.552779 \tValidation Loss: 2031.322333\n","Epoch: 134 \tTraining Loss: 2964.700810 \tValidation Loss: 2011.420881\n","Epoch: 135 \tTraining Loss: 2947.934025 \tValidation Loss: 1999.793054\n","Epoch: 136 \tTraining Loss: 2927.857933 \tValidation Loss: 2028.234616\n","Epoch: 137 \tTraining Loss: 2991.439473 \tValidation Loss: 2033.458024\n","Epoch: 138 \tTraining Loss: 2929.965417 \tValidation Loss: 1984.978362\n","Epoch: 139 \tTraining Loss: 2813.058770 \tValidation Loss: 2034.760772\n","Epoch: 140 \tTraining Loss: 2988.226581 \tValidation Loss: 1986.862810\n","Epoch: 141 \tTraining Loss: 2838.654261 \tValidation Loss: 2029.220552\n","Epoch: 142 \tTraining Loss: 2993.805883 \tValidation Loss: 2034.428959\n","Epoch: 143 \tTraining Loss: 3034.998634 \tValidation Loss: 1992.081167\n","Epoch: 144 \tTraining Loss: 3048.072590 \tValidation Loss: 1615.405461\n","Epoch: 145 \tTraining Loss: 2956.180697 \tValidation Loss: 2034.275422\n","Epoch: 146 \tTraining Loss: 2935.154308 \tValidation Loss: 2035.148673\n","Epoch: 147 \tTraining Loss: 2960.938939 \tValidation Loss: 1984.078640\n","Epoch: 148 \tTraining Loss: 2989.947203 \tValidation Loss: 2028.170251\n","Epoch: 149 \tTraining Loss: 3016.067489 \tValidation Loss: 2029.785993\n","Epoch: 150 \tTraining Loss: 2956.562368 \tValidation Loss: 2016.260969\n","Epoch: 151 \tTraining Loss: 2952.449048 \tValidation Loss: 2032.406650\n","Epoch: 152 \tTraining Loss: 2946.248186 \tValidation Loss: 2021.232042\n","Epoch: 153 \tTraining Loss: 2925.403901 \tValidation Loss: 2014.879400\n","Epoch: 154 \tTraining Loss: 2984.241625 \tValidation Loss: 2024.616221\n","Epoch: 155 \tTraining Loss: 3060.200839 \tValidation Loss: 2014.791236\n","Epoch: 156 \tTraining Loss: 2953.748161 \tValidation Loss: 2030.621701\n","Epoch: 157 \tTraining Loss: 2959.278219 \tValidation Loss: 2032.612497\n","Epoch: 158 \tTraining Loss: 2938.963840 \tValidation Loss: 2022.904975\n","Epoch: 159 \tTraining Loss: 2931.157980 \tValidation Loss: 2029.410391\n","Epoch: 160 \tTraining Loss: 2872.269093 \tValidation Loss: 1957.796136\n","Epoch: 161 \tTraining Loss: 2851.417710 \tValidation Loss: 2014.703064\n","Epoch: 162 \tTraining Loss: 2950.243076 \tValidation Loss: 2028.726623\n","Epoch: 163 \tTraining Loss: 2923.004252 \tValidation Loss: 2019.862658\n","Epoch: 164 \tTraining Loss: 3002.585760 \tValidation Loss: 2008.553308\n","Epoch: 165 \tTraining Loss: 2974.337593 \tValidation Loss: 1973.983444\n","Epoch: 166 \tTraining Loss: 2998.624560 \tValidation Loss: 2027.922157\n","Epoch: 167 \tTraining Loss: 3003.559415 \tValidation Loss: 2024.845359\n","Epoch: 168 \tTraining Loss: 2981.172820 \tValidation Loss: 1994.307299\n","Validation loss decreased (1141.601498 --> 1121.416110).  Saving model ...\n","Epoch: 169 \tTraining Loss: 2872.389763 \tValidation Loss: 1121.416110\n","Epoch: 170 \tTraining Loss: 2909.657879 \tValidation Loss: 2025.288456\n","Epoch: 171 \tTraining Loss: 2892.977561 \tValidation Loss: 2015.190548\n","Epoch: 172 \tTraining Loss: 2879.788155 \tValidation Loss: 2015.861475\n","Epoch: 173 \tTraining Loss: 2922.894904 \tValidation Loss: 2024.592578\n","Epoch: 174 \tTraining Loss: 2975.331683 \tValidation Loss: 2029.457494\n","Epoch: 175 \tTraining Loss: 2946.773914 \tValidation Loss: 2019.968446\n","Epoch: 176 \tTraining Loss: 2991.032187 \tValidation Loss: 2014.297189\n","Epoch: 177 \tTraining Loss: 2933.883617 \tValidation Loss: 1987.279557\n","Epoch: 178 \tTraining Loss: 2872.345177 \tValidation Loss: 2021.560589\n","Epoch: 179 \tTraining Loss: 2977.091975 \tValidation Loss: 2016.330158\n","Epoch: 180 \tTraining Loss: 2984.025018 \tValidation Loss: 2012.316093\n","Epoch: 181 \tTraining Loss: 2969.888387 \tValidation Loss: 2018.052428\n","Epoch: 182 \tTraining Loss: 2905.699891 \tValidation Loss: 2020.584001\n","Epoch: 183 \tTraining Loss: 2965.712649 \tValidation Loss: 2019.882640\n","Epoch: 184 \tTraining Loss: 2952.466889 \tValidation Loss: 2019.672077\n","Epoch: 185 \tTraining Loss: 2907.305329 \tValidation Loss: 2021.199661\n","Epoch: 186 \tTraining Loss: 3002.721960 \tValidation Loss: 2015.002384\n","Epoch: 187 \tTraining Loss: 2936.721544 \tValidation Loss: 1947.834923\n","Epoch: 188 \tTraining Loss: 2889.994571 \tValidation Loss: 2012.018362\n","Epoch: 189 \tTraining Loss: 2927.501893 \tValidation Loss: 1271.786283\n","Epoch: 190 \tTraining Loss: 3023.479810 \tValidation Loss: 2018.265939\n","Epoch: 191 \tTraining Loss: 2938.130107 \tValidation Loss: 2008.703164\n","Epoch: 192 \tTraining Loss: 3028.692537 \tValidation Loss: 1609.300569\n","Epoch: 193 \tTraining Loss: 2957.321563 \tValidation Loss: 1980.681460\n","Epoch: 194 \tTraining Loss: 2929.884471 \tValidation Loss: 2015.305378\n","Epoch: 195 \tTraining Loss: 2956.386488 \tValidation Loss: 1604.595964\n","Epoch: 196 \tTraining Loss: 2987.067067 \tValidation Loss: 2016.641097\n","Epoch: 197 \tTraining Loss: 2530.314500 \tValidation Loss: 1668.969232\n","Epoch: 198 \tTraining Loss: 2851.649975 \tValidation Loss: 2019.339791\n","Epoch: 199 \tTraining Loss: 2966.346758 \tValidation Loss: 2010.629766\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MPiYWv9UoFP_","colab_type":"code","outputId":"c406b90d-a99c-4c45-9a30-99677744ca4b","executionInfo":{"status":"ok","timestamp":1586726249133,"user_tz":300,"elapsed":953,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["net.load_state_dict(torch.load('lstm_test.pt'))"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":237}]},{"cell_type":"code","metadata":{"id":"LOYJ_iJ0gCED","colab_type":"code","outputId":"c3c52e73-4188-46a6-82e7-971c3bd56516","executionInfo":{"status":"ok","timestamp":1586726250898,"user_tz":300,"elapsed":1262,"user":{"displayName":"Cuauhtémoc Daniel Suárez Ramírez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjQPoZOkbyUJA6gwA5q-NodnQQneCHn6a64cwiJ=s64","userId":"07373854176311364379"}},"colab":{"base_uri":"https://localhost:8080/","height":199}},"source":["from sklearn.metrics import mean_absolute_error\n","\n","# track test loss\n","test_loss = 0.0\n","mean_abs = 0.0\n","\n","net.eval()\n","# iterate over test data\n","#bin_op.binarization()\n","test_h = net.init_hidden(batch_size)\n","\n","for batch_idx, (X_temp, X_num, X_cat, labels) in enumerate(test_loader):\n","    if(train_on_gpu):\n","        X_temp, X_num, X_cat, labels = X_temp.cuda(), X_num.cuda(), X_cat.type(torch.LongTensor).cuda(), labels.cuda()\n","    \n","    test_h = tuple([each.data for each in test_h])\n","\n","    # get the output from the model\n","    lstm_output, h, num_output, cat_output = net(X_temp, h, X_num, X_cat)\n","\n","    output = (weight_lstm*lstm_output + weight_num*num_output + weight_cat*cat_output) \n","    # / (weight_lstm + weight_num + weight_cat)\n","\n","    # calculate the loss and perform backprop\n","    loss = criterion(output.squeeze(), labels.double())\n","\n","    out_np = output.detach().squeeze().cpu().numpy().astype(int).T\n","    target_np = labels.detach().cpu().numpy().astype(int).T\n","\n","    print([(out, tar) for out, tar in zip(out_np, target_np)])\n","\n","    mean_abs += mean_absolute_error(out_np, target_np) * batch_size\n","        \n","    test_loss += loss.item() * batch_size\n","\n","# calculate average losses\n","test_loss = np.sqrt(test_loss / len(test_loader.dataset))\n","mean_abs /= len(test_loader.dataset)\n","print('Test Loss: {:.6f}\\n'.format(test_loss))\n","print('Mean Abs Loss: {:.6f}\\n'.format(mean_abs))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[(4, 3), (4, 3), (4, 2), (4, 2), (4, 2), (4, 3), (4, 3), (4, 3), (4, 2), (4, 6), (4, 4), (4, 4), (4, 5), (4, 3), (4, 2), (4, 16)]\n","[(4, 4), (4, 10), (4, 5), (4, 3), (4, 8), (5, 2), (4, 9), (4, 66), (5, 3), (4, 9), (4, 11), (4, 8), (4, 25), (4, 4), (5, 3), (5, 2)]\n","[(4, 3), (9, 3), (9, 6), (9, 3), (9, 8), (9, 8), (9, 6), (9, 9), (9, 12), (9, 18), (9, 24), (9, 19), (7, 3), (7, 15), (9, 12), (7, 12)]\n","[(9, 17), (9, 9), (7, 3), (7, 12), (9, 33), (9, 11), (7, 6), (7, 2), (9, 7), (9, 5), (7, 2), (9, 11), (9, 23), (9, 4), (9, 7), (7, 5)]\n","[(7, 6), (7, 10), (7, 12), (7, 5), (7, 10), (8, 2), (6, 3), (7, 32), (7, 13), (8, 27), (7, 9), (7, 12), (8, 16), (7, 6), (8, 3), (7, 7)]\n","[(8, 2), (8, 6), (6, 10), (7, 6), (7, 2), (8, 3), (8, 5), (7, 2), (7, 5), (8, 17), (7, 18), (7, 3), (8, 8), (6, 2), (7, 15), (8, 5)]\n","Test Loss: 8.595820\n","\n","Mean Abs Loss: 4.628571\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zzQ2R1qpANSB","colab_type":"text"},"source":["## DA-RNN"]},{"cell_type":"code","metadata":{"id":"XXan7xNDo960","colab_type":"code","colab":{}},"source":["nhidden_encoder = 128\n","nhidden_decoder = 128"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BT2EqRbMAVm7","colab_type":"code","colab":{}},"source":["def init_hidden(x, hidden_size: int):\n","    if train_on_gpu:\n","            return Variable(X.data.new(1, X.size(0), self.encoder_num_hidden).zero_()).cuda()\n","        else:\n","            return Variable(X.data.new(1, X.size(0), self.encoder_num_hidden).zero_())\n","\n","class Encoder(nn.Module):\n","    def __init__(self, T,\n","                 input_size,\n","                 encoder_num_hidden,\n","                 parallel=False):\n","        \n","        super(Encoder, self).__init__()\n","        self.encoder_num_hidden = encoder_num_hidden\n","        self.input_size = input_size\n","        self.parallel = parallel\n","        self.T = T\n","\n","        # Fig 1. Temporal Attention Mechanism: Encoder is LSTM\n","        self.encoder_lstm = nn.LSTM(\n","            input_size=self.input_size,\n","            hidden_size=self.encoder_num_hidden,\n","            num_layers = 1,\n","            batch_first = True\n","        )\n","\n","        # Construct Input Attention Mechanism via deterministic attention model\n","        # Eq. 8: W_e[h_{t-1}; s_{t-1}] + U_e * x^k\n","        self.encoder_attn = nn.Linear(\n","            in_features=2 * self.encoder_num_hidden + self.T - 1,\n","            out_features=1\n","        )\n","\n","    def forward(self, X):\n","        X_tilde = Variable(X.data.new(\n","            X.size(0), self.T - 1, self.input_size).zero_())\n","        X_encoded = Variable(X.data.new(\n","            X.size(0), self.T - 1, self.encoder_num_hidden).zero_())\n","\n","        # h_n, s_n: initial states with dimention hidden_size\n","        h_n = self._init_states(X)\n","        s_n = self._init_states(X)\n","\n","\n","\n","        for t in range(self.T - 1):\n","            # batch_size * input_size * (2 * hidden_size + T - 1)\n","            x = torch.cat((h_n.repeat(self.input_size, 1, 1).permute(1, 0, 2),\n","                           s_n.repeat(self.input_size, 1, 1).permute(1, 0, 2),\n","                           X.permute(0, 2, 1)), dim=2)\n","\n","            x = self.encoder_attn(\n","                x.view(-1, self.encoder_num_hidden * 2 + self.T - 1))\n","\n","            # get weights by softmax\n","            alpha = F.softmax(x.view(-1, self.input_size))\n","            x_tilde = torch.mul(alpha, X[:, t, :])\n","\n","            self.encoder_lstm.flatten_parameters()\n","\n","            # encoder LSTM\n","            _, final_state = self.encoder_lstm(x_tilde.unsqueeze(0), (h_n, s_n))\n","            h_n = final_state[0]\n","            s_n = final_state[1]\n","\n","            X_tilde[:, t, :] = x_tilde\n","            X_encoded[:, t, :] = h_n\n","\n","        return X_tilde, X_encoded\n","\n","    def _init_states(self, X):\n","        if train_on_gpu:\n","            return Variable(X.data.new(1, X.size(0), self.encoder_num_hidden).zero_()).cuda()\n","        else:\n","            return Variable(X.data.new(1, X.size(0), self.encoder_num_hidden).zero_())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TzJLH5wzAgWV","colab_type":"code","colab":{}},"source":["class Decoder(nn.Module):\n","    \"\"\"decoder in DA_RNN.\"\"\"\n","\n","    def __init__(self, T, decoder_num_hidden, encoder_num_hidden):\n","        \"\"\"Initialize a decoder in DA_RNN.\"\"\"\n","        super(Decoder, self).__init__()\n","        self.decoder_num_hidden = decoder_num_hidden\n","        self.encoder_num_hidden = encoder_num_hidden\n","        self.T = T\n","\n","        self.attn_layer = nn.Sequential(\n","            nn.Linear(2 * decoder_num_hidden + encoder_num_hidden, encoder_num_hidden),\n","            nn.Tanh(),\n","            nn.Linear(encoder_num_hidden, 1)\n","        )\n","        self.lstm_layer = nn.LSTM(\n","            input_size=1,\n","            hidden_size=decoder_num_hidden\n","        )\n","        self.fc = nn.Linear(encoder_num_hidden + 1, 1)\n","        self.fc_final = nn.Linear(decoder_num_hidden + encoder_num_hidden, 1)\n","\n","        self.fc.weight.data.normal_()\n","\n","    def forward(self, X_encoded, y_prev):\n","        \"\"\"forward.\"\"\"\n","        d_n = self._init_states(X_encoded)\n","        c_n = self._init_states(X_encoded)\n","\n","        for t in range(self.T - 1):\n","\n","            x = torch.cat((d_n.repeat(self.T - 1, 1, 1).permute(1, 0, 2),\n","                           c_n.repeat(self.T - 1, 1, 1).permute(1, 0, 2),\n","                           X_encoded), dim=2)\n","\n","            beta = F.softmax(self.attn_layer(\n","                x.view(-1, 2 * self.decoder_num_hidden + self.encoder_num_hidden)).view(-1, self.T - 1))\n","\n","            # Eqn. 14: compute context vector\n","            # batch_size * encoder_hidden_size\n","            context = torch.bmm(beta.unsqueeze(1), X_encoded)[:, 0, :]\n","            if t < self.T - 1:\n","                # Eqn. 15\n","                # batch_size * 1\n","                y_tilde = self.fc(\n","                    torch.cat((context, y_prev[:, t].unsqueeze(1)), dim=1))\n","\n","                # Eqn. 16: LSTM\n","                self.lstm_layer.flatten_parameters()\n","                _, final_states = self.lstm_layer(\n","                    y_tilde.unsqueeze(0), (d_n, c_n))\n","\n","                d_n = final_states[0]  # 1 * batch_size * decoder_num_hidden\n","                c_n = final_states[1]  # 1 * batch_size * decoder_num_hidden\n","\n","        # Eqn. 22: final output\n","        y_pred = self.fc_final(torch.cat((d_n[0], context), dim=1))\n","\n","        return y_pred\n","\n","    def _init_states(self, X):\n","        if train_on_gpu:\n","            return Variable(X.data.new(1, X.size(0), self.encoder_num_hidden).zero_()).cuda()\n","        else:\n","            return Variable(X.data.new(1, X.size(0), self.encoder_num_hidden).zero_())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nv1r9oLOQt0m","colab_type":"code","colab":{}},"source":["class DA_rnn(nn.Module):\n","    \"\"\"da_rnn.\"\"\"\n","\n","    def __init__(self, temp_columns, T,\n","                 encoder_num_hidden,\n","                 decoder_num_hidden,\n","                 batch_size,\n","                 learning_rate,\n","                 epochs,\n","                 parallel=False):\n","        \"\"\"da_rnn initialization.\"\"\"\n","        super(DA_rnn, self).__init__()\n","        self.encoder_num_hidden = encoder_num_hidden\n","        self.decoder_num_hidden = decoder_num_hidden\n","        self.parallel = parallel\n","        self.T = T\n","\n","        self.Encoder = Encoder(input_size=temp_columns,\n","                               encoder_num_hidden=encoder_num_hidden,\n","                               T=T)\n","        self.Decoder = Decoder(encoder_num_hidden=encoder_num_hidden,\n","                               decoder_num_hidden=decoder_num_hidden,\n","                               T=T)\n","\n","\n","        if self.parallel:\n","            self.encoder = nn.DataParallel(self.encoder)\n","            self.decoder = nn.DataParallel(self.decoder)\n","\n","    def forward(self, X, y_prev, y_gt):\n","        input_weighted, input_encoded = self.Encoder(\n","            Variable(torch.from_numpy(X).type(torch.FloatTensor).to(self.device)))\n","        y_pred = self.Decoder(input_encoded, Variable(\n","            torch.from_numpy(y_prev).type(torch.FloatTensor).to(self.device)))\n","\n","\n","    def train_forward(self, X, y_prev):\n","        input_weighted, input_encoded = self.Encoder(X)\n","        y_pred = self.Decoder(input_encoded, y_prev)\n","\n","        return input_weighted, input_encoded, y_pred"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-vXNirruQ2kC","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}
